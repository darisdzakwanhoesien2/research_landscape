Analyzing Sustainability Reports Using Natural Language Processing
(missing doi
Climate change is a far-reaching, global phenomenon that will impact many aspects of our society, including the global stock market \cite{dietz2016climate}. In recent years, companies have increasingly been aiming to both mitigate their environmental impact and adapt to the changing climate context. This is reported via increasingly exhaustive reports, which cover many types of climate risks and exposures under the umbrella of Environmental, Social, and Governance (ESG). However, given this abundance of data, sustainability analysts are obliged to comb through hundreds of pages of reports in order to find relevant information. We leveraged recent progress in Natural Language Processing (NLP) to create a custom model, ClimateQA, which allows the analysis of financial reports in order to identify climate-relevant sections based on a question answering approach. We present this tool and the methodology that we used to develop it in the present article.

---

Tests of English for Academic Purposes in University Admissions
10.1002/9781118411360.wbcla139
This chapter charts the history, surveys the current developments, and discusses the future trends in English for academic purposes (EAP) assessments used for admissions to post-secondary English-medium institutions. A historical overview of the origin and growth of EAP assessments used for university admissions is provided, focusing on the evolution of test philosophies and on test content for the various assessments. Current trends of EAP testing in admissions are discussed, focusing on test constructs, delivery method, scoring methods and technologies, and score reporting and interpretation. The chapter reports on current research using the argument-based approach to test validation, and presents exemplary studies addressing validity inferences including domain definition, evaluation, generalization, explanation, extrapolation, and utilization. It concludes with a discussion of future trends and of critical development and research issues to be addressed. 
 
 
Keywords: 
 
Assessment evaluation; 
Language for academic purposes; 
Language for specific purposes

---

Unveiling the blackbox within <scp>ESG</scp> ratings' blackbox: Toward a framework for analyzing AI adoption and its impacts
10.1002/bsd2.70038
Abstract Artificial intelligence (AI) is transforming entire industries at an unprecedented pace. Yet, established technology adoption theories offer limited tools for characterizing business AI integration and analyzing its effects. These primarily focus on the factors facilitating or hindering adoption, rather than on adoption patterns and impacts. This paper introduces a novel conceptual framework to address this key gap and applies it to the case of the ESG rating industry. ESG raters play a pivotal role in sustainable finance, providing metrics that guide investment decisions globally. However, little is known about the extent and nature of their AI usage and its implications. Through a mixed‐methods approach combining the analysis of job postings, patent filings, research publications, and corporate websites, we examine AI adoption among major ESG raters. Our investigation explores the specific AI technologies employed, their functional applications, the innovations developed, the intensity of AI integration, and the potential impacts of raters' AI adoption. Our results reveal widespread and growing AI adoption across the industry. Our findings show that raters extensively leverage Natural Language Processing to streamline data collection, processing, and analysis. Furthermore, they have pioneered Machine Learning innovations that significantly expand their sustainability assessment capabilities in various domains. These findings mark a considerable departure from prior academic and gray literature that characterized major ESG raters as having minimal AI use, prompting critical questions regarding the implications of this technological transformation for ESG ratings' reliability, transparency, and potential biases.

---

Advancing Firm-Specific ESG Sentiment Analysis: A Machine Learning Approach to Clustering, Prediction, and Financial Performance Implications
10.1002/bsd2.70150
ABSTRACT As corporate sustainability faces increasing scrutiny from investors and policymakers, measuring firm‐specific ESG sentiment remains a challenge. Traditional ESG indices often aggregate data at the industry or national level, overlooking firm‐level variations. This study introduces a data‐driven framework that integrates clustering analysis, principal component analysis (PCA), and machine learning models to identify distinct ESG sentiment patterns. Using K‐means and Hierarchical Clustering, firms are segmented based on ESG sentiment scores, uncovering significant differences in sustainability disclosures and financial risk. Predictive modeling with Random Forest and XGBoost further demonstrates that firms with higher ESG sentiment experience lower financial risk and greater investor confidence. Statistical validation through ANOVA analysis confirms the robustness of the identified clusters, revealing substantial heterogeneity in ESG reporting. Grounded in legitimacy and signaling theories, this research explains how firms strategically disclose ESG information to enhance credibility and attract capital. By providing a quantitative ESG assessment framework, this study offers valuable insights for investors, policymakers, and corporate leaders seeking to improve investment decisions, regulatory compliance, and sustainability benchmarking. Addressing limitations in traditional ESG ratings, our findings contribute to advancing ESG measurement methodologies and laying the foundation for dynamic firm‐specific ESG sentiment modeling.

---

Artificial Intelligence-Based <scp>ESG</scp> Greenwashing Detection: Road to Net Zero Carbon and Its Impact on Corporate Performance
10.1002/bsd2.70228
ABSTRACT To respond to public criticism on environmental issues, some businesses have significantly improved their environmental performance. Others, on the other hand, have responded symbolically by making little to no changes or by performing greenwashing. While much research has examined greenwashing, AI‐based techniques for identifying it have received less attention. The aim of this study is to validate the robustness of our AI‐based greenwashing detection (AI‐GW). In this study, our proposed AI‐GW model is cross‐tested with the existing ESG datasets from trusted and reputable financial and market data providers, namely Thomson Reuters and Bloomberg. Further, we examine the impact of greenwashing on corporate performance. To test the hypotheses, we use panel data gathered from all the Indonesian companies that have provided full ESG disclosures from 2017 to 2022. This study finds no difference between greenwashing scores based on our AI‐GW and the database. This study also finds a positive correlation between AI‐GW and greenwashing scores from a database. Further, the findings show that greenwashing consistently has a negative significant effect on financial performance when using our AI‐GW‐derived scores and the database‐derived data. The results of this study provide support for the validity of the AI‐based greenwashing detection method we developed.

---

How can organizations measure the integration of environmental, social, and governance (ESG) criteria? Validation of an instrument using item response theory to capture workers' perception
10.1002/bse.3675
Evaluation of corporate sustainability parameters has become part of organizational management. Item Response Theory (IRT) serves as a valuable approach for quantifying and exploring observable variables associated with these parameters. Therefore, this article aims to propose, apply, and validate an instrument utilizing IRT to measure environmental, social, and governance (ESG) impacts on corporate sustainability performance, using the perspective of workers. Workers were chosen as key stakeholders for the present study, as they were pointed by previous research as central to corporate sustainability decisions. The present paper addresses a literature gap by introducing, implementing, and validating a research instrument, along with a comparative scale, to assess the effects of integrating ESG criteria on corporate sustainability performance. Workers of two major Brazilian electricity sector companies were respondents for this instrument. Multigroup IRT method was used to assess item discrimination and difficulty levels, and to construct a scale reflecting workers' perceptions of integrating ESG criteria. The questionnaire consisted of 15 items, demonstrating Content Validity Coefficients (CVC) above .97, confirming the instrument's content effectiveness. Cronbach's Alpha and McDonald's Omega were greater than .94 for both companies. Questionnaire items demonstrated satisfactory discrimination ability and varying difficulty levels, resulting in a five‐level scale to gauge the impacts of ESG criteria. This research contributes through the introduction and validation of the research instrument, the latent trait assessment scale, and the evaluation of workers' perceptions regarding ESG criteria impacts on corporate sustainability performance. In conclusion, the proposed instrument showed robust psychometric properties, displaying favorable reliability and validity parameters. It effectively measures the impacts of ESG criteria, providing interpretable levels aligned with the unique characteristics of its constituent items

---

Measuring sustainable transformation of small and medium-sized enterprises using management systems standards
10.1002/bse.3995
Small‐ and medium‐sized enterprises (SMEs) represent a significant number of industries in need of moving towards more sustainable development. A sustainability perspective on business activities is today crucial for SMEs to achieve a new level of competitiveness. Identifying which is a suitable set of sustainability indicators for an individual SME is a big challenge. The availability of so many methods and systems can leave SMEs reluctant to implement them in their small business. Therefore, this research aims to create a sustainability maturity model for SMEs to visualize goals and assess progress on their transformation journey. Progress is assessed by indicators, exemplified by indicators of the Global Reporting Initiative and the maturity is verified as stepwise growth along international management system standards. Using the developed model, it is possible to determine the SMEs' sustainability performance. The research has resulted in a stepwise classification of the organization's maturity is compiled into training and auditing material in different regions and sectors.

---

Disclosure Dynamism: TCFD Aligned Climate Claims of UK Corporates
10.1002/bse.4189
ABSTRACT This research examines how climate claims by companies from the United Kingdom have changed over the years, especially when they became certain about the mandate of the Taskforce on Climate‐related Financial Disclosure (TCFD). We use text data from FTSE 100 companies for eight consecutive years, starting from 2016, and apply the robust ClimateBERT algorithm to analyse company statements related to climate claims, where they claim how they take care of climate in their business operations. Our findings show that the total number of corporate climate claims made has substantially increased since 2016, resulting in an overall improvement in corporate environmental claims till 2023. This coincides with the official announcement of the TCFD mandate.Our analyses also indicate that the proportion of claims in each report has increased over the years despite economic uncertainties. Additionally, the study findings reveal that even industries with minimal or negligible climate claims can still be associated with carbon‐intensive activities. The complementary features of the legitimacy and stakeholder theories support our findings. By applying ClimateBERT, our research mitigates existing data challenges, yielding an efficient framework for analysing text through a robust natural language processing model. Our findings will assist policymakers in identifying necessary modifications to corporate climate disclosure and will help assess the impact of the Taskforce intervention on climate‐related financial disclosure.

---

Environmental reporting: transparency to stakeholders or stakeholders’ manipulation? An analysis of disclosure tone and the role of board of directors
10.1002/csr.1350
We study whether environmental reporting serves as a transparency tool to communicate sound environmental policies to stakeholders or rather as a manipulation tool of stakeholders' perceptions. In particular, we focus on the relationship between environmental disclosure tone and future environmental performance and we furthermore explore the role of the board of directors' monitoring and stakeholder orientation in shaping this relationship. Using a sample of 288 US oil and gas firms, we find that the bias towards positive language does not reflect purely opportunistic managerial reasons, but rather is a transparency tool to signal future environmental performance. In addition, we document that the stakeholder orientation of the board plays a transparency role in communicating the firm's superior performance. Our findings contribute to the debate on whether discretionary strategies in environmental reporting are more about increased transparency or about stakeholder manipulation. Moreover, they help investors and policymakers to interpret managers' language choices. Copyright © 2014 John Wiley & Sons, Ltd and ERP Environment.

---

Corporate social responsibility and sustainability issues of small- and medium-sized enterprises
10.1002/csr.2083
Corporate social responsibility (CSR) presents a valuable tool of a better management of enterprises for the benefit of the whole society. Small‐ and medium‐sized enterprises (SMEs) have been facing this challenge for a shorter time than large enterprises. The aim of the article is to evaluate the impact of the CSR indicators on the sustainability of SMEs in Central European (CE) countries. Factors of the sustainability of SMEs are innovative ways to win new markets and retain existing customers, the innovation of our products and services, and lower probability of SMEs' bankruptcy. Linear regression analysis was applied to verify causal relationships. The sample size was constructed of 1,585 SMEs on basis of the questionnaire's answers. The results show that the knowledge of the CSR concept and its assertion in business is the most important CSR indicator with a positive impact on each factor of SMEs' sustainability.

---

A need for assurance: Do internal control systems integrate environmental, social, and governance factors?
10.1002/csr.2361
In the article, we provide an original linkage between the corporate environmental, social, and governance (ESG) rating and the cost of internal control system (ICSC) stemmed from two closely related frameworks: the 2017 CoSO Framework, which calls to strengthen internal control systems to integrate ESG issues, and the EU directive on nonfinancial reporting (2014/95/EU) that entered into force in 2017. Thus, we evaluate both introductions showing ESG integration in the internal control activities. We cover firms listed on Milan Exchange from 2016 to 2019, providing a thorough analysis with robustness tests. The findings imply that firms should consider both ESG rating and the internal control system cost as strategic corporate tools for value enhancement; therefore, companies should allocate the resources appropriately to internal control activities to incorporate ESG issues and create value since internal control provides the first assurance for ESG integration. The limitations of this study pave the way for further research directions; incorporating the new amendment of the EU directive on nonfinancial disclosure, allowing for a better valuation creation assessment; and whether there is a substitution between sustainability performance and other corporate issues such as taxes and marketing expenditure.

---

Role of environmental, social, and governance rating data in predicting financial risk and risk management
10.1002/csr.2567
Traditional industrial enterprises are primarily high pollution enterprises and high energy consumption. The achievement of the “dual carbon” motive in Chinese economic activities and environmental protection plays a chief role incorporating the environmental, social, and governance (ESG) into the risk management system of industrial enterprises. However, the industrial enterprises participating in ESG rating account for 46.97% of the listed enterprises, and the industrial enterprises that pay attention to ESG information disclosure account for 35.77% of the listed enterprises. Is ESG rating essential for the enhancement of industrial enterprises? Based on the transformation and upgrading of industrial enterprises, this research investigates how the ESG rating and ESG information disclosure affect the financial risk of industrial enterprises in China. The findings suggest that: (1) In China for industrial enterprise ESG plays an essential role which results in decrease in financial problems of industrial enterprise. (2) ESG is influential on monetary risk of industrial enterprise, the quality of enterprise ESG information disclosure plays an important intermediary role. (3) The lagging of ESG rating by one period is significantly positive related to the financial risk of industrial enterprises, and the financial risk prediction of industrial enterprises can be effectively indicated by the ESG rating data, which exhibits a stronger correlation coefficient than the existing rating system.

---

How do firms react to capital market liberalization? Evidence from ESG reporting greenwashing
10.1002/csr.2808
Based on catering theory, this study employs a difference‐in‐differences model to investigate the impact of capital market liberalization on environmental, social, and governance (ESG) reporting greenwashing using mainland Hong Kong Connection Programs as a quasinatural experiment. Our findings indicate that corporate management increasingly engages in ESG reporting greenwashing in capital market liberalization to cater to foreign investors. This conclusion remains valid after controlling for endogeneity. Mechanistic analysis indicates that investor sentiment increases with capital market liberalization. In response, corporate management intensifies ESG reporting greenwashing to cater to foreign investors, and analysts' focus does not serve as a supervisory mechanism for restraining ESG reports greenwashing. Heterogeneity tests demonstrate that ESG reports greenwashing within the capital market diminishes for mandatory disclosure and state‐owned firms. However, ESG reports greenwashing intensifies among firms facing high financing constraints amid capital market liberalization. Overall, we identify a significant catering effect on ESG reporting amid capital market liberalization, offering a novel theoretical framework on greenwashing in ESG disclosures.

---

Pro-environmental action plan for managers of distributed service facilities
10.1002/csr.2988
Given the variety of ways in which companies can communicate their pro‐ecological stances, it may be challenging to differentiate cursory Environmental, Social, and Corporate Governance (ESG) reporting from meaningful reporting, which is quantitatively supported and adequately monitored. Industry‐wide, a gap has emerged between corporate commitment to decarbonisation and tangible actions. For the most part, businesses continue to fall back on non‐verified data provided by a company that has not been externally audited. This study follows a qualitative research initiative focused on enhancing the questionable maturity of reporting in a reference business entity while simultaneously generating scientific knowledge. Through a series of documented interventions, we devise an environmental strategy action plan that complements highly abstract ESG strategies, with a depiction outlining specific measures for achieving key environmental performance indicators. Energy managers are provided with a validated structure for such a document, containing guidance on what information to include to effectively implement environmental statements in the overall sustainability strategies of their companies. We outline various avenues for companies, legislators, regulators, and organisations working on sustainability reporting frameworks to enhance reporting quality.

---

Environmental, social, and governance evaluation for European small and medium enterprises: A multicriteria approach
10.1002/csr.3018
The exposure to environmental, social, and governance (ESG) risks can be effectively measured by companies to identify opportunities for long‐term sustainable growth, along with the social and environmental impact. This process is crucial for listed small and medium‐sized enterprises (SMEs) wanting additional support in their ESG transition, and for European SMEs it will be required by the implementation of the Corporate Sustainability Reporting Directive (CSRD), starting from 2026. In this contribution, we propose to apply a multicriteria decision aiding approach to assess the sustainability profiles of SMEs. The methodology, which allows the measurements of a firm's ESG efforts (ESGness), is applied to a sample of European‐listed SMEs, controlling for potential sector‐specific effects, in order to understand what is the situation on the ESG front, and to identify ESG leaders and laggards. The model can provide valuable information for the firm, and for a broad spectrum of stakeholders, including policymakers and investors. The obtained rankings show some degree of robustness across different model parameterizations. The benefits of voluntary disclosure of sustainability information are investigated under a prudential scoring framework.

---

Environmental Sustainability Award Winners: Do They Communicate Their Environmental Performance Without Potential Greenwashing?
10.1002/csr.3088
ABSTRACT Sustainability awards are often seen as a mark of credibility and can help companies attract new customers, investors and partners. However, there is some question as to whether the companies that win sustainability awards—and therefore who ought to be genuinely committed to sustainability—correctly communicate their environmental performances according to internationally recognised principles for the fair use of environmental labels and claims such as those set by ISO standards. This study examined the web communication practices of a sample of 100 Italian companies that had won a sustainability award. Our findings showed that, while most of these companies boasted their environmental performance in several ways, they did not always follow the above‐mentioned communication principles. This suggests that companies need further training and education on how to communicate their environmental performance correctly and in a substantiated manner, thus preventing the risk of greenwashing.

---

Investor Responses to <scp>ESG</scp> News Sentiment: Exploring Differential Effects and Industry Moderation
10.1002/csr.3163
ABSTRACT Environmental, social, and governance (ESG) principles have gained prominence in the capital markets. While ESG ratings are widely used to assess corporate sustainability, their disagreement and time lag limit their effectiveness for investors. This study proposes ESG news sentiment as an alternative measure of corporate ESG activities and investigates the investor responses to it. Using sustainability reports, we developed lexicons to classify news into ESG and non‐ESG categories and further categorized ESG news into environmental, social, and governance dimensions. Through hierarchical regression analysis, we examine the impact of media‐derived ESG factors on market value and stock returns. Our findings reveal that ESG news sentiment is more positively associated with firm performance compared to non‐ESG news. This is particularly evident for social and governance dimensions, with effects varying across industries depending on their ESG concerns. These results demonstrate the value of ESG news sentiment analysis in understanding market responses to corporate sustainability practices.

---

Transforming <scp>ESG</scp> Analytics With Machine Learning: A Systematic Literature Review Using <scp>TCCM</scp> Framework
10.1002/csr.70089
ABSTRACT The integration of machine learning into environmental, social, and governance (ESG) research has gained significant traction, offering advanced predictive capabilities, enhanced decision‐making, and improved transparency in sustainable finance. Despite a growing body of literature, no systematic effort has been made to comprehensively map the intersection of ESG and machine learning. This study conducts a systematic literature review, synthesizing insights from 57 high‐quality articles published between 2004 and 2024 on the Web of Science and Scopus. Employing bibliometric analysis, thematic analysis, and the TCCM framework, this study identifies dominant research themes, methodological advancements, and persistent gaps in ESG‐machine learning literature. Findings reveal the increasing application of machine learning in ESG score prediction, risk assessment, investment decision‐making, and controversy detection while highlighting key challenges such as data standardization, model explainability, and algorithmic bias. Random Forest consistently demonstrated the highest prediction accuracy across several studies. This study offers valuable implications for academics, practitioners, and policymakers, providing a structured foundation for advancing ESG analytics and fostering responsible artificial intelligence‐driven sustainability strategies.

---

Greenwashing in Brazilian Corporations: A Machine Learning Approach to Unmask the Discourse-Practice Gap
10.1002/csr.70133
ABSTRACT This study investigates greenwashing in the Brazilian corporate sector by analysing the gap between companies' sustainability claims and their actual practices. We introduce a novel, AI‐powered methodology based on Decoupling Theory to detect greenwashing. Our approach triangulates three distinct data sources from 2022 to 2023: (i) internal corporate discourse (ESG reports quantified via machine learning); (ii) external market perception (Merco ESG reputation index); and (iii) actual environmental practice (objective greenhouse gas (GHG) emissions data). Our econometric analysis reveals a significant discourse‐practice gap, providing strong evidence of greenwashing. We found a substantial misalignment between companies' AI‐calculated ESG scores and their external reputation rankings, with only a small fraction of firms showing consistency. More strikingly, the analysis uncovered a paradoxical positive correlation between environmental discourse and emissions: a 1% increase in a company's AI‐calculated environmental score is associated with an approximate 5.24% increase in its GHG emissions. This suggests that environmental disclosures may serve as a tool for image management rather than reflecting tangible emission reductions. These findings empirically validate key facets of greenwashing theory within an emerging market context. The results underscore the urgent need for more robust regulation, mandatory standardized reporting, and independent verification to mitigate deceptive environmental claims and foster genuine corporate sustainability.

---

Small but Impactful: The Growing Role of SMES in Sustainability and ESG Reporting
10.1002/csr.70199
ABSTRACT Integrating environmental and social practices into small and medium‐sized enterprises (SMEs) remains challenging, particularly, due to limited corporate social responsibility (CSR) implementation and the lack of effective environmental management tools. This study explores the drivers behind sustainability adoption and environmental, social and governance (ESG) disclosure among 196 Italian manufacturing SMEs in the northern region. Using a mixed‐methods approach and PLS‐SEM analysis, the research identifies the main factors influencing SMEs' alignment with the sustainable development goals (SDGs). Results show that corporate beliefs are more influential in guiding sustainability strategies than leaders' personal backgrounds or firm characteristics. Additionally, the availability of sustainability accounting tools and the impact of the COVID‐19 pandemic have accelerated sustainability transitions. The study provides early but significant insights into how SMEs are approaching responsible practices and offers guidance for policymakers and institutions to support this shift. It highlights the urgent need for targeted frameworks that help SMEs fully integrate social and environmental considerations, thus contributing meaningfully to the global SDGs agenda.

---

Corporate Sustainability Transition: Methodological Analysis for a Rating Model
10.1002/csr.70350
ABSTRACT This study introduces a new rating model for the evaluation of corporate sustainability, addressing the inconsistencies and divergences that characterize current ESG assessment systems. The model is hierarchically structured, comprising 99 indicators organized into 19 modules, and is designed to be adaptable by sector and firm size. It provides a standardized yet flexible analytical framework that enhances transparency, comparability, and replicability of sustainability assessments. The framework is aligned with key European regulatory initiatives, including the EU Taxonomy and the CSRD Directive, and integrates both quantitative and qualitative dimensions. The indicators were developed through a comparative analysis of leading ESG frameworks (Bloomberg, Refinitiv, Moody's) and European regulatory sources and were refined through a modular structure that groups them into submodules, modules, and pillars. A dual weighting algorithm, sectoral and dimensional, was applied to ensure comparability across industries and firms of different sizes. The methodological process follows progressive steps: data collection at the company level, calculation of ESG metrics, aggregation into indicators, and consolidation into the three ESG pillars. Through conceptual validation and comparison with existing rating models, the framework demonstrates methodological coherence and practical potential. In particular, it addresses the issue of so‐called “aggregate confusion” in ESG ratings by reducing measurement divergence and promoting greater alignment with shared standards. The proposed approach supports the voluntary adoption of sustainable practices and facilitates improved management of environmental and social risks. Its main contribution lies in providing a transparent and adaptable tool that combines standardization with the flexibility required by diverse corporate contexts. The study delivers both theoretical and practical value, offering a structured basis for advancing ESG performance evaluation and harmonization.

---

The Disclosure Fog: Institutional Investors and Corporate Greenwashing
10.1002/ijfe.3096
Corporate environmental, social, and governance (ESG) disclosure is often a superficial signal rather than something of substance and is an easily negligible form of greenwashing. Here, we explore the relationship between corporate disclosure greenwashing and institutional investors using data from Chinese listed heavily polluting firms from 2012 to 2021. We hypothesise and discover that while institutional investors encourage firms to publish social responsibility reports, they may actually discourage the use of sustainability‐related phrases in these reports. The findings hold even after performing several endogeneity and robustness tests. We also identify three mechanisms through which institutional investors influence corporate greenwashing: corporate governance, information transparency, and corporate operations. Additionally, pressure‐sensitive institutional investors are more likely to facilitate corporate greenwashing than pressure‐insensitive and pressure‐uncertain investors. However, after the Guidance on Building Green Financial System's implementation in 2016, the positive relationship between disclosure greenwashing and institutional investors has rapidly declined. The promotional effect of institutional investor‐induced greenwashing is also lower in state‐owned enterprises. Overall, our findings not only enrich the literature on institutional investors and corporate information disclosure but also have implications for improving the green financial system.

---

STAGER checklist: Standardized testing and assessment guidelines for evaluating generative artificial intelligence reliability
10.1002/imo2.7
Generative artificial intelligence (AI) holds immense potential for medical applications, but the lack of a comprehensive evaluation framework and methodological deficiencies in existing studies hinder its effective implementation. Standardized assessment guidelines are crucial for ensuring reliable and consistent evaluation of generative AI in healthcare. Our objective is to develop robust, standardized guidelines tailored for evaluating generative AI performance in medical contexts. Through a rigorous literature review utilizing the Web of Sciences, Cochrane Library, PubMed, and Google Scholar, we focused on research testing generative AI capabilities in medicine. Our multidisciplinary team of experts conducted discussion sessions to develop a comprehensive 32‐item checklist. This checklist encompasses critical evaluation aspects of generative AI in medical applications, addressing key dimensions such as question collection, querying methodologies, and assessment techniques. The checklist and its broader assessment framework provide a holistic evaluation of AI systems, delineating a clear pathway from question gathering to result assessment. It guides researchers through potential challenges and pitfalls, enhancing research quality and reporting and aiding the evolution of generative AI in medicine and life sciences. Our framework furnishes a standardized, systematic approach for testing generative AI's applicability in medicine. For a concise checklist, please refer to Table S or visit GenAIMed.org.

---

Corporate governance performance ratings with machine learning
10.1002/isaf.1505
We use machine learning with a cross-sectional research design to predict governance controversies and to develop a measure of the governance component of the environmental, social, governance (ESG) metrics. Based on comprehensive governance data from 2,517 companies over a period of 10 years and investigating nine machine-learning algorithms, we find that governance controversies can be predicted with high predictive performance. Our proposed governance rating methodology has two unique advantages compared with traditional ESG ratings: it rates companies' compliance with governance responsibilities and it has predictive validity. Our study demonstrates a solution to what is likely the greatest challenge for the finance industry today: how to assess a company's sustainability with validity and accuracy. Prior to this study, the ESG rating industry and the literature have not provided evidence that widely adopted governance ratings are valid. This study describes the only methodology for developing governance performance ratings based on companies' compliance with governance responsibilities and for which there is evidence of predictive validity.

---

Open-Source Data-Driven Prediction of Environmental, Social, and Governance (ESG) Ratings Using Deep Learning Techniques
10.1002/isaf.70003
ABSTRACT The evaluation of ESG ratings by ESG rating agencies is time‐consuming and requires the participation of numerous human specialists. In this paper, we propose a method for creating proxies of ESG scores by collecting corporate ESG news and publicly available ESG‐related data using data crawling techniques and deep learning‐based classification technology while minimizing human involvement. To validate the effectiveness of the proposed approach, we suggest three hypotheses. Two of them are related to the connection between open‐source information and ESG ratings, while one concerns the link between proxy ESG rating and firm performance. To validate the effectiveness of the proposed approach, we conduct an empirical analysis based on 976 unique companies listed by the Korean Corporate Governance Agency (KCGS) from 2016 to 2019. Initially, we gather ESG indicators from open sources including disclosures and firms' news articles from a news portal site. We utilize Bidirectional Encoder Representations from Transformers (BERT) to classify news articles into environment, social, and governance categories and determine their sentiments. We confirm that ESG news sentiment and variables extracted from open‐source data are related to ESG ratings. Furthermore, we find a significantly positive relationship between E, S, and G ratings predicted based on open‐source data and Tobin's Q.

---

STUMPING E-RATER: CHALLENGING THE VALIDITY OF AUTOMATED ESSAY SCORING
10.1002/j.2333-8504.2001.tb01845.x
For this study, various writing experts were invited to “challenge” e-rater – an automated essay scorer that relies on natural language processing techniques – by composing essays in response to Graduate Record Examinations (GRE®) Writing Assessment prompts with the intention of undermining its scoring capability. Specifically, using detailed information about e-rater's approach to essay scoring, writers tried to “trick” the computer-based system into assigning scores that were higher or lower than deserved. E-rater's automated scores on these “problem essays” were compared with scores given by two trained, human readers, and the difference between the scores constituted the standard for judging the extent to which e-rater was fooled. Challengers were differentially successful in writing problematic essays. Expert writers were more successful in tricking e-rater into assigning scores that were too high than in duping e-rater into awarding scores that were too low. The study provides information on ways in which e-rater, and perhaps other automated essay scoring systems, may fail to provide accurate evaluations, if used as the sole method of scoring in high-stakes assessments. The results suggest possible avenues for improving automated scoring methods.

---

Performance of two large language models for data extraction in evidence synthesis.
10.1002/jrsm.1732
Accurate data extraction is a key component of evidence synthesis and critical to valid results. The advent of publicly available large language models (LLMs) has generated interest in these tools for evidence synthesis and created uncertainty about the choice of LLM. We compare the performance of two widely available LLMs (Claude 2 and GPT‐4) for extracting pre‐specified data elements from 10 published articles included in a previously completed systematic review. We use prompts and full study PDFs to compare the outputs from the browser versions of Claude 2 and GPT‐4. GPT‐4 required use of a third‐party plugin to upload and parse PDFs. Accuracy was high for Claude 2 (96.3%). The accuracy of GPT‐4 with the plug‐in was lower (68.8%); however, most of the errors were due to the plug‐in. Both LLMs correctly recognized when prespecified data elements were missing from the source PDF and generated correct information for data elements that were not reported explicitly in the articles. A secondary analysis demonstrated that, when provided selected text from the PDFs, Claude 2 and GPT‐4 accurately extracted 98.7% and 100% of the data elements, respectively. Limitations include the narrow scope of the study PDFs used, that prompt development was completed using only Claude 2, and that we cannot guarantee the open‐source articles were not used to train the LLMs. This study highlights the potential for LLMs to revolutionize data extraction but underscores the importance of accurate PDF parsing. For now, it remains essential for a human investigator to validate LLM extractions.

---

Enhancing environmental, social, and governance, performance and reporting through integration of life cycle sustainability assessment framework
10.1002/sd.3265
We introduce an innovative framework integrating Life Cycle Sustainability Assessment (LCSA) impact categories with Environmental, Social, and Governance (ESG) factors, offering a unified approach for ESG assessment and reporting. It covers sustainable development's key aspects, enabling a detailed evaluation of environmental, economic, and social performance across product and system life cycles, in line with the Sustainable Development Goals (SDGs). Incorporating the UN's 10 principles, the framework fosters a synergy to improve ESG reporting, adaptable across industries. To demonstrate its practicality, a theoretical application in Canada's oil and gas sector highlights how this framework can provide actionable insights for SDG‐aligned performance improvements. This example illustrates how the framework can identify and address sustainability issues, thereby improving ESG performance. Beyond its theoretical contributions, the framework serves as a valuable tool for practitioners and investors, promoting informed and comprehensive ESG reporting. Ultimately, it aims to enhance organizations' contributions towards achieving the SDGs and advancing global sustainability.

---

How Media Coverage of Corporate Social Irresponsibility Increases Financial Risk
10.1002/smj.2647
Research summary: This article explores the relationship between corporate social irresponsibility (CSI) and financial risk. We posit that media coverage of CSI generates risk by providing conditions that increase the potential for stakeholder sanctions. Through analyzing an international panel of 539 firms during 2008–2013, we find that firms receiving higher CSI coverage face higher financial risk. We show that the reach of the reporting media outlet is a critical condition for this relationship. Once the outlet has a high reach, the severity of CSI coverage is a boundary condition that further reinforces the effect. Our findings complement existing theory about the risk-mitigating effect of corporate social responsibility by illuminating the risk-generating effect of CSI coverage. For executives, these insights suggest complementary strategies for corporate risk management. 
 
Managerial summary: This article examines the effect of negative news on financial risk. It shows that negative media articles regarding environmental, social, and governance (ESG) issues increase a firm's credit risk. It also provides a detailed analysis of the impact of an article's reach and severity, i.e., how many readers are exposed to the article and how harshly it criticizes the firm. The results allow to quantitatively assess the risk that emanates from negative ESG news. For executives, three strategies are derived for limiting a firm's exposure to this risk: balancing corporate social responsibility programs with operational safety programs, reporting suboptimal environmental and social performance transparently and proactively, and avoiding acquisition targets and markets with a legacy of negative news. Copyright © 2017 John Wiley & Sons, Ltd.

---

Automated Essay Scoring in a High Stakes Testing Environment
10.1007/978-1-4419-6530-1_10
This chapter discusses the use of automated essay scoring (AES) as a possible replacement for an annual statewide high-stakes writing test. The examples provided are drawn from development work in the state of Florida, but might apply to any state in the United States. In the first section, literature associated with the frequency, costs, and consequences of high-stakes testing is reviewed. In the second section, automated essay scoring is introduced and a description of how it works as an assessment tool is provided. In the third section, an example of how AES is used as an instructional tool is given and I argue for a tighter integration of assessment with instruction. Finally, I propose that AES actually replace the high-stakes testing program for accountability (and other) purposes, and provide a list of advantages for proceeding in this fashion.

---

Information Extraction from Text
10.1007/978-1-4614-3223-4_2
Information extraction is the task of finding structured information from unstructured or semi-structured text. It is an important task in text mining and has been extensively studied in various research communities including natural language processing, information retrieval and Web mining. It has a wide range of applications in domains such as biomedical literature mining and business intelligence. Two fundamental tasks of information extraction are named entity recognition and relation extraction. The former refers to finding names of entities such as people, organizations and locations. The latter refers to finding the semantic relations such as FounderOf and HeadquarteredIn between entities. In this chapter we provide a survey of the major work on named entity recognition and relation extraction in the past few decades, with a focus on work from the natural language processing community.

---

Administration of SAP HANA Cloud
10.1007/978-1-4842-8569-5_2
The SAP Business Technology Platform (SAP BTP) offers a wide range of services. Every SAP BTP service offers one or more service plans. A service plan is a subset of a service. The SAP HANA Cloud database service, for example, is available in a variety of configurations and sizes, as shown in Figure 2-1.

---

Explainability for Linear Supervised Models
10.1007/978-1-4842-9029-3_2
A supervised learning model is a model that is being used to train an algorithm to map the input data with the output data. A supervised learning model can be of two types: regression and classification. In a regression scenario, the output variable is numerical, and in the case of classification, the output variable is binary or multinomial. A binary output variable has two outcomes, such as true and false, accept and reject, yes and no, etc. In the case of multinomial output variables, the outcome can be more than two, such as high, medium, and low. In this chapter, we are going to use explainable libraries to explain a regression model and a classification model, while training a linear model.

---

Automated Machine Learning for Studying the Trade-Off Between Predictive Accuracy and Interpretability
10.1007/978-3-030-29726-8_4
Automated Machine Learning (Auto-ML) methods search for the best classification algorithm and its best hyper-parameter settings for each input dataset. Auto-ML methods normally maximize only predictive accuracy, ignoring the classification model’s interpretability – an important criterion in many applications. Hence, we propose a novel approach, based on Auto-ML, to investigate the trade-off between the predictive accuracy and the interpretability of classification-model representations. The experiments used the Auto-WEKA tool to investigate this trade-off. We distinguish between white box (interpretable) model representations and two other types of model representations: black box (non-interpretable) and grey box (partly interpretable). We consider as white box the models based on the following 6 interpretable knowledge representations: decision trees, If-Then classification rules, decision tables, Bayesian network classifiers, nearest neighbours and logistic regression. The experiments used 16 datasets and two runtime limits per Auto-WEKA run: 5 h and 20 h. Overall, the best white box model was more accurate than the best non-white box model in 4 of the 16 datasets in the 5-hour runs, and in 7 of the 16 datasets in the 20-hour runs. However, the predictive accuracy differences between the best white box and best non-white box models were often very small. If we accept a predictive accuracy loss of 1% in order to benefit from the interpretability of a white box model representation, we would prefer the best white box model in 8 of the 16 datasets in the 5-hour runs, and in 10 of the 16 datasets in the 20-hour runs.

---

Creating Investment-Grade Corporate Sustainability Metrics
10.1007/978-3-030-55613-6_4
Rising interest in sustainable investing has led to intensified scrutiny of environmental, social, and governance (ESG) reporting. But this heightened interest has convinced many investors and market analysts that the existing ESG metrics lack clear definitional foundations, methodological consistency, analytic rigor, and reliable verification, thus creating doubts about their true validity, comparability, and capacity to distinguish corporate sustainability leaders from laggards. This chapter surveys the gaps and shortcomings in today’s ESG data and calls for a mandatory corporate sustainability reporting structure backed by government standards, review, and enforcement. Such a framework ensure that sustainability-minded investors have access to investment-grade ESG metrics that inspire trust and confidence.

---

Automated Grading of Essays: A Review
10.1007/978-3-030-68449-5_25
Grading essay type of examinations is a kind of assessment where teachers have to evaluate descriptive answers written by students. Evaluating explanatory answers is a challenging task. Evaluating essay type questions are often laborious, requiring more time compared to multiple-choice questions. Also, the evaluation process is a subjective one leading to inaccuracies and considerable variation in grading. In recent times, many researchers have developed techniques for Automated Grading of Essays (AGE) using various machine learning methods. This paper reviews and summarizes the existing literature on methods for automated grading of essays. The paper reviews applications of approaches such as Natural Language Processing and Deep Learning for AGE. The paper identifies a few areas for further research in solving the problem of AGE.

---

On Interpretability and Similarity in Concept-Based Machine Learning
10.1007/978-3-030-72610-2_3
Machine Learning (ML) provides important techniques for classification and predictions. Most of these are black-box models for users and do not provide decision-makers with an explanation. For the sake of transparency or more validity of decisions, the need to develop explainable/interpretable ML-methods is gaining more and more importance. Certain questions need to be addressed:  How does an ML procedure derive the class for a particular entity? Why does a particular clustering emerge from a particular unsupervised ML procedure? What can we do if the number of attributes is very large? What are the possible reasons for the mistakes for concrete cases and models?  For binary attributes, Formal Concept Analysis (FCA) offers techniques in terms of intents of formal concepts, and thus provides plausible reasons for model prediction. However, from the interpretable machine learning viewpoint, we still need to provide decision-makers with the importance of individual attributes to the classification of a particular object, which may facilitate explanations by experts in various domains with high-cost errors like medicine or finance.  We discuss how notions from cooperative game theory can be used to assess the contribution of individual attributes in classification and clustering processes in concept-based machine learning. To address the 3rd question, we present some ideas on how to reduce the number of attributes using similarities in large contexts.

---

Evaluating Fidelity of Explainable Methods for Predictive Process Analytics
10.1007/978-3-030-79108-7_8
Predictive process analytics focuses on predicting the future states of running instances of a business process. While advanced machine learning techniques have been used to increase the accuracy of predictions, the resulting predictive models lack transparency. Explainable machine learning methods can be used to interpret black-box models. However, it is unclear how fit for purpose these methods are in explaining process predictive models. In this paper, we aim to investigate the capabilities of two explainable methods, LIME and SHAP, in reproducing the decision-making processes of black-box process predictive models. We focus on fidelity metrics and propose a method to evaluate the faithfulness of LIME and SHAP when explaining process predictive models built on a Gradient Boosting Machine classifier. We conduct the evaluation using three real-life event logs and analyze the fidelity evaluation results to derive insights. The research contributes to evaluating the trustworthiness of explainable methods for predictive process analytics as a fundamental and key step towards human user-oriented evaluation.

---

Carbonwashing: ESG Data Greenwashing in a Post-Paris World
10.1007/978-3-030-83650-4_3
Despite the increased attention and capital incentives around corporate sustainability, the development of sustainability reporting standards and monitoring systems has been progressing at a slow pace. As a result, companies have misaligned incentives to deliberately or selectively communicate information not matched with actual environmental impacts or make largely unsubstantiated promises around future ambitions. These incidents are broadly called “greenwashing,” but there is no clear consensus on its definition and taxonomy. We pay particular attention to the threat of greenwashing concerning carbon emission reductions by coining a new term, “carbonwashing.” Since carbon mitigation is the universal goal, the corporate carbon performance data supply chain is relatively more advanced than that of the entire sustainability data landscape. Nonetheless, the threat of carbonwashing persists, even far more severe than general greenwashing due to the financial values attached to corporate carbon performance. This chapter contextualizes sustainable finance-related carbonwashing via an outline of the communication as well as the measurement, reporting, and verification (MRV) of carbon emission mitigation performance.

---

Sigmalaw PBSA - A Deep Learning Model for Aspect-Based Sentiment Analysis for the Legal Domain
10.1007/978-3-030-86472-9_12
Legal information retrieval holds a significant importance to lawyers and legal professionals. Its significance has grown as a result of the vast and rapidly increasing amount of legal documents available via electronic means. Legal documents, which can be considered flat file databases, contain information that can be used in a variety of ways, including arguments, counter-arguments, justifications, and evidence. As a result, developing automated mechanisms for extracting important information from legal opinion texts can be regarded as an important step toward introducing artificial intelligence into the legal domain. Identifying advantageous or disadvantageous statements within these texts in relation to legal parties can be considered as a critical and time consuming task. This task is further complicated by the relevance of context in automatic legal information extraction. In this paper, we introduce a solution to predict sentiment value of sentences in legal documents in relation to its legal parties. The Proposed approach employs a fine-grained sentiment analysis (Aspect-Based Sentiment Analysis) technique to achieve this task. Sigmalaw PBSA is a novel deep learning-based model for ABSA which is specifically designed for legal opinion texts. We evaluate the Sigmalaw PBSA model and existing ABSA models on the SigmaLaw-ABSA dataset which consists of 2000 legal opinion texts fetched from a public online data base. Experiments show that our model outperforms the state-of-the-art models. We also conduct an ablation study to identify which methods are most effective for legal texts.

---

Lexicon-based Methods vs. BERT for Text Sentiment Analysis
10.1007/978-3-031-16500-9_7
The performance of sentiment analysis methods has greatly increased in recent years. This is due to the use of various models based on the Transformer architecture, in particular BERT. However, deep neural network models are difficult to train and poorly interpretable. An alternative approach is rule-based methods using sentiment lexicons. They are fast, require no training, and are well interpreted. But recently, due to the widespread use of deep learning, lexicon-based methods have receded into the background. The purpose of the article is to study the performance of the SO-CAL and SentiStrength lexicon-based methods, adapted for the Russian language. We have tested these methods, as well as the RuBERT neural network model, on 16 text corpora and have analyzed their results. RuBERT outperforms both lexicon-based methods on average, but SO-CAL surpasses RuBERT for four corpora out of 16.

---

Explainable Artificial Intelligence (XAI): Conception, Visualization and Assessment Approaches Towards Amenable XAI
10.1007/978-3-031-18292-1_3
AbstractFor last decade, due to the accessibility of huge databases and recent advancements in deep learning methodology, machine learning systems have arrived at or transcended tremendous performance in a spacious variety of tasks. One can see this speedy development in speech analysis, image recognition, sentiment analysis, strategic game planning and many more, for e.g. in medical field, it’s used for diagnosing different diseases, like breast cancer etc., based on their symptoms. But many state-of- the-art models is facing lack of transparency and interpretability which is a major hindrance in many applications, e.g. finance and healthcare where visualization, interpretation and explanation for model's decision is an obligation for trust. This is an implicit problem of the current techniques carried by sub-symbolism (e.g. Deep Neural Networks) that were not shown in the last hype of AI (specifically, rule based models and expert systems). Models underlying this problem come within the so-called Explainable AI (XAI) field, which is extensively acknowledged as a racial feature for the practical deployment of AI models. As a result, explainable artificial intelligence (XAI) has turned into scientific interest in last recent years. So, this chapter epitomizes contemporary developments in Explainable AI that describes explainability in Machine Learning, constituting a fiction definition of explainable Machine Learning that envelopes such prior conceptual propositions with a considerable focus on the audience for which the explainability is needed. Except of this definition, this chapter starts a confabulation on its various techniques that are essentials for analysing interpretability and explainability of Artificial Intelligence, and also gives a comparison between two medical experiments, that are based on predicting heart disease using disparate Explainable Artificial Intelligence techniques, which can give a lead for researchers as well as practitioners or newcomers in the field of Artificial Intelligence for selecting suitable methods with Explainable AI to grasp the advances of AI in their action sectors, without any previous bias for its dearth of interpretability.KeywordsExplainable artificial intelligenceMachine learningInterpretabilityExplainabilityBlack box modelPost-Hoc method

---

AI for Sustainable Finance: Governance Mechanisms for Institutional and Societal Approaches
10.1007/978-3-031-21147-8_12
Artificial intelligence (AI) for sustainable finance has been increasingly employed over the past several years to address the sustainable development goals (SDGs). Two major approaches have emerged: institutional and societal AI for sustainable finance. Broadly described, institutional AI for sustainable finance is used for activities such as environmental, social and governance (ESG) investing, while societal AI for sustainable finance is used to support underbanked and unbanked individuals through financial inclusion initiatives. Despite the growing reliance on such digital tools, particularly during the coronavirus disease 2019 (COVID-19) pandemic, governance mechanisms and regulatory frameworks remain fragmented and underutilized or inhibit progress toward the 17 UN SDGs. While major proposals and reports were released by standard-setting and regulatory bodies leading up to 2020, the COVID-19 pandemic indeed caused major setbacks to adoption and implementation, which in turn have also resulted in inconclusive data and lessons learned. As the global community begins to navigate out of the pandemic, policy makers, through multilateral and cross-sector agreements, are looking to renew governance mechanisms that mitigate new and pre-existing risks while cultivating sustainability and facilitating innovation.

---

Ontology-Based Automatic Reasoning and NLP for Tracing Software Requirements into Models with the OntoTrace Tool
10.1007/978-3-031-29786-1_10
Context and motivation. Traceability is an essential part of quality assurance tasks for software maintainability, validation, and verification. However, the effort required to create and maintain traces is still high compared to their benefits. Problem. Some authors have proposed traceability tools to address this challenge, yet some of those tools require historical traceability data to generate traces, representing an entry barrier to software development teams that do not do traceability. Another common requirement of existing traceability tools is the scope of artefacts to be traced, hindering the adaptability of traceability tools in practice. Principal ideas. Motivated by the mentioned challenges, in this paper we propose OntoTraceV2.0: a tool for supporting trace generation of arbitrary software artefacts without depending on historical traceability data. The architecture of OntoTraceV2.0 integrates ontology-based automatic reasoning to facilitate adaptability for tracing arbitrary artefacts and natural language processing for discovering traces based on text-based similarity between artefacts. We conducted a quasi-experiment with 36 subjects to validate OntoTraceV2.0 in terms of efficiency, effectiveness, and satisfaction. Contribution. We found that OntoTraceV2.0 positively affects the subjects’ efficiency and satisfaction during trace generation compared to a manual approach. Although the subjects’ average effectiveness is higher using OntoTraceV2.0, we observe no statistical difference with the manual trace generation approach. Even though such results are promising, further replications are needed to avoid certain threats to validity. We conclude the paper by analysing the experimental results and limitations we found, drawing on future challenges, and proposing the next research endeavours.

---

Explaining the Unexplainable: Role of XAI for Flight Take-Off Time Delay Prediction
10.1007/978-3-031-34107-6_7
Flight Take-Off Time (TOT) delay prediction is essential to optimizing capacity-related tasks in Air Traffic Management (ATM) systems. Recently, the ATM domain has put afforded to predict TOT delays using machine learning (ML) algorithms, often seen as “black boxes”, therefore it is difficult for air traffic controllers (ATCOs) to understand how the algorithms have made this decision. Hence, the ATCOs are reluctant to trust the decisions or predictions provided by the algorithms. This research paper explores the use of explainable artificial intelligence (XAI) in explaining flight TOT delay to ATCOs predicted by ML-based predictive models. Here, three post hoc explanation methods are employed to explain the models’ predictions. Quantitative and user evaluations are conducted to assess the acceptability and usability of the XAI methods in explaining the predictions to ATCOs. The results show that the post hoc methods can successfully mimic the inference mechanism and explain the models’ individual predictions. The user evaluation reveals that user-centric explanation is more usable and preferred by ATCOs. These findings demonstrate the potential of XAI to improve the transparency and interpretability of ML models in the ATM domain.

---

IndQNER: Named Entity Recognition Benchmark Dataset from the Indonesian Translation of the Quran
10.1007/978-3-031-35320-8_12
Indonesian is classified as underrepresented in the Natural Language Processing (NLP) field, despite being the tenth most spoken language in the world with 198 million speakers. The paucity of datasets is recognized as the main reason for the slow advancements in NLP research for underrepresented languages. Significant attempts were made in 2020 to address this drawback for Indonesian. The Indonesian Natural Language Understanding (IndoNLU) benchmark was introduced alongside IndoBERT pre-trained language model. The second benchmark, Indonesian Language Evaluation Montage (IndoLEM), was presented in the same year. These benchmarks support several tasks, including Named Entity Recognition (NER). However, all NER datasets are in the public domain and do not contain domain-specific datasets. To alleviate this drawback, we introduce IndQNER, a manually annotated NER benchmark dataset in the religious domain that adheres to a meticulously designed annotation guideline. Since Indonesia has the world’s largest Muslim population, we build the dataset from the Indonesian translation of the Quran. The dataset includes 2475 named entities representing 18 different classes. To assess the annotation quality of IndQNER, we perform experiments with BiLSTM and CRF-based NER, as well as IndoBERT fine-tuning. The results reveal that the first model outperforms the second model achieving 0.98 F1 points. This outcome indicates that IndQNER may be an acceptable evaluation metric for Indonesian NER tasks in the aforementioned domain, widening the research’s domain range.

---

GreenScreen: A Multimodal Dataset for Detecting Corporate Greenwashing in the Wild
10.1007/978-3-031-56435-2_8
"Greenwashing, a form of deceptive marketing where organizations attempt to convince consumers that their offerings and operations are environmentally sound, can cause lasting damage to sustainability efforts by confusing consumers and eroding trust in genuine pro-sustainability actions. Nonetheless, capturing greenwashing ""in the wild"" remains challenging because greenwashed content frequently employs subliminal messaging through abstract semantic concepts that require subjective interpretation and contextualization within the context of the parent company's actual environmental performance. Moreover

---

Nano-ESG: Extracting Corporate Sustainability Information from News Articles
10.1007/978-3-031-88717-8_24
Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce. An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at https://github.com/Bailefan/Nano-ESG.

---

Active Learning Based Weak Supervision for Textual Survey Response Classification
10.1007/978-3-319-18117-2_23
Analysing textual responses to open-ended survey questions has been one of the challenging applications for NLP. Such unstructured text data is a rich data source of subjective opinions about a specific topic or entity; but it is not amenable to quick and comprehensive analysis. Survey coding is the process of categorizing such text responses using a pre-specified hierarchy of classes (often called a code-frame). In this paper, we identify the factors constraining the automation approaches to this problem and observe that a completely supervised learning approach is not feasible in practice. We then present details of our approach which uses multi-label text classification as a first step without requiring labeled training data. This is followed by the second step of active learning based verification of survey response categorization done in first step. This weak supervision using active learning helps us to optimize the human involvement as well as to adapt the process for different domains. Efficacy of our method is established using the high agreement with real-life, manually annotated benchmark data.

---

Software and Web-Based Tools for Sustainability Management in Micro-, Small- and Medium-Sized Enterprises
10.1007/978-3-319-23455-7_14
Recently, new approaches to organizational level sustainability management and reporting have emerged in the form of software and web-based applications. At first glance, it appears that such software and web-tools are applicable in small and medium-sized enterprises (SMEs), as they offer user-friendly and cost-effective alternatives to assess, manage and report on company-wide sustainability activities. Nevertheless, it remains academically and practically uncertain if such technological advancements will be adopted by a great number of SMEs. Using the Individual-Technology-Organization-Environment (ITOE) model as a theoretical framework and empirical data from a recent survey with 1,250 German SMEs, this paper investigates various firm-internal and external factors that might influence managers’ decisions to adopt or reject this new technology. This paper reveals which factors might play a role in the adoption of such web-tools in SMEs. In addition, this paper proposes a conceptual framework of an IT-assisted sustainability analysis and reporting scheme for micro-enterprises and startups. Based on existing software for larger enterprises, the paper describes the main content and potential layout of such a web-based tool.

---

Deep Learning in Automated Essay Scoring
10.1007/978-3-319-91464-0_30
This paper explores the application of deep learning in automated essay scoring (AES). It uses the essay dataset #8 from the Automated Student Assessment Prize competition, hosted by the Kaggle platform, and a state-of-the-art Suite of Automatic Linguistic Analysis Tools (SALAT) to extract 1,463 writing features. A non-linear regressor deep neural network is trained to predict holistic scores on a scale of 10–60. This study shows that deep learning holds the promise to improve significantly the accuracy of AES systems, but that the current dataset and most essay datasets fall short of providing them with enough expertise (hand-graded essays) to exploit that potential. After the tuning of different sets of hyperparameters, the results show that the levels of agreement, as measured by the quadratic weighted kappa metric, obtained on the training, validation, and testing sets are 0.84, 0.63, and 0.58, respectively, while an ensemble (bagging) produced a kappa value of 0.80 on the testing set. Finally, this paper upholds that more than 1,000 hand-graded essays per writing construct would be necessary to adequately train the predictive student models on automated essay scoring, provided that all score categories are equally or fairly represented in the sample dataset.

---

A clustering-based approach to ontology alignment
10.1007/978-3-642-25073-6_10
Ontology alignment is an important problem for the linked data web, as more and more ontologies and ontology instances get published for specific domains such as government and healthcare. A number of (semi-)automated alignment systems have been proposed in recent years. Most combine a set of similarity functions on lexical, semantic and structural features to align ontologies. Although these functions work well in many cases of ontology alignments, they fail to capture alignments when terms or structure varies vastly across ontologies. In this case, one is forced to rely on manual alignment. In this paper, we study whether it is feasible to re-use such expert provided ontology alignments for new alignment tasks. We focus in particular on many-to-one alignments, where the opportunity for re-use is feasible if alignments are stable. Specifically, we define the notion of a cluster as being made of multiple entities in the source ontology S that are mapped to the same entity in the target ontology τ. We test the stability hypothesis that the formed clusters of source ontology are stable across alignments to different target ontologies. If this hypothesis is valid, the clusters of an ontology S, built from an existing alignment with an ontology τ, can be effectively exploited to align S with a new ontology τ′. Evaluation on both manual and automated high-quality alignments show remarkable stability of clusters across ontology alignments in the financial domain and the healthcare and life sciences domain. Experimental evaluation also demonstrates the effectiveness of utilizing the stability of clusters in improving the alignment process in terms of precision and recall.

---

Ontology-Driven Software Development
10.1007/978-3-642-31226-7
This book is about a significant step forward in software development. It brings state-of-the-art ontology reasoning into mainstream software development and its languages. Ontology Driven Software Development is the essential, comprehensive resource on enabling technologies, consistency checking and process guidance for ontology-driven software development (ODSD). It demonstrates how to apply ontology reasoning in the lifecycle of software development, using current and emerging standards and technologies. You will learn new methodologies and infrastructures, additionally illustrated using detailed industrial case studies. The book will help you: Learn how ontology reasoning allows validations of structure models and key tasks in behavior models. Understand how to develop ODSD guidance engines for important software development activities, such as requirement engineering, domain modeling and process refinement. Become familiar with semantic standards, such as the Web Ontology Language (OWL) and the SPARQL query language. Make use of ontology reasoning, querying and justification techniques to integrate software models and to offer guidance and traceability supports. This book is helpful for undergraduate students and professionals who are interested in studying how ontologies and related semantic reasoning can be applied to the software development process. In addition, itwill also be useful for postgraduate students, professionals and researchers who are going to embark on their research in areas related to ontology or software engineering.

---

Crowdsourcing Fact Extraction from Scientific Literature
10.1007/978-3-642-39146-0_15
Scientific publications constitute an extremely valuable body of knowledge and can be seen as the roots of our civilisation. However, with the exponential growth of written publications, comparing facts and findings between different research groups and communities becomes nearly impossible. In this paper, we present a conceptual approach and a first implementation for creating an open knowledge base of scientific knowledge mined from research publications. This requires to extract facts - mostly empirical observations - from unstructured texts (mainly PDF’s). Due to the importance of extracting facts with high-accuracy and the impreciseness of automatic methods, human quality control is of utmost importance. In order to establish such quality control mechanisms, we rely on intelligent visual interfaces and on establishing a toolset for crowdsourcing fact extraction, text mining and data integration tasks.

---

Aspect and Entity Extraction for Opinion Mining
10.1007/978-3-642-40837-3_1
Opinion mining or sentiment analysis is the computational study of people’s opinions, appraisals, attitudes, and emotions toward entities such as products, services, organizations, individuals, events, and their different aspects. It has been an active research area in natural language processing and Web mining in recent years. Researchers have studied opinion mining at the document, sentence and aspect levels. Aspect-level (called aspect-based opinion mining) is often desired in practical applications as it provides the detailed opinions or sentiments about different aspects of entities and entities themselves, which are usually required for action. Aspect extraction and entity extraction are thus two core tasks of aspect-based opinion mining. In this chapter, we provide a broad overview of the tasks and the current state-of-the-art extraction techniques.

---

Towards sustainable futures
10.1007/978-981-15-0874-5_1
Case studies are an important way for learners to develop professional skills. In this book, we present ten case studies in the area of business sustainability that address the United Nations’ Sustainable Development Goals. Each case study is accompanied by teaching notes and assessment questions as well as marking guides.

---

Categorical Data: Need, Encoding, Selection of Encoding Method and Its Emergence in Machine Learning Models—A Practical Review Study on Heart Disease Prediction Dataset Using Pearson Correlation
10.1007/978-981-19-6631-6_26
AbstractData can be defined as the joint collection of facts and statistics, which yields meaningful insights on proper analysis. In general, real-world data is a combination of both categorical and numerical data. Many Machines learning algorithms do not support categorical data, therefore, to utilize the data efficiently categorical data should be converted into numerical data without any distortion in the data distribution. The process of transforming the categorical data into numerical data is called “categorical encoding”. Categorical encoding is one of the crucial steps in data preprocessing, as most of the machine learning models work better with numerical data. There are many types of categorical encoding techniques, each technique has trade-offs and has a notable influence on the outcome of the analysis so, choosing an optimal technique based on the situation is a challenging task. The main objective of this paper is to provide insights on choosing a technique that not only converts the categorical data into numerical data but also which helps in making the transformed data to become a much better representative of the target variable. In this paper, we analysed and implemented various encoding techniques on the heart disease prediction dataset and were attentive in selecting the best encoding technique which meets the main objectives of the paper. As per the best of our knowledge, this is the first paper that focuses completely on the analysis of basic categorical encoding techniques based on their correlation with the target variable.KeywordsEncodingCategorical dataOne hot encodingLabel encodingFrequency encodingOrdinal encodingMean encodingHeart disease predictionPearson correlation

---

Establishment of NLP-Based Greenwashing Pattern Detection Service
10.1007/978-981-99-1252-0_32
This study aims to establish a search service that provides greenwashing pattern detection and infographic of enterprises through AI-based NLP technology. The purpose of it is to provide decision indicators by checking data on greenwashing trends of companies by users, including investment institutions and general consumers. We used a deep learning model to adopted evaluation factors to score the ‘greenwashing’ and collected the title of media data with active monitoring of companies based on those factors. In addition, construct greenwashing sentence discrimination model and a corporate greenwashing scoring model using the BERT model. As a result, a service provided scores using data collected through its keywords through visualization and the process of calculation when users input the name of the company.

---

Automated formal specification generation and refinement from requirement documents
10.1007/bf03192554
The automatic generation of formal specifications from requirements suppresses the complexity of formal models manual creation and reveals the immediate benefits of its usage, such as the possibility to carry out refinements, and property verification, which contributes to project cost reduction and quality improvement. This paper proposes a Controlled Natural Language (CNL), a subset of English, used to write use case specifications according to a template. From these use cases a complete strategy and tools enable the generation of process algebraic formal models in the CSP notation. We define templates that represent requirements at different levels of abstraction, capturing different views of the system behavior. Moreover, a refinement notion is defined to connect the generated CSP models through an event mapping relation between abstract and concrete models. This notion is further applied to detail use case specifications and to automate its execution.

---

On ontology-driven document clustering using core semantic features
10.1007/s10115-010-0370-4
Incorporating semantic knowledge from an ontology into document clustering is an important but challenging problem. While numerous methods have been developed, the value of using such an ontology is still not clear. We show in this paper that an ontology can be used to greatly reduce the number of features needed to do document clustering. Our hypothesis is that polysemous and synonymous nouns are both relatively prevalent and fundamentally important for document cluster formation. We show that nouns can be efficiently identified in documents and that this alone provides improved clustering. We next show the importance of the polysemous and synonymous nouns in clustering and develop a unique approach that allows us to measure the information gain in disambiguating these nouns in an unsupervised learning setting. In so doing, we can identify a core subset of semantic features that represent a text corpus. Empirical results show that by using core semantic features for clustering, one can reduce the number of features by 90% or more and still produce clusters that capture the main themes in a text corpus.

---

Sentiment analysis using deep learning architectures: a review
10.1007/s10462-019-09794-5
Social media is a powerful source of communication among people to share their sentiments in the form of opinions and views about any topic or article, which results in an enormous amount of unstructured information. Business organizations need to process and study these sentiments to investigate data and to gain business insights. Hence, to analyze these sentiments, various machine learning, and natural language processing-based approaches have been used in the past. However, deep learning-based methods are becoming very popular due to their high performance in recent times. This paper provides a detailed survey of popular deep learning models that are increasingly applied in sentiment analysis. We present a taxonomy of sentiment analysis and discuss the implications of popular deep learning architectures. The key contributions of various researchers are highlighted with the prime focus on deep learning approaches. The crucial sentiment analysis tasks are presented, and multiple languages are identified on which sentiment analysis is done. The survey also summarizes the popular datasets, key features of the datasets, deep learning model applied on them, accuracy obtained from them, and the comparison of various deep learning models. The primary purpose of this survey is to highlight the power of deep learning architectures for solving sentiment analysis problems.

---

An automated essay scoring systems: a systematic literature review
10.1007/s10462-021-10068-2
Assessment in the Education system plays a significant role in judging student performance. The present evaluation system is through human assessment. As the number of teachers' student ratio is gradually increasing, the manual evaluation process becomes complicated. The drawback of manual evaluation is that it is time-consuming, lacks reliability, and many more. This connection online examination system evolved as an alternative tool for pen and paper-based methods. Present Computer-based evaluation system works only for multiple-choice questions, but there is no proper evaluation system for grading essays and short answers. Many researchers are working on automated essay grading and short answer scoring for the last few decades, but assessing an essay by considering all parameters like the relevance of the content to the prompt, development of ideas, Cohesion, and Coherence is a big challenge till now. Few researchers focused on Content-based evaluation, while many of them addressed style-based assessment. This paper provides a systematic literature review on automated essay scoring systems. We studied the Artificial Intelligence and Machine Learning techniques used to evaluate automatic essay scoring and analyzed the limitations of the current studies and research trends. We observed that the essay evaluation is not done based on the relevance of the content and coherence.The online version contains supplementary material available at 10.1007/s10462-021-10068-2.

---

KnowMIS-ABSA: an overview and a reference model for applications of sentiment analysis and aspect-based sentiment analysis
10.1007/s10462-021-10134-9
Abstract The analysis of the opinions of customers and users has been always of great interest in supporting decision-making in many fields, especially in marketing. Sentiment analysis (SA) is the umbrella term for techniques and approaches that analyze user’s sentiments, emotions, opinions in text or other media. The need for a better understanding of these opinions paved the way to novel approaches that focus on the analysis of the sentiment related to specific features of a product, giving birth to the field of aspect-based sentiment analysis (ABSA). Although the increasing interest in this discipline, there is still confusion regarding the basic concepts of ABSA: terms like sentiment, affect, emotion, opinion, are used as synonyms while they represent different concepts. This often leads to an incorrect analysis of the users’ opinions.This work presents an overview of the state-of-the-art techniques and approaches for ABSA, highlighting the main critical issues related to current trends in this field. Following this analysis, a new reference model for SA and ABSA, namely the KnowMIS-ABSA model, is proposed. The model is grounded on the consideration that sentiment, affect, emotion and opinion are very different concepts and that it is profoundly wrong to use the same metric and the same technique to measure them. Accordingly, we argue that different tools and metrics should be adopted to measure each of the dimensions of an opinion. A qualitative case study, regarding product reviews, is proposed to motivate the advantages of the KnowMIS-ABSA model.

---

Survey on aspect detection for aspect-based sentiment analysis
10.1007/s10462-022-10252-y
Sentiment analysis is an important tool to automatically understand the user-generated content on the Web. The most fine-grained sentiment analysis is concerned with the extraction and sentiment classification of aspects and has been extensively studied in recent years. In this work, we provide an overview of the first step in aspect-based sentiment analysis that assumes the extraction of opinion targets or aspects. We define a taxonomy for the extraction of aspects and present the most relevant works accordingly, with a focus on the most recent state-of-the-art methods. The three main classes we use to classify the methods designed for the detection of aspects are pattern-based, machine learning, and deep learning methods. Despite their differences, only a small number of works belong to a unique class of methods. All the introduced methods are ranked in terms of effectiveness. In the end, we highlight the main ideas that have led the research on this topic. Regarding future work, we deemed that the most promising research directions are the domain flexibility and the end-to-end approaches.

---

Challenges and future in deep learning for sentiment analysis: a comprehensive review and a proposed novel hybrid approach
10.1007/s10462-023-10651-9
Social media is used to categorise products or services, but analysing vast comments is time-consuming. Researchers use sentiment analysis via natural language processing, evaluating methods and results conventionally through literature reviews and assessments. However, our approach diverges by offering a thorough analytical perspective with critical analysis, research findings, identified gaps, limitations, challenges and future prospects specific to deep learning-based sentiment analysis in recent times. Furthermore, we provide in-depth investigation into sentiment analysis, categorizing prevalent data, pre-processing methods, text representations, learning models, and applications. We conduct a thorough evaluation of recent advances in deep learning architectures, assessing their pros and cons. Additionally, we offer a meticulous analysis of deep learning methodologies, integrating insights on applied tools, strengths, weaknesses, performance results, research gaps, and a detailed feature-based examination. Furthermore, we present in a thorough discussion of the challenges, drawbacks, and factors contributing to the successful enhancement of accuracy within the realm of sentiment analysis. A critical comparative analysis of our article clearly shows that capsule-based RNN approaches give the best results with an accuracy of 98.02% which is the CNN or RNN-based models. We implemented various advanced deep-learning models across four benchmarks to identify the top performers. Additionally, we introduced the innovative CRDC (Capsule with Deep CNN and Bi structured RNN) model, which demonstrated superior performance compared to other methods. Our proposed approach achieved remarkable accuracy across different databases: IMDB (88.15%), Toxic (98.28%), CrowdFlower (92.34%), and ER (95.48%). Hence, this method holds promise for automated sentiment analysis and potential deployment.

---

Multi-level textual-visual alignment and fusion network for multimodal aspect-based sentiment analysis
10.1007/s10462-023-10685-z
Multimodal Aspect-Based Sentiment Analysis (MABSA) is an essential task in sentiment analysis that has garnered considerable attention in recent years. Typical approaches in MABSA often utilize cross-modal Transformers to capture interactions between textual and visual modalities. However, bridging the semantic gap between modalities spaces and addressing interference from irrelevant visual objects at different scales remains challenging. To tackle these limitations, we present the Multi-level Textual-Visual Alignment and Fusion Network (MTVAF) in this work, which incorporates three auxiliary tasks. Specifically, MTVAF first transforms multi-level image information into image descriptions, facial descriptions, and optical characters. These are then concatenated with the textual input to form a textual+visual input, facilitating comprehensive alignment between visual and textual modalities. Next, both inputs are fed into an integrated text model that incorporates relevant visual representations. Dynamic attention mechanisms are employed to generate visual prompts to control cross-modal fusion. Finally, we align the probability distributions of the textual input space and the textual+visual input space, effectively reducing noise introduced during the alignment process. Experimental results on two MABSA benchmark datasets demonstrate the effectiveness of the proposed MTVAF, showcasing its superior performance compared to state-of-the-art approaches. Our codes are available at https://github.com/MKMaS-GUET/MTVAF.

---

Explainable artificial intelligence (XAI) in finance: a systematic literature review
10.1007/s10462-024-10854-8
Abstract As the range of decisions made by Artificial Intelligence (AI) expands, the need for Explainable AI (XAI) becomes increasingly critical. The reasoning behind the specific outcomes of complex and opaque financial models requires a thorough justification to improve risk assessment, minimise the loss of trust, and promote a more resilient and trustworthy financial ecosystem. This Systematic Literature Review (SLR) identifies 138 relevant articles from 2005 to 2022 and highlights empirical examples demonstrating XAI's potential benefits in the financial industry. We classified the articles according to the financial tasks addressed by AI using XAI, the variation in XAI methods between applications and tasks, and the development and application of new XAI methods. The most popular financial tasks addressed by the AI using XAI were credit management, stock price predictions, and fraud detection. The three most commonly employed AI black-box techniques in finance whose explainability was evaluated were Artificial Neural Networks (ANN), Extreme Gradient Boosting (XGBoost), and Random Forest. Most of the examined publications utilise feature importance, Shapley additive explanations (SHAP), and rule-based methods. In addition, they employ explainability frameworks that integrate multiple XAI techniques. We also concisely define the existing challenges, requirements, and unresolved issues in applying XAI in the financial sector.

---

ESG ratings explainability through machine learning techniques
10.1007/s10479-023-05514-z
Environmental, Social, and Governance (ESG) scores are quantitative assessments of companies’ commitment to sustainability that have become extremely popular tools in the financial industry. However, transparency in the ESG assessment process is still far from being achieved. In fact there is no full disclosure on how the ratings are computed. As a matter of fact, rating agencies determine ESG ratings (as a function of the E, S and G scores) through proprietary models which public knowledge is limited to what the data provider effectively chooses to disclose, that, in many cases, is restricted only to the main ideas and essential principles of the procedure. The goal of this work is to exploit machine learning techniques to shed light on the ESG ratings issuance process. In particular, we focus on the Refinitiv data provider, widely used both from practitioners and from academics, and we consider white-box and black-box mathematical models to reconstruct the E, S, and G ratings’ assessment model. The results show that it is possible to replicate the underlying assessment process with a satisfying level of accuracy, shedding light on the proprietary models employed by the data provider. However, there is evidence of persisting unlearnable noise that even more complex models cannot eliminate. Finally, we consider some interpretability instruments to identify the most important factors explaining the ESG ratings.

---

Aspect-gated graph convolutional networks for aspect-based sentiment analysis
10.1007/s10489-020-02095-3
Aspect-based sentiment analysis aims to predict the sentiment polarity of each specific aspect term in a given sentence. However, the previous models ignore syntactical constraints and long-range sentiment dependencies and mistakenly identify irrelevant contextual words as clues for judging aspect sentiment. In addition, these models usually use aspect-independent encoders to encode sentences, which can lead to a lack of aspect information. In this paper, we propose an aspect-gated graph convolutional network (AGGCN), that includes a special aspect gate designed to guide the encoding of aspect-specific information from the outset and construct a graph convolution network on the sentence dependency tree to make full use of the syntactical information and sentiment dependencies. The experimental results on multiple SemEval datasets demonstrate the effectiveness of the proposed approach, and our model outperforms the strong baseline models.

---

Knowledge graph-extended retrieval augmented generation for question answering
10.1007/s10489-025-06885-5
Large Language Models (LLMs) and Knowledge Graphs (KGs) offer a promising approach to robust and explainable Question Answering (QA). While LLMs excel at natural language understanding, they suffer from knowledge gaps and hallucinations. KGs provide structured knowledge but lack natural language interaction. Ideally, an AI system should be both robust to missing facts as well as easy to communicate with. This paper proposes such a system that integrates LLMs and KGs without requiring training, ensuring adaptability across different KGs with minimal human effort. The resulting approach can be classified as a specific form of a Retrieval Augmented Generation (RAG) with a KG, thus, it is dubbed Knowledge Graph-extended Retrieval Augmented Generation (KG-RAG). It includes a question decomposition module to enhance multi-hop information retrieval and answer explainability. Using In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting, it generates explicit reasoning chains processed separately to improve truthfulness. Experiments on the MetaQA benchmark show increased accuracy for multi-hop questions, though with a slight trade-off in single-hop performance compared to LLM with KG baselines. These findings demonstrate KG-RAG’s potential to improve transparency in QA by bridging unstructured language understanding with structured knowledge retrieval.

---

Measurement Issues in Environmental Corporate Social Responsibility (ECSR): Toward a Transparent, Reliable, and Construct Valid Instrument
10.1007/s10551-011-0967-x
One of the major roadblocks in conducting Environmental Corporate Social Responsibility (ECSR) research is operationalization of the construct. Existing ECSR measurement tools either require primary data gathering or special subscriptions to proprietary databases that have limited replicability. We address this deficiency by developing a transparent ECSR measure, with an explicit coding scheme, that strictly relies on publicly available data. Our ECSR measure tests favorably for internal consistency and inter-rater reliability, as well as convergent and discriminant validity.

---

Impression Management and Organizational Audiences: The Fiat Group Case
10.1007/s10551-013-1991-9
In this paper we investigate whether, and how, corporate management strategically uses disclosure to manage the perceptions of different organizational audiences. In particular, we examine the interactions between the FIAT Group and three of its key organizational audiences—the local press, the international press, and the financial analysts, which are characterized by different levels of salience for the company. We focus on both how management reacts to the optimism level existing within each audience and how the narrative disclosure tone adopted by FIAT influences the ex-post optimism in the local and international press or in the financial analyst community. We investigate the disclosure of the FIAT Group over a 6-year period (2004–2009), during which 70 price-sensitive press releases were published. On the basis of 1,887 (331) news articles published in Italian (international) newspapers and 411 analyst reports, we report evidence of different strategic patterns in the interaction processes between FIAT and its audiences. Our findings also indicate some differences in the way FIAT is affected by, and in turn, affects the sentiment of each audience, thus highlighting that the salience of the stakeholder is an important driver of the adoption of impression management techniques. Taken together, our findings point to issues related to setting the “tone at the top” and potential ethical matters.

---

Strategies for climate change and impression management : a case study among Canada’s large industrial emitters
10.1007/s10551-014-2322-5
This paper explores the justifications and impression management strategies that industrial companies use to rationalize their impacts on climate change. These strategies influence the perceptions of stakeholders through the use of techniques of neutralization intended to legitimize the impacts of corporate operations in the area of climate change. Based on a qualitative and inductive approach, 10 case studies were conducted of large Canadian industrial emitters. Interviews were conducted with managers and environmental specialists (n = 32). Public documentation was also collected when available. This study identifies six main neutralization techniques that industrial emitters use to rationalize their impacts: self-proclaimed excellence, promotion of a systemic view, denial and minimization, denouncing unfair treatment and deceptive appearances, economic and technological blackmail, and blaming others. The paper develops a better understanding of corporate arguments and strategies aimed at influencing the perceptions of stakeholders, including policymakers. The study also contributes to the literature on impression management by shedding light on new strategies and techniques of neutralization used by managers to shape the perceptions of stakeholders on socially sensitive issues.

---

GHG Reporting and Impression Management: An Assessment of Sustainability Reports from the Energy Sector
10.1007/s10551-015-2979-4
The objective of this study was to analyze the quality of climate information disclosed by companies and the impression management strategies they have developed to justify or conceal negative aspects of their performance. The study is based on a qualitative content analysis of the sustainability reports of 21 energy-sector companies that use the Global Reporting Initiative (GRI) with A or A+ application levels over a period of 5 years (n = 105). It contributes to the literature on climate disclosure by demonstrating the ineffectiveness of the external assurance process in ensuring the quality and representativeness of the data. Significant non-compliance with GRI standards was identified in 86 of the 93 reports audited by a third party. In addition, six of the 21 companies surveyed were found to disclose increasingly opaque information over time, concealing information on the measurement and methodology used. Through this study, four impression management strategies were identified. These are employed either to justify certain information (by minimizing impacts, excuses and commitment) or to conceal it (through strategic omissions and manipulation of figures). In exposing the high incidence of non-compliance in GRI reporting and the use of impression management strategies by companies, this study shows that it will be difficult or impossible for stakeholders to reasonably assess, monitor and compare companies’ climate performance on the basis of these reports.

---

Do ESG Controversies Matter for Firm Value? Evidence from International Data
10.1007/s10551-016-3213-8
The aim of this paper is to investigate the relationship between environmental, social, and governance (ESG) controversies and firm market value. We use a unique dataset of more than 4000 firms from 58 countries during 2002–2011. Primary analysis surprisingly shows that ESG controversies are associated with greater firm value. However, when interacted with the corporate social performance (CSP) score, ESG controversies are found to have no direct effect on firm value while the interaction appears to be highly and significantly positive. Building on this evidence, we attempt to explore the channels through which CSP may enhance market value. Conducting sample split analysis indicates that higher CSP score has an impact on market value only for high-attention firms, those firms which are larger, perform better, located in countries with greater press freedom, more searched on the Internet, more followed by analysts, and have an improved corporate social reputation. Thus, our findings provide new insights on the role of firm visibility through which firms can profit from their CSP.

---

Won’t Get Fooled Again: The Effects of Internal and External CSR ECO-Labeling
10.1007/s10551-017-3512-8
Although most consumers are positive about socially responsible companies, in order to benefit from CSR efforts, effective and clear CSR communication is important. However, due to the constantly rising profusion of eco-labels, based on either own claims from the organization or claims made by an external third party, consumers may encounter difficulties in identifying truly responsible firms, which could result in less effective CSR initiatives, even for those responsible firms. Therefore, building on attribution theory, this study seeks to identify how uncertified internal CSR claims and external third-party CSR labels should be used in order to deter greenwashing and increase positive consumer evaluations. Within a 3 (external third-party CSR label: positive vs. negative vs. no label) × 2 (uncertified internal CSR claim: present vs. absent) design, respondents are exposed to different coffee product packages measuring their attitude toward the brand, corporate credibility, purchase intention, and scent perception, as well as perceived attributional CSR motives. Overall, findings indicate that especially an external CSR label affects consumer responses toward the firm. Moreover, perceived CSR motives serve as a mediator between an external CSR label and corporate credibility and brand attitude, respectively. These findings warrant further consideration of introducing an external multilevel rating systems by governmental law.

---

Every Little Helps? ESG News and Stock Market Reaction
10.1007/s10551-017-3667-3
Stories about corporate social responsibility have become very frequent over the past decade, and managers can no longer ignore their impact on firm value. In this paper, we investigate the extent and the determinants of the stock market’s reaction following ordinary news related to environmental, social and governance issues—the so-called ESG factors. To that purpose, we use an original database provided by Covalence EthicalQuote. Our empirical analysis is based on about 33,000 ESG news (positive or negative), targeting one hundred listed companies over the period 2002–2010. On average, firms facing negative events experience a drop in their market value of 0.1%, whereas companies gain nothing on average from positive announcements. We find also that market participants are responsive to the media, but they do not react to firms’ press releases or to NGOs’ disclosures. Moreover, our results indicate that sector’s reputation mitigates the loss (the goodwill hypothesis) and that cultural proximity and lexical contents of ESG disclosures play a significant role in the magnitude of the impact.

---

Do Corporate Social Responsibility Reports Convey Value Relevant Information? Evidence from Report Readability and Tone
10.1007/s10551-020-04496-3
Corporate social responsibility (CSR) reporting is becoming mainstream, yet there is limited research on whether and how CSR reports communicate value relevant information. We examine the effects of CSR report readability and tone on future CSR performance and the market reaction around the release of CSR reports. Using a hand-collected dataset of Fortune 500 companies that published stand-alone CSR reports from 2002 to 2014, we find that 1-year-ahead CSR performance is positively associated with the changes in both CSR report readability and tone, suggesting that more readable text and more optimistic tone in a firm’s CSR report are indicative of better future CSR performance. Furthermore, consistent with the view that CSR reports communicate important value relevant information to the market, we document significant market reactions to report readability and tone around the release of CSR reports. Additional analyses suggest that CSR report readability enhances the association between the abnormal returns and the change in CSR report tone, and that the market reaction to CSR report readability is more pronounced for firms with lower analyst following and higher financial opacity. Taken together, our results substantiate the important roles of CSR report readability and tone in communicating future CSR performance and imparting value relevant information to the market.

---

When Do Corporate Good Deeds Become a Burden? The Role of Corporate Social Responsibility Following Negative Events
10.1007/s10551-023-05511-z
Abstract This study investigates the differential roles of corporate social responsibility (CSR) in the context of negative events. By categorizing CSR and negative events by their respective stakeholder groups, primary and secondary stakeholders, we theorize and test differential impacts of CSR and their interaction effects with different types of negative events. We propose that, while CSR toward secondary stakeholders offers the monotonous risk-tempering effect, CSR toward primary stakeholders has heterogeneous effects when facing negative events. Specifically, the effect of CSR toward primary stakeholders varies with the type of negative events. When negative events are associated with secondary stakeholders in the domain of morality, CSR toward primary stakeholders presents a risk-amplifying effect. When the negative events are associated with primary stakeholders in the domain of capability, however, CSR toward primary stakeholders does not present a significant risk-amplifying effect. In contrast, CSR toward secondary stakeholders presents the risk-tempering effect regardless of the type of negative events. We find general support for these arguments when we analyze the market responses to the news events of RepRisk, which provides data of various corporate negative events covered by the media.

---

Evaluation of machine learning-based information extraction algorithms: criticisms and recommendations
10.1007/s10579-008-9079-3
We survey the evaluation methodology adopted in information extraction (IE), as defined in a few different efforts applying machine learning (ML) to IE. We identify a number of critical issues that hamper comparison of the results obtained by different researchers. Some of these issues are common to other NLP-related tasks: e.g., the difficulty of exactly identifying the effects on performance of the data (sample selection and sample size), of the domain theory (features selected), and of algorithm parameter settings. Some issues are specific to IE: how leniently to assess inexact identification of filler boundaries, the possibility of multiple fillers for a slot, and how the counting is performed. We argue that, when specifying an IE task, these issues should be explicitly addressed, and a number of methodological characteristics should be clearly defined. To empirically verify the practical impact of the issues mentioned above, we perform a survey of the results of different algorithms when applied to a few standard datasets. The survey shows a serious lack of consensus on these issues, which makes it difficult to draw firm conclusions on a comparative evaluation of the algorithms. Our aim is to elaborate a clear and detailed experimental methodology and propose it to the IE community. Widespread agreement on this proposal should lead to future IE comparative evaluations that are fair and reliable. To demonstrate the way the methodology is to be applied we have organized and run a comparative evaluation of ML-based IE systems (the Pascal Challenge on ML-based IE) where the principles described in this article are put into practice. In this article we describe the proposed methodology and its motivations. The Pascal evaluation is then described and its results presented.

---

MEMD-ABSA: a multi-element multi-domain dataset for aspect-based sentiment analysis
10.1007/s10579-025-09820-9
Aspect-based sentiment analysis is a long-standing research interest in the field of opinion mining, and in recent years, researchers have gradually shifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA tasks. However, the datasets currently used in the research are limited to individual elements of specific tasks, usually focusing on in-domain settings, ignoring implicit aspects and opinions, and with a small data scale. To address these issues, we propose a large-scale Multi-Element Multi-Domain dataset (MEMD) that covers the four elements across five domains, including nearly 20,000 review sentences and 30,000 quadruples annotated with both explicit and implicit aspects and opinions for ABSA research. Meanwhile, we conduct experiments on multiple ABSA subtasks under the open domain setting to verify the effectiveness of several generative and non-generative baselines, and the results show that open domain ABSA as well as mining implicit aspects and opinions remain ongoing challenges to be addressed.

---

Sustainability benchmarking tool (SBT): theoretical and conceptual model proposition of a composite framework
10.1007/s10668-019-00512-3
Sustainable development and sustainability notions are among trending topics of twenty-first century. Elevated sustainability concerns of various stakeholders have been forcing members of all industries to evolve into their more environmentally and socially responsible versions. However, a complete framework with a true sustainability and benchmarking focus is yet to be delivered. Within this study, an innovative, holistic, versatile and scalable tool was developed to assess and benchmark sustainability performance of organizations and supply chains. The proposed framework was established upon trivet structure of triple bottom line philosophy and fueled by lean, Six Sigma and life cycle analysis methodologies for accurate and effective measurement of sustainability performance. Completeness of the framework was ensured through development of first-generation key performance indicator pool with 33 indicators, a unique work environment assessment mechanism for safety and environmental protection issues in terms of 11 risk categories and by construction of an ownership structure for ease of framework deployment. Proposed framework is expected to help with true sustainability performance improvement and benchmarking objectives at a range of business levels from facility to sectoral operations. Both small- and medium-sized enterprises and large corporations could benefit from SBT Framework since it eliminates unit-based comparisons within its standardized performance measurement modules. Industries with lower profit margins could also gain competitive edge through continuous discovery of improvement opportunities. Furthermore, some manufacturing industries with unique characteristics such as wood products industries with their carbon sequestration potential and electric car manufacturers with their renewable energy-dependent final products could document their strengths more effectively through this science-based assessment mechanism.

---

Green, blue or black, but washing–What company characteristics determine greenwashing?
10.1007/s10668-021-01602-x
The purpose of this paper is to study what are the characteristics that make firms less or more prone to greenwashing. We collect data from sustainability disclosures of the S&P top 100 companies, to investigate the determinants of greenwashing. We use content analysis to measure the level of reporting of the companies. We define the “greenwashing” variable as the difference between what the company says it does in terms of commitment to sustainability, and what the company actually does as evaluated by external parties (Bloomberg ESG scores). Our results show that companies in environmentally sensitive industries greenwash less than their counterparts in other industries, as well as companies following the GRI guidelines. Companies that issue a sustainability report and assure it greenwash less than those that do not do it. Contrary to our intuition, companies in industries with close proximity and high visibility greenwash more than their counterparts. A limitation of the paper is the inclusion in the sample of data from one country. Our findings have implications for policy-makers, particularly in Europe, where some European states have already regulated on green issues reporting and lately on blue issues. It might be interesting to consider both the industry effect and the relevance of reporting mechanisms when developing regulation and policies in order to improve the quality of sustainability reporting. We contribute to literature by proposing a new quantitative measure to assess greenwashing practices, to better understand the effect of industry and reporting mechanisms on greenwashing.

---

A review of the benefits and drawbacks of high-stakes final examinations in higher education
10.1007/s10734-023-01148-z
Abstract High-stakes examinations enjoy widespread use as summative assessments in higher education. We review the arguments for and against their use, across seven common themes: memory recall and knowledge retention; student motivation and learning; authenticity and real-world relevance; validity and reliability; academic misconduct and contract cheating; stress, anxiety and wellbeing; and fairness and equity. For each theme, we evaluate empirical evidence for the perceived pedagogical benefits and pedagogical drawbacks of high-stakes examinations. We find that relatively few of the perceived academic benefits of high-stakes examinations have a strong evidence base. Support for their use is largely rooted in opinion and pragmatism, rather than being justified by scientific evidence or pedagogical merit. By contrast, there is substantial evidence for pedagogical drawbacks of high-stakes summative examinations. We conclude that the current heavy reliance on high-stakes final examinations in many university subjects is poorly justified by the balance of empirical evidence.

---

Reduced implication-bias logic loss for neuro-symbolic learning
10.1007/s10994-023-06436-4
Integrating logical reasoning and machine learning by ap- proximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems. How- ever, some differentiable operators could bring a signiﬁcant bias during backpropagation and degrade the performance of Neuro-Symbolic learning. In this paper, we reveal that this bias, named Implication Bias is common in loss functions derived from fuzzy logic operators. Furthermore, we propose a simple yet effective method to transform the biased loss functions into Reduced Implication-bias Logic Loss (RILL) to address the above problem. Empirical study shows that RILL can achieve signiﬁcant improvements compared with the biased logic loss functions, especially when the knowl- edge base is incomplete, and keeps more robust than the compared methods when labelled data is insufﬁcient.

---

Application of multi-level matching between financial performance and corporate social responsibility in the banking industry
10.1007/s11156-016-0582-0
This study applies a new matching method to examine the old yet debatable idea that high corporate social responsibility (CSR) is associated with improved bank financial performance (FP). The conventional matching method focuses on one treatment effect. Thus, the old method is considered inappropriate when banks exhibit various degrees of CSR. To address this problem, we first apply the new multi-level matching method to multi-degree CSR and therefore contribute to studies that consider multi-degree CSR without adopting a multi-level matching method. We propose that “the more CSR, the better the FP,” which implies that banks engaged in more CSR exhibit better FP. Results before and after the matching significantly differ. CSR insignificantly influences bank FP before matching, but CSR has a strongly positive influence on bank FP after matching. Such effect on bank FP is further strengthened when banks increase their CSR activities, which supports our argument.

---

Information extraction from scientific articles: a survey
10.1007/s11192-018-2921-5
In last few decades, with the advent of World Wide Web (WWW), world is being overloaded with huge data. This huge data carries potential information that once extracted, can be used for betterment of humanity. Information from this data can be extracted using manual and automatic analysis. Manual analysis is not scalable and efficient, whereas, the automatic analysis involves computing mechanisms that aid in automatic information extraction over huge amount of data. WWW has also affected overall growth in scientific literature that makes the process of literature review quite laborious, time consuming and cumbersome job for researchers. Hence a dire need is felt to automatically extract potential information out of immense set of scientific articles to automate the process of literature review. Therefore, in this study, aim is to present the overall progress concerning automatic information extraction from scientific articles. The information insights extracted from scientific articles are classified in two broad categories i.e. metadata and key-insights. As available benchmark datasets carry a significant role in overall development in this research domain, existing datasets against both categories are extensively reviewed. Later, research studies in literature that have applied various computational approaches applied on these datasets are consolidated. Major computational approaches in this regard include Rule-based approaches, Hidden Markov Models, Conditional Random Fields, Support Vector Machines, Naive-Bayes classification and Deep Learning approaches. Currently, there are multiple projects going on that are focused towards the dataset construction tailored to specific information needs from scientific articles. Hence, in this study, state-of-the-art regarding information extraction from scientific articles is covered. This study also consolidates evolving datasets as well as various toolkits and code-bases that can be used for information extraction from scientific articles.

---

Rethinking SME default prediction: a systematic literature review and future perspectives.
10.1007/s11192-020-03856-0
Over the last dozen years, the topic of small and medium enterprise (SME) default prediction has developed into a relevant research domain that has grown for important reasons exponentially across multiple disciplines, including finance, management, accounting, and statistics. Motivated by the enormous toll on SMEs caused by the 2007-2009 global financial crisis as well as the recent COVID-19 crisis and the consequent need to develop new SME default predictors, this paper provides a systematic literature review, based on a statistical, bibliometric analysis, of over 100 peer-reviewed articles published on SME default prediction modelling over a 34-year period, 1986 to 2019. We identified, analysed and reviewed five streams of research and suggest a set of future research avenues to help scholars and practitioners address the new challenges and emerging issues in a changing economic environment. The research agenda proposes some new innovative approaches to capture and exploit new data sources using modern analytical techniques, like artificial intelligence, machine learning, and macro-data inputs, with the aim of providing enhanced predictive results.

---

SA-ASBA: a hybrid model for aspect-based sentiment analysis using synthetic attention in pre-trained language BERT model with extreme gradient boosting
10.1007/s11227-022-04881-x
Aspect-based sentiment analysis (ABSA) is a granular-level sentiment analysis task that aims to detect the sentiment polarities of a specified aspect in the text. This research shows excessive curiosity in modelling target and context through attention networks to attain effective feature representations for sentiment detection works. We have proposed a synthetic attention in bidirectional encoder representations from transformers (SA-BERT) with an extreme gradient boosting (XGBoost) classifier to classify sentiment polarity in the review dataset. The proposed model generates dynamic word vector encoding of the aspect and corresponding context of the reviews. Then, the aspect and context of the reviews are meaningfully represented by a transformer that can input the vector word in parallel. After that, the model uses the synthetic attention mechanism to learn essential parts of context and aspects in reviews. Finally, the model places overall representation in the sentiment classification layer to predict sentiment polarity. Both proposed SA-BERT and SA-BERT-XGBoost models achieved the highest accuracy (92.02 and 93.71%) on the restaurant16 and highest F-1 scores (81.19 and 81.64%) on the restaurant14 dataset, respectively. The average accuracy and F1 scores are approximately 2 and 3.04% higher than the baseline models (DLCF-DCA-CDM, R-GAT+BERT, ASGCN-DG, AEN-BERT and BERT-PT). Therefore, proposed models outperform in comparison with baseline models.

---

Adaptive user modelling in car racing games using behavioural and physiological data
10.1007/s11257-017-9192-3
Personalised content adaptation has great potential to increase user engagement in video games. Procedural generation of user-tailored content increases the self-motivation of players as they immerse themselves in the virtual world. An adaptive user model is needed to capture the skills of the player and enable automatic game content altering algorithms to fit the individual user. We propose an adaptive user modelling approach using a combination of unobtrusive physiological data to identify strengths and weaknesses in user performance in car racing games. Our system creates user-tailored tracks to improve driving habits and user experience, and to keep engagement at high levels. The user modelling approach adopts concepts from the Trace Theory framework; it uses machine learning to extract features from the user's physiological data and game-related actions, and cluster them into low level primitives. These primitives are transformed and evaluated into higher level abstractions such as experience, exploration and attention. These abstractions are subsequently used to provide track alteration decisions for the player. Collection of data and feedback from 52 users allowed us to associate key model variables and outcomes to user responses, and to verify that the model provides statistically significant decisions personalised to the individual player. Tailored game content variations between users in our experiments, as well as the correlations with user satisfaction demonstrate that our algorithm is able to automatically incorporate user feedback in subsequent procedural content generation.

---

Challenges and Advances in Information Extraction from Scientific Literature: a Review
10.1007/s11837-021-04902-9
Scientific articles have long been the primary means of disseminating scientific discoveries. Over the centuries, valuable data and potentially groundbreaking insights have been collected and buried deep in the mountain of publications. In materials engineering, such data are spread across technical handbooks specification sheets, journal articles, and laboratory notebooks in myriad formats. Extracting information from papers on a large scale has been a tedious and time-consuming job to which few researchers have wanted to devote their limited time and effort, yet is an activity that is essential for modern data-driven design practices. However, in recent years, significant progress has been made by the computer science community on techniques for automated information extraction from free text. Yet, transformative application of these techniques to scientific literature remains elusive—due not to a lack of interest or effort but to technical and logistical challenges. Using the challenges in the materials science literature as a driving motivation, we review the gaps between state-of-the-art information extraction methods and the practical application of such methods to scientific texts, and offer a comprehensive overview of work that can be undertaken to close these gaps.

---

A novel automated essay scoring approach for reliable higher educational assessments
10.1007/s12528-021-09283-1
E-learning is gradually gaining prominence in higher education, with universities enlarging provision and more students getting enrolled. The effectiveness of automated essay scoring (AES) is thus holding a strong appeal to universities for managing an increasing learning interest and reducing costs associated with human raters. The growth in e-learning systems in the higher education system and the demand for consistent writing assessments has spurred research interest in improving the accuracy of AES systems. This paper presents a transformer-based neural network model for improved AES performance using Bi-LSTM and RoBERTa language model based on Kaggle’s ASAP dataset. The proposed model uses Bi-LSTM model over pre-trained RoBERTa language model to address the coherency issue in essays that is ignored by traditional essay scoring methods, including traditional NLP pipelines, deep learning-based methods, a mixture of both. The comparison of the experimental results on essay scoring with human raters concludes that the proposed model outperforms the existing methods in essay scoring in terms of QWK score. The comparative analysis of results demonstrates the applicability of the proposed model in automated essay scoring at higher education level.

---

Deep Neural Approaches to Relation Triplets Extraction: a Comprehensive Survey
10.1007/s12559-021-09917-7
The task of relation extraction is about identifying entities and relations among them in free text for the enrichment of structured knowledge bases (KBs). In this paper, we present a comprehensive survey of this important research topic in natural language processing. Recently, with the advances made in the continuous representation of words (word embeddings) and deep neural architectures, many research works are published in the area of relation extraction. To help future research, we present a comprehensive review of the recently published research works in relation extraction. Previous surveys on this task covered only one aspect of relation extraction that is pipeline-based relation extraction approaches at the sentence level. In this survey, we cover sentence-level relation extraction to document-level relation extraction, pipeline-based approaches to joint extraction approaches, annotated datasets to distantly supervised datasets along with few very recent research directions such as zero-shot or few-shot relation extraction, noise mitigation in distantly supervised datasets. Regarding neural architectures, we cover convolutional models, recurrent network models, attention network models, and graph convolutional models in this survey. We survey more than 100 publications in the field of relation extraction and present them in a structured way based on their similarity in the specific task they tried to solve, their model architecture, the datasets they used for experiments. We include the current state-of-the-art performance in several datasets in this paper for comparison. In this paper, we have covered different aspects of research in relation extraction field with a key focus on recent deep neural network-based methods. Also, we identify possible future research directions. Hopefully, this will help future researchers to identify the current research gaps and take the field forward.

---

Cognitive-Inspired Deep Learning Models for Aspect-Based Sentiment Analysis: A Retrospective Overview and Bibliometric Analysis
10.1007/s12559-024-10331-y
Abstract As cognitive-inspired computation approaches, deep neural networks or deep learning (DL) models have played important roles in allowing machines to reach human-like performances in various complex cognitive tasks such as cognitive computation and sentiment analysis. This paper offers a thorough examination of the rapidly developing topic of DL-assisted aspect-based sentiment analysis (DL-ABSA), focusing on its increasing importance and implications for practice and research advancement. Leveraging bibliometric indicators, social network analysis, and topic modeling techniques, the study investigates four research questions: publication and citation trends, scientific collaborations, major themes and topics, and prospective research directions. The analysis reveals significant growth in DL-ABSA research output and impact, with notable contributions from diverse publication sources, institutions, and countries/regions. Collaborative networks between countries/regions, particularly between the USA and China, underscore global engagement in DL-ABSA research. Major themes such as syntax and structure analysis, neural networks for sequence modeling, and specific aspects and modalities in sentiment analysis emerge from the analysis, guiding future research endeavors. The study identifies prospective avenues for practitioners, emphasizing the strategic importance of syntax analysis, neural network methodologies, and domain-specific applications. Overall, this study contributes to the understanding of DL-ABSA research dynamics, providing a roadmap for practitioners and researchers to navigate the evolving landscape and drive innovations in DL-ABSA methodologies and applications.

---

News and ESG investment criteria: What’s behind it?
10.1007/s13278-024-01209-w
Abstract News written in the press about different companies generates consumer feelings that can condition the reputation of these companies and, consequently, their financial results. One of the practices that might improve a company’s reputation is the Environmental, Social and Governance (ESG) investment criteria. In this research, using Natural Language Processing techniques like Sentiment Analysis and Word2Vec, we detected those ESG-related terms that the written press uses in news articles about companies. Thus, we have been able to discover and analyze those terms that improve sympathy toward companies, and those that worsen it. Our findings show that those terms related to sustainable development, good social practices and ethical governance improve the general public’s opinion of a company, while those related to greenwashing and socialwashing worsen it. Therefore, this methodology is valid for enabling companies to detect those terms that improve or worsen their reputation, and thus help them make decisions that improve their image.

---

Assessing Students’ Use of Evidence and Organization in Response-to-Text Writing: Using Natural Language Processing for Rubric-Based Automated Scoring
10.1007/s40593-017-0143-2
This paper presents an investigation of score prediction based on natural language processing for two targeted constructs within analytic text-based writing: 1) students’ effective use of evidence and, 2) their organization of ideas and evidence in support of their claim. With the long-term goal of producing feedback for students and teachers, we designed a task-dependent model, for each dimension, that aligns with the scoring rubric and makes use of the source material. We believe the model will be meaningful and easy to interpret given the writing task. We used two datasets of essays written by students in grades 5–6 and 6–8. Our experimental results show that our task-dependent model (consistent with the rubric) performs as well as if not outperforms competitive baselines. We also show the potential generalizability of the rubric-based model by performing cross-corpus experiments. Finally, we show that the predictive utility of different feature groups in our rubric-based modeling approach is related to how much each feature group covers a rubric’s criteria.

---

Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric Scores Determined?
10.1007/s40593-020-00211-5
This article investigates the feasibility of using automated scoring methods to evaluate the quality of student-written essays. In 2012, Kaggle hosted an Automated Student Assessment Prize contest to find effective solutions to automated testing and grading. This article: a) analyzes the datasets from the contest – which contained hand-graded essays – to measure their suitability for developing competent automated grading tools; b) evaluates the potential for deep learning in automated essay scoring (AES) to produce sophisticated testing and grading algorithms; c) advocates for thorough and transparent performance reports on AES research, which will facilitate fairer comparisons among various AES systems and permit study replication; d) uses both deep neural networks and state-of-the-art NLP tools to predict finer-grained rubric scores, to illustrate how rubric scores are determined from a linguistic perspective, and to uncover important features of an effective rubric scoring model. This study’s findings first highlight the level of agreement that exists between two human raters for each rubric as captured in the investigated essay dataset, that is, 0.60 on average as measured by the quadratic weighted kappa (QWK). Only one related study has been found in the literature which also performed rubric score predictions through models trained on the same dataset. At best, the predictive models had an average agreement level (QWK) of 0.53 with the human raters, below the level of agreement among human raters. In contrast, this research’s findings report an average agreement level per rubric with the two human raters’ resolved scores of 0.72 (QWK), well beyond the agreement level between the two human raters. Further, the AES system proposed in this article predicts holistic essay scores through its predicted rubric scores and produces a QWK of 0.78, a competitive performance according to recent literature where cutting-edge AES tools generate agreement levels between 0.77 and 0.81, results computed as per the same procedure as in this article. This study’s AES system goes one step further toward interpretability and the provision of high-level explanations to justify the predicted holistic and rubric scores. It contends that predicting rubric scores is essential to automated essay scoring, because it reveals the reasoning behind AIED-based AES systems. Will building AIED accountability improve the trustworthiness of the formative feedback generated by AES? Will AIED-empowered AES systems thoroughly mimic, or even outperform, a competent human rater? Will such machine-grading systems be subjected to verification by human raters, thus paving the way for a human-in-the-loop assessment mechanism? Will trust in new generations of AES systems be improved with the addition of models that explain the inner workings of a deep learning black box? This study seeks to expand these horizons of AES to make the technique practical, explainable, and trustable.

---

Automated Feedback and Automated Scoring in the Elementary Grades: Usage, Attitudes, and Associations with Writing Outcomes in a Districtwide Implementation of MI Write
10.1007/s40593-020-00236-w
This study examined a naturalistic, districtwide implementation of an automated writing evaluation (AWE) software program called MI Write in elementary schools. We specifically examined the degree to which aspects of MI Write were implemented, teacher and student attitudes towards MI Write, and whether MI Write usage along with other predictors like demographics and writing self-efficacy explained variability in students’ performance on a proximal and distal measure of writing performance. The participants included 1935 students in Grades 3–5 and 135 writing teachers from 14 elementary schools in a mid-Atlantic school district. Findings indicated that though MI Write was somewhat under-utilized, teachers and students held positive attitudes towards the AWE system. Usage of MI Write had a mixed and limited predictive effect on outcomes: The number of essays written had a small predictive effect on state test performance for Grades 3 and 5; gain on revision had a moderate predictive effect on posttest writing quality and a small predictive effect for Grade 5 state test performance. Students’ average AWE scores showed consistently moderate to large predictive effects for all outcomes. Interpreted in light of the underlying architecture of MI Write, findings have implications for other school districts considering implementing AWE as well as the design of AWE systems intended to support the teaching and learning of writing.

---

Surveying neuro-symbolic approaches for reliable artificial intelligence of things
10.1007/s40860-024-00231-1
Abstract The integration of Artificial Intelligence (AI) with the Internet of Things (IoT), known as the Artificial Intelligence of Things (AIoT), enhances the devices’ processing and analysis capabilities and disrupts such sectors as healthcare, industry, and oil. However, AIoT’s complexity and scale are challenging for traditional machine learning (ML). Deep learning offers a solution but has limited testability, verifiability, and interpretability. In turn, the neuro-symbolic paradigm addresses these challenges by combining the robustness of symbolic AI with the flexibility of DL, enabling AI systems to reason, make decisions, and generalize knowledge from large datasets better. This paper reviews state-of-the-art DL models for IoT, identifies their limitations, and explores how neuro-symbolic methods can overcome them. It also discusses key challenges and research opportunities in enhancing AIoT reliability with neuro-symbolic approaches, including hard-coded symbolic AI, multimodal sensor data, biased interpretability, trading-off interpretability, and performance, complexity in integrating neural networks and symbolic AI, and ethical and societal challenges.

---

Syntactic-Enhanced Multi-Task Learning Model for Aspect Sentiment Triplet Extraction
10.1007/s41019-025-00289-8
Abstract Aspect sentiment triplet extraction (ASTE), which aims to extract aspect terms, opinion terms, and sentiment polarity from textual comments, is a crucial task in aspect-based sentiment analysis. Most existing approaches focus on leveraging contextual information while neglecting the effective utilization of syntactic structures within the text. To improve extraction performance, this paper proposes a syntax-enhanced multi-task learning model, SE-ASTE, which jointly extracts aspect sentiment triplets by incorporating syntax connections and dependency edge type information. Specifically, the ASTE task is decomposed into three sub-tasks: opinion entity extraction, relation detection, and sentiment extraction. To capture syntactic dependencies, we employ a graph convolutional network with an attention mechanism, which computes the importance of dependency edges and aggregates node information in a targeted manner to generate a syntax-enhanced contextual representation. Subsequently, a self-attention module is utilized to generate task-specific features, while a sentiment extraction module, based on a affine scorer, captures sentiment relationships between words. Experimental results on the ASTE-Data-V2 dataset demonstrate that SE-ASTE achieves an average improvement of 1.45% in the F1-score compared to baseline models, highlighting its effectiveness in aspect sentiment triplet extraction.

---

Automated measures of sentiment via transformer- and lexicon-based sentiment analysis (TLSA)
10.1007/s42001-023-00233-8
The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable social scientists to adopt modern computational methods for valid and reliable sentiment analysis of English text, we propose an open and free web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse tools and offers validation metrics, empowering users with limited computational knowledge and resources to reap the benefit of state-of-the-art computational methods. Two cases demonstrate the functionality and usability of TLSA. The performance of different tools varied to a large extent based on the dataset, supporting the importance of validating various sentiment tools in a specific context.

---

Effects of firm-level ESG performance on creditworthiness in Japanese listed companies
10.1007/s42495-022-00084-7
Against a backdrop of international public policies on sustainability such as SDGs, sustainable investment assets in the world stood at $30.7 trillion in 2018. In conjunction with the developing market of sustainable investment, academic research has proved that ESG performances positively affect financial performances in many cases. However, most of the previous studies have not analysed linkages between ESG performances and debt financing, nor have they targeted Asian markets. Thus, the purpose of this study is to analyse the effects of ESG performances on creditworthiness in Japanese companies. This empirical analysis used the ordered logit model where the dependent variable is S&P Credit Ratings, and the independent variables are ESG evaluations by Arabesque S-Ray. We used data for CY2019, and our final sample included 15 independent variables for 63 observations: 945 items were under the analysis. In result, we found that ESG total performance and governance performance positively affect a company’s creditworthiness, whilst there were no significant results concerning social and environmental performance. Therefore, business companies should consider improving governance performance for better credit ratings and financial institutions should consider governance performance in credit analysis. We also found that the effects of ESG total performance may change depending on whether ESG evaluations are based on financially material ESG issues, and that the effects of ESG total performance and pillar performances (environment, social, and governance, respectively) may differ. ESG evaluation agencies are recommended to disclose pillar performance evaluations with details of methodology for investors to consider ESG evaluations in investment decisions.

---

Regulating ESG Rating and Data Product Providers: Critically Examining EU Regulation through the Lens of Functional Regulatory Consistency
10.1017/err.2024.53
Abstract The expansion of EU regulatory governance in the financial sector since the end of the global financial crisis 2008 has given rise to the need to examine regulatory consistency in the volumes of financial regulation that may have cross-cutting implications. In this light, this article examines the effectiveness of the Regulation of ESG infomediaries through the lens of “functional regulatory consistency” with other infomediary regulations, for credit rating agencies and stock market benchmarks. It argues that this lens most aptly reveals the three key weaknesses of the regulatory regime for ESG infomediaries. These relate to sub-optimal coverage of scope, over-inclusiveness in the application of regulatory standards and under-inclusiveness where appropriate governance is not provided. the sub-optimal coverage of scope raises the question of whether ESG stock market index providers should indeed be regulated as ESG infomediaries or as stock market benchmarks more generally falling within the Benchmarks Regulation 2016. Over-inclusiveness and under-inclusiveness in the regulatory provision reflects blind spots in applying functional regulatory consistency, where it is inappropriate due to distinguishing features in business models, market structures or market relations.

---

Revisiting communicative competence in the age of AI: Implications for large-scale testing
10.1017/s0267190525000078
Abstract Changes in the characterization of communicative competence, especially in the context of large-scale testing, are typically driven by an evolving understanding of real-world communication and advancements in test construct theories. Recent advances in AI technology have fundamentally altered the way language users communicate and interact, prompting a reassessment of how communicative competence is defined and how language tests are constructed. In response to these significant changes, an AI-mediated interactionalist approach is proposed to expand communicative competence. This approach advocates for extending the traditional concept of communicative competence to encompass AI digital literacy skills and broadened cognitive and linguistic capabilities. These skills enable effective AI tool usage, as well as the interpretation and application of AI-generated outputs and feedback, to improve communication. Embedding these competencies into language assessments ensures alignment with contemporary communication dynamics, enhancing the relevance of language assessments, and preparing learners for navigating AI-augmented communication environments. While high-stakes testing faces considerable challenges in adopting this expanded construct, low-stakes formative assessments, where scores do not influence critical decisions about individuals and where opportunities exist to rectify errors in score-based actions, if any, provide a fertile ground for exploring the integration of AI tools into assessments. In these contexts, educators can explore giving learners access to various AI tools, such as editing and generative tools, to enhance assessment practices. These explorations can start to address some of the conceptual challenges involved in applying this expanded construct definition in high-stakes environments and contribute to resolving practical issues.

---

Machine Learning for Information Extraction in Informal Domains
10.1023/a:1007601113994
We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems.

---

The time is ripe for ESG + Nutrition: evidence-based nutrition metrics for Environmental, Social, and Governance (ESG) investing
10.1038/s41430-022-01075-9
The globe faces a nutrition crisis. Suboptimal diet is the leading cause of poor health worldwide, with devastating social, environmental, equity, and economic consequences [1]. In poor diet quality was estimated to cause 12 million deaths due to non-communicable diseases (NCDs) globally [1]. In the US, treatment of cardiovascular diseases, diabetes, and cancers accounted for 1 in 4 dollars in healthcare — and 18% higher spending than in 2009 [2]. Solutions to address the global health and economic burdens of nutrition-related disease must re-imagine and reform the food system – including new approaches to in ﬂ uence the private sector, which plays a critical role in supplying and in ﬂ uencing food choices, nutrition, and health outcomes of consumers. Among different levers, investors – including institutional investors, family of ﬁ ces, and venture capital – are powerful and underutilized stakeholders for stimulating change. The rise of Environmental, Social, and Governance (ESG) investing presents a remarkable new opportunity to align ﬁ nancial returns with bene ﬁ ts for society and the planet. This paradigm shift recognizes that long-term ﬁ nancial performance is directly linked to environmental and societal impact [3]. From 2012 to 2020, the value of global ESG-driven assets tripled to $40.5 trillion [4], and now represents nearly half of the world ’ s ﬁ nancial assets under management. Businesses have taken note. In 2021, 60 top global businesses committed to publicly supporting and reporting on a common set of Stakeholder Capitalism Metrics for ESG reporting [5]. And, at the 2021 UN Climate Change Conference (COP26), The International Financial Reporting Standards (IFRS) Foundation announced a new International Sustainability Standards Board to develop, consolidate,

---

A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing
10.1038/s41524-023-01003-w
The ever-increasing number of materials science articles makes it hard to infer chemistry-structure-property relations from published literature. We used natural language processing (NLP) methods to automatically extract material property data from the abstracts of polymer literature. As a component of our pipeline, we trained MaterialsBERT, a language model, using 2.4 million materials science abstracts, which outperforms other baseline models in three out of ﬁve named entity recognition datasets when used as the encoder for text. Using this pipeline, we obtained ∼ 300,000 material property records from ∼ 130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse range of applications such as fuel cells, supercapacitors, and polymer solar cells to recover non-trivial insights. The data extracted through our pipeline is made available through a web platform at https://polymerscholar.org which can be used to locate material property data recorded in abstracts conveniently. This work demonstrates the feasibility of an automatic pipeline that starts from published literature and ends with a complete set of extracted material property information. sequence tagging.

---

A type-augmented knowledge graph embedding framework for knowledge graph completion
10.1038/s41598-023-38857-5
Knowledge graphs (KGs) are of great importance to many artificial intelligence applications, but they usually suffer from the incomplete problem. Knowledge graph embedding (KGE), which aims to represent entities and relations in low-dimensional continuous vector spaces, has been proved to be a promising approach for KG completion. Traditional KGE methods only concentrate on structured triples, while paying less attention to the type information of entities. In fact, incorporating entity types into embedding learning could further improve the performance of KG completion. To this end, we propose a universal Type-augmented Knowledge graph Embedding framework (TaKE) which could utilize type features to enhance any traditional KGE models. TaKE automatically captures type features under no explicit type information supervision. And by learning different type representations of each entity, TaKE could distinguish the diversity of types specific to distinct relations. We also design a new type-constrained negative sampling strategy to construct more effective negative samples for the training process. Extensive experiments on four datasets from three real-world KGs (Freebase, WordNet and YAGO) demonstrate the merits of our proposed framework. In particular, combining TaKE with the recent tensor factorization KGE model SimplE can achieve state-of-the-art performance on the KG completion task.

---

Prompt-based fine-tuning with multilingual transformers for language-independent sentiment analysis
10.1038/s41598-025-03559-7
In the era of global digital communication, understanding user sentiment across multiple languages is a critical challenge with wide-ranging applications in opinion mining, customer feedback analysis, and social media monitoring. This study advances the field of language-independent sentiment analysis by leveraging prompt-based fine-tuning with state-of-the-art transformer models. The performance of classical machine learning approaches, hybrid deep learning architectures, and multilingual transformer models is evaluated across eight typologically diverse languages: Arabic, English, French, German, Hindi, Italian, Portuguese, and Spanish. Baseline models are established using traditional machine learning approaches such as Support Vector Machines (SVM) and Logistic Regression, with feature extraction methods like TF-IDF. A hybrid deep learning model is introduced, combining Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNNs) to capture local and sequential text patterns. Building on these, pre-trained multilingual transformer models, specifically BERT-base-multilingual and XLM-RoBERTa, are fine-tuned for language-independent sentiment classification tasks. The key contribution lies in the implementation of prompt-based fine-tuning strategies for language independent sentiment analysis. Using (1) prefix prompts and (2) cloze-style prompts, a unified framework is established that employs templates designed in one language and evaluates their performance on data from the remaining \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$(n-1)$$\end{document}(n-1) languages. Experimental results demonstrate that transformer models, particularly XLM-RoBERTa with prompt-based fine-tuning outperform both classical and deep learning methods. With only 32 training examples per class, prefix prompts produce results comparable to standard fine-tuning, which typically uses 70-80% of the data for training. This highlights the potential of prompt-based learning for scalable, multilingual sentiment analysis in diverse language settings.

---

Use of deep learning-based NLP models for full-text data elements extraction for systematic literature review tasks
10.1038/s41598-025-03979-5
Systematic literature review (SLR) is an important tool for Health Economics and Outcomes Research (HEOR) evidence synthesis. SLRs involve the identification and selection of pertinent publications and extraction of relevant data elements from full-text articles, which can be a manually intensive procedure. Previously we developed machine learning models to automatically identify relevant publications based on pre-specified inclusion and exclusion criteria. This study investigates the feasibility of applying Natural Language Processing (NLP) approaches to automatically extract data elements from the relevant scientific literature. First, 239 full-text articles were collected and annotated for 12 important variables including study cohort, lab technique, and disease type, for proper SLR summary of Human papillomavirus (HPV) Prevalence, Pneumococcal Epidemiology, and Pneumococcal Economic Burden. The three resulting annotated corpora are shared publicly at [https://github.com/Merck/NLP-SLR-corpora], to provide training data and a benchmark baseline for the NLP community to further research this challenging task. We then compared three classic Named Entity Recognition (NER) algorithms, namely Conditional Random Fields (CRF), Long Short-Term Memory (LSTM), and the Bidirectional Encoder Representations from Transformers (BERT) models, to assess performance on the data element extraction task. The annotation corpora contain 4,498, 579, and 252 annotated entity mentions for HPV Prevalence, Pneumococcal Epidemiology, and Pneumococcal Economic Burden tasks respectively. Deep learning algorithms achieved superior performance in recognizing the targeted SLR data elements, compared to conventional machine learning algorithms. LSTM models have achieved 0.890, 0.646 and 0.615 micro-averaged F1 scores for three tasks respectively. CRF models could not provide comparable performance on most of the elements of interest. Although BERT-based models are known to generally achieve superior performance on many NLP tasks, we did not observe improvement in our three tasks. Deep learning algorithms have achieved superior performance compared with machine learning models on multiple SLR data element extraction tasks. LSTM model, in particular, is more preferable for deployment in supporting HEOR SLR data element extraction, due to its better performance, generalizability, and scalability as it’s cost-effective in our SLR benchmark datasets.

---

A hybrid self attentive linearized phrase structured transformer based RNN for financial sentence analysis with sentence level explainability
10.1038/s41598-025-09265-8
As financial institutions want openness and accountability in their automated systems, the task of understanding model choices has become more crucial in the field of financial text analysis. In this study, we propose xFiTRNN, a hybrid model that integrates self-attention mechanisms, linearized phrase structure, and a contextualized transformer-based Recurrent Neural Network (RNN) to enhance both model performance and explainability in financial sentence prediction. The model captures subtle contextual information from financial texts while maintaining explainability. xFiTRNN provides transparent, sentence-level insights into predictions by incorporating advanced explainability techniques such as LIME (Local Interpretable Model-agnostic Explanations) and Anchors. Extensive evaluations on benchmark financial datasets demonstrate that xFiTRNN not only achieves a remarkable prediction performance but also enhances explainability in the financial sector. This work highlights the potential of hybrid transformer-based RNN architectures for fostering more accountable and understandable Artificial Intelligence (AI) applications in finance.

---

A scalable framework for evaluating multiple language models through cross-domain generation and hallucination detection
10.1038/s41598-025-15203-5
Large language models (LLMs) have significantly advanced in recent years, greatly enhancing the capabilities of retrieval-augmented generation (RAG) systems. However, challenges such as semantic similarity, bias/sentiment, and hallucinations persist, especially in domain-specific applications. This paper introduces MultiLLM-Chatbot, a scalable RAG-based benchmarking framework designed to evaluate five popular LLMs GPT-4-Turbo, CLAUDE-3.7-Sonnet, LLAMA-3.3-70B, DeepSeek-R1-Zero, and Gemini-2.0-Flash across five domains: Agriculture, Biology, Economics, Internet of Things (IoT), and Medical. Fifty peer-reviewed research papers (10 per domain) were used to generate 250 standardized queries, resulting in 1,250 model responses. Texts from PDFs were extracted using PyPDF2, segmented to preserve factual coherence, embedded with sentence-transformer models, and indexed in Elasticsearch for efficient retrieval. Each response was analyzed across 4 dimensions: cosine similarity for semantic similarity, VADER sentiment analysis for sentiment detection, TF-IDF scoring, and named entity recognition (NER) for hallucination identification and factual verification. A composite scoring scheme aggregates these metrics to rank model performance. Experimental results show LLAMA-3.3-70B as the overall best-performing model, leading in all 5 domains. The proposed framework is implemented using Colab notebooks, which offer a reproducible, extensive pipeline for domain-specific LLM benchmarking. Through the combination of cross-domain analysis and multi-metric evaluation, this study fills in the gaps in current LLM benchmarking procedures and offers a modular architecture that can be adjusted to new domains and future LLM advancements. The findings inform model selection strategies for researchers and practitioners seeking trustworthy LLM deployment across diverse industrial and scientific sectors.

---

A digital twin model for grain enterprise financial shared service centers based on distributed deep learning and neural symbolic reasoning
10.1038/s41598-025-24350-8
This paper presents a comprehensive digital twin model for grain enterprise financial shared service centers that integrates distributed deep learning capabilities with neural symbolic reasoning mechanisms to address complex financial management challenges. The proposed model employs a hierarchical architectural framework that combines the pattern recognition strengths of deep neural networks with the interpretability and knowledge representation capabilities of symbolic reasoning systems. The hybrid neural architecture integrates multilayer perceptrons, recurrent neural networks, and convolutional neural networks within a distributed computing framework, while the neural symbolic reasoning engine incorporates knowledge graphs and rule-based inference mechanisms for interpretable decision support. Experimental validation on real-world financial datasets demonstrates superior performance with 94.7% accuracy in financial prediction tasks, representing significant improvements over baseline approaches. Practical deployment across three major grain enterprise financial shared service centers showed substantial operational improvements, including 66.4% reduction in transaction processing time, 130.7% increase in process automation level, and 87.5% decrease in error rates. The economic analysis reveals annual operational cost savings exceeding $8.3 million across participating enterprises, validating the practical viability and transformative potential of the proposed approach in complex financial management environments.

---

Explainable AI analysis for smog rating prediction
10.1038/s41598-025-92788-x
Smog poses a direct threat to human health and the environment. Addressing this issue requires understanding how smog is formed. While major contributors include industries, fossil fuels, crop burning, and ammonia from fertilizers, vehicles play a significant role. Individually, a vehicle’s contribution to smog may be small, but collectively, the vast number of vehicles has a substantial impact. Manually assessing the contribution of each vehicle to smog is impractical. However, advancements in machine learning make it possible to quantify this contribution. By creating a dataset with features such as vehicle model, year, fuel consumption (city), and fuel type, a predictive model can classify vehicles based on their smog impact, rating them on a scale from 1 (poor) to 8 (excellent). This study proposes a novel approach using Random Forest and Explainable Boosting Classifier models, along with SMOTE (Synthetic Minority Oversampling Technique), to predict the smog contribution of individual vehicles. The results outperform previous studies, with the proposed model achieving an accuracy of 86%. Key performance metrics include a Mean Squared Error of 0.2269, R-Squared (R2) of 0.9624, Mean Absolute Error of 0.2104, Explained Variance Score of 0.9625, and a Max Error of 4.3500. These results incorporate explainable AI techniques, using both agnostic and specific models, to provide clear and actionable insights. This work represents a significant step forward, as the dataset was last updated only five months ago, underscoring the timeliness and relevance of the research.

---

Clinical entity augmented retrieval for clinical information extraction
10.1038/s41746-024-01377-1
Large language models (LLMs) with retrieval-augmented generation (RAG) have improved information extraction over previous methods, yet their reliance on embeddings often leads to inefficient retrieval. We introduce CLinical Entity Augmented Retrieval (CLEAR), a RAG pipeline that retrieves information using entities. We compared CLEAR to embedding RAG and full-note approaches for extracting 18 variables using six LLMs across 20,000 clinical notes. Average F1 scores were 0.90, 0.86, and 0.79; inference times were 4.95, 17.41, and 20.08 s per note; average model queries were 1.68, 4.94, and 4.18 per note; and average input tokens were 1.1k, 3.8k, and 6.1k per note for CLEAR, embedding RAG, and full-note approaches, respectively. In conclusion, CLEAR utilizes clinical entities for information retrieval and achieves >70% reduction in token usage and inference time with improved performance compared to modern methods.

---

Data extraction from polymer literature using large language models
10.1038/s43246-024-00708-9
Automated data extraction from materials science literature at scale using artificial intelligence and natural language processing techniques is critical to advance materials discovery. However, this process for large spans of text continues to be a challenge due to the specific nature and styles of scientific manuscripts. In this study, we present a framework to automatically extract polymer-property data from full-text journal articles using commercially available (GPT-3.5) and open-source (LlaMa 2) large language models (LLM), in tandem with the named entity recognition (NER)-based MaterialsBERT model. Leveraging a corpus of ~ 2.4 million full text articles, our method successfully identified and processed around 681,000 polymer-related articles, resulting in the extraction of over one million records corresponding to 24 properties of over 106,000 unique polymers. We additionally conducted an extensive evaluation of the performance and associated costs of the LLMs used for data extraction, compared to the NER model. We suggest methodologies to optimize costs, provide insights on effective inference via in-context few-shots learning, and illuminate gaps and opportunities for future studies utilizing LLMs for natural language processing in polymer science. The extracted polymer-property data has been made publicly available for the wider scientific community via the Polymer Scholar website. Automated data extraction from materials science literature using artificial intelligence and natural language processing techniques is key to advance materials discovery. Here, the authors present a framework to automatically extract polymer-property data from full-text journal articles using commercially available and open-source large language models.

---

AI language models could both help and harm equity in marine policymaking
10.1038/s44183-025-00132-7
AI Large Language Models (LLMs), like GPT, are starting to reshape some aspects of international environmental policymaking; potentially assisting with certain tedious, resource-intensive work like analyzing and drafting policy instruments, building capacity, and aiding public consultation processes. We are cautiously hopeful that LLMs could be used to promote a marginally more balanced footing among decision makers—particularly benefiting developing countries who face capacity constraints that put them at a disadvantage in negotiations. To explore their realistic potentials, limitations, and risks, we present a case study of an AI chatbot for the recently adopted Biodiversity Beyond National Jurisdiction Agreement and critique its answers to key policy questions. While our case study suggests some promising opportunities, it also raises concerns that LLMs could deepen existing inequities. For instance, they may introduce biases by generating text that overrepresents the perspectives of mainly Western economic centers of power, while neglecting developing countries’ viewpoints.

---

Introduction to optimal control, adaptive control and reinforcement learning
10.1049/pbce081e_ch1
In this book, we show how to use RL techniques to unify optimal control and adaptive control. By this we mean that a novel class of adaptive control structures will be developed that learn the solutions of optimal control problems in real time by measuring data along the system trajectories online. We call these optimal adaptive controllers. These optimal adaptive controllers have structures based on the actor-critic learning architecture.

---

Early Detection of Environmental Issues from Social Media using IndoBERT and LDA: Case Study of Pollution and Deforestation in Indonesia
10.1051/e3sconf/202564505005
This study proposes a method for the early detection of environmental issues in Indonesia by leveraging social media data, particularly from Twitter. Environmental problems such as air pollution and deforestation pose serious risks to public health, biodiversity, and economic sustainability. However, traditional monitoring systems are often delayed or limited in coverage. To address this, we combined IndoBERT a pretrained language model for Indonesian for sentiment analysis and entity extraction, with Latent Dirichlet Allocation (LDA) for topic modeling. The dataset, collected using specific keywords related to pollution and deforestation, underwent a rigorous preprocessing pipeline before analysis. Results show that public sentiment is predominantly negative, reflecting strong concerns about air quality and illegal logging. LDA revealed coherent topic clusters, such as haze-related urban pollution and deforestation linked to mining and palm oil expansion. These findings highlight the potential of social media mining as a complementary tool for real-time environmental monitoring. The proposed framework provides actionable insights for policymakers, NGOs, and smart city platforms to detect and respond to emerging environmental threats more proactively.

---

Artificial intelligence in ESG investing: A scoring model for accuracy and accountability
10.1051/shsconf/202522503017
This article meticulously examines the utilization of big data and artificial intelligence (AI) to tackle the significant challenges encountered in ESG (Environmental, Social, and Governance) investments. These challenges primarily include the inconsistent ESG ratings across different rating agencies and the lack of transparency in AI models, which can hinder informed decision-making. The study meticulously constructs an AI-based ESG scoring model by integrating advanced techniques such as natural language processing (NLP), topic modeling, and machine learning. It also proposes a comprehensive explainable AI framework to enhance investment confidence and provide clear insights into the decision-making process. According to the research reports, these techniques significantly boost the effectiveness and granularity of ESG analysis, allowing for more precise and reliable assessments. However, the study acknowledges persistent challenges such as algorithmic bias, data heterogeneity, style issues, and conceptual problems that need to be addressed. It provides some thoughtful recommendations for future research to tackle these issues effectively. This review aims to bolster investor and business confidence and improve the overall performance of ESG investing by promoting more transparent and consistent practices.

---

Revitalising the Regional Languages of France
10.1057/9780230286177_8
Although in decline for years, the RLs have refused to go away, and there is now some state recognition in the shape of teaching subsidies, some presence in the media and cultural events, and also some important institutional innovations. There are a number of reasons for this change of heart. The Algerian war and decolonisation in the late 1950s and early 1960s gave rise to the idea that the French regions had also been the victims of ‘colonisation’ and that what the French state had rejected, namely their language and culture, was no longer to be despised but admired and saved. This was, at least, the view of the strongest political RL militants.

---

Explainable artificial intelligence modeling for corporate social responsibility and financial performance
10.1057/s41260-022-00291-z
In this paper, we examine the relation between corporate social responsibility and corporate financial performance in a bullish market. Previous studies have heterogeneous results, mainly due to differences in the samples and statistical approaches used. To resolve these issues, we use an innovative approach through explainable artificial intelligence (XAI). To reflect the recent expansions of CSR practices, we propose a longitudinal analysis of the US market from 2014–2019. We find that in a bullish market, CSR is negatively related to financial market performance. Through the use of XAI, we show that CSR exclusively improves the financial performance of the most sustainable companies. We also highlight the existence of thresholds that modify the relation between the level of CSR and our financial variables.

---

High-fidelity reconstruction of turbulent flow from spatially limited data using enhanced super-resolution generative adversarial network
10.1063/5.0066077
In this study, a deep learning-based approach is applied with the aim of reconstructing high-resolution turbulent flow fields using minimal flow fields data. A multi-scale enhanced super-resolution generative adversarial network with a physics-based loss function is introduced as a model to reconstruct the high-resolution flow fields. The model capability to reconstruct high-resolution laminar flows is examined using data of laminar flow around a square cylinder. The results reveal that the model can accurately reproduce the high-resolution flow fields even when limited spatial information is provided. The case of turbulent channel flow is used to assess the ability of the model to reconstruct the high-resolution wall-bounded turbulent flow fields. The instantaneous and statistical results obtained from the model agree well with the ground truth data, indicating that the model can successfully learn to map the coarse flow fields to the high-resolution ones. Furthermore, the computational cost of the proposed model, which is examined carefully, is found to be effectively low. This demonstrates that using high-fidelity training data with physics-guided generative adversarial network-based models can be practically efficient in reconstructing high-resolution turbulent flow fields from extremely coarse data.

---

Optimal control for a Formula One car with variable parameters
10.1080/00423114.2014.889315
The minimum-lap-time optimal control problem for a Formula One race car is solved using direct transcription and nonlinear programming. Features of this work include significantly reduced full-lap solution times and the simultaneous optimisation of the driven line, the driver controls and multiple car set-up parameters. It is shown that significant reductions in the driven lap time can be obtained from track-specific set-up parameter optimisation. Reduced computing times are achieved using a combination of a track description based on curvilinear coordinates, analytical derivatives and model non-dimensionalisation. The curvature of the track centre line is found by solving an auxiliary optimal control problem that negates the difficulties associated with integration drift and trajectory closure.

---

Identification and modelling of race driving styles
10.1080/00423114.2021.1930070
A good understanding and modelling of the human driver is essential for modern vehicle development, particularly in motorsports, where the race car should fit its driver perfectly. At the same time, an objective assessment and especially imitation of professional race drivers is difficult due to individual driving styles, complex and non-deterministic decision making processes, and small stability margins. In this paper, we present a holistic approach to identify and model individual race driving styles in a robust way. We develop the Driver Identification and Metric Ranking Algorithm (DIMRA) as a data-based method for an in-depth objective analysis and assessment of professional race drivers. Supported by this knowledge, we extend and adapt the imitation learning framework Probabilistic Modeling of Driver Behavior (ProMoD) in order to model race drivers in a complex simulation environment. An evaluation with data from professional race drivers shows the capability of DIMRA to derive metrics which describe human race driving styles, as well as ProMoD to robustly generate competitive laps with human-like controls in a professional motorsport driving simulator. The ability to identify and imitate individual driving styles does not only support the performance optimisation of race cars but could also aid the development of road cars and driver assistance systems in future work.

---

Revisiting SME default predictors: The Omega Score
10.1080/00472778.2022.2135718
ABSTRACT SME default prediction is a long-standing issue in the finance and management literature. Proper estimates of the SME risk of failure can support policymakers in implementing restructuring policies, rating agencies and credit analytics firms in assessing creditworthiness, public and private investors in allocating funds, entrepreneurs in accessing funds, and managers in developing effective strategies. Drawing on the extant management literature, we argue that introducing management- and employee-related variables into SME prediction models can improve their predictive power. To test our hypotheses, we use a unique sample of SMEs and propose a novel and more accurate predictor of SME default, the Omega Score, developed by the Least Absolute Shrinkage and Selection Operator (LASSO). Results were further confirmed through other machine-learning techniques. Beyond traditional financial ratios and payment behavior variables, our findings show that the incorporation of change in management, employee turnover, and mean employee tenure significantly improve the model’s predictive accuracy. Video Abstract Read the transcript Watch the video on Vimeo © 2022 The Author(s). Published with license by Taylor & Francis Group, LLC.

---

Non-financial indicators for credit risk analysis of Chinese technology-oriented micro and small enterprises
10.1080/01605682.2022.2072781
Abstract Technology-oriented micro and small enterprises (TMSEs) play an important role in technology innovation, employment increase, and economic growth. Due to their high risk and resource consuming nature, the Chinese government has set up special funding, and encouraged banks to provide credit support for TMSEs. However, traditional enterprises’ credit evaluation methods are not suitable for TMSEs due to their characteristics. How to assess the credit risk of TMSEs is a relatively new and challenging topic. This study is to identify novel indicators that can capture the creditrelated characteristics of TMSEs. Specifically, we proposed that innovation capability and business models can be used to identify some special aspects of TMSEs’ credit risk. To validate the new indicators, we compared the performance of traditional financial indicators and the new indicators. The results showed that innovation and business model indicators can effectively improve the performance of classifiers in the credit risk assessment of TMSEs.

---

Validating Automated Essay Scoring: A (Modest) Refinement of the “Gold Standard”
10.1080/08957347.2014.1002920
By far, the most frequently used method of validating (the interpretation and use of) automated essay scores has been to compare them with scores awarded by human raters. Although this practice is questionable, human-machine agreement is still often regarded as the “gold standard.” Our objective was to refine this model and apply it to data from a major testing program and one system of automated essay scoring. The refinement capitalizes on the fact that essay raters differ in numerous ways (e.g., training and experience), any of which may affect the quality of ratings. We found that automated scores exhibited different correlations with scores awarded by experienced raters (a more compelling criterion) than with those awarded by untrained raters (a less compelling criterion). The results suggest potential for a refined machine-human agreement model that differentiates raters with respect to experience, expertise, and possibly even more salient characteristics.

---

Integration of local position-POS awareness and global dense connection for ABSA
10.1080/0952813x.2023.2217811
Aspect-based sentiment analysis (ABSA) is a key problem in text analysis. However, previous work ignores the fact that the joint effects of local and global features affect the classification accuracy. Therefore, an ABSA model based on local position-part of speech (POS) awareness and global dense connection (LPP-GDC) is proposed to fully grasp the information from both local and global features concurrently. First, the BERT pre-trained model is used to obtain the word vectors. Second, position-POS awareness mechanism is designed to focus on words of important POS in the local context. Then, the multi-head self-attention mechanism and a densely connected graph convolutional network are constructed to capture the global information. Finally, the results are obtained by the dynamic feature fusion method. Experiments on three public datasets show that LPP-GDC obtains state-of-the-art performance.

---

Informational Content and Assurance of Textual Disclosures: Evidence on Integrated Reporting
10.1080/09638180.2019.1677486
This paper examines the economic benefits associated with textual attributes and the external assurance of integrated reporting (IR), an innovative form of corporate disclosure that connects financial and environmental, social and governance (ESG) information in a single report. We investigate the setting of South Africa, where IR has been mandatory since 2010 for listed companies. We find that IR readability is associated with a higher market valuation, conciseness with higher stock liquidity and tone bias with less dispersed analysts’ estimates. Results suggest that market participants appreciate IRs that are readable, short and focused, as well as hint at tone management strategies targeting analysts. We also show that assurance on IR moderates the negative effects of poor textual attributes: if firms publish IRs that are difficult to read but assure them, this compensates for the negative influence of reading difficulty on a market value; if long IRs are assured, this dampens the negative effect of verbosity on liquidity; if firms assure IRs, analysts’ forecast dispersion is lower, therefore suggesting that assurance acts as a credibility-enhancing mechanism for external users. Finally, we show that textual attributes and assurance matter for broader audiences interested in the ESG dimensions of a firm’s performance.

---

Does Voluntary ESG Reporting Resolve Disagreement among ESG Rating Agencies?
10.1080/09638180.2022.2088588
US companies are increasingly responding to demand from investors and other stakeholders for transparent information about companies’ environmental, social, and governance (ESG) performance by issuing ESG reports on a voluntary basis. We examine whether these reports help to resolve the previously documented disagreement among ESG rating agencies about individual companies’ ESG performance. Consistent with this possibility, we find that disagreement among ESG rating agencies is lower for firms that voluntarily issue ESG reports. In particular, disclosures about the environmental and social dimensions help reduce disagreement about the company’s performance on those dimensions. Using textual analysis, we find that longer reports are associated with reduced disagreement among ESG raters while reports with more positive tones or that use a greater number of sticky words are associated with heightened disagreement. The association between ESG disclosure and ESG disagreement is more pronounced when firms obtain third-party attestations on their ESG reports, especially from accounting firms, and when firms adhere to advanced levels of Global Reporting Initiative (GRI) reporting standards. Finally, ESG disagreement is positively associated with disagreement and uncertainty in the capital market, providing strong motivation for firms to voluntarily disclose ESG reports to reduce ESG disagreement.

---

Volume and Tone of Environmental Disclosure: A Comparative Analysis of a Corporation and its Stakeholders
10.1080/0969160x.2015.1007465
AbstractIn this paper, we seek to provide a description of disclosures arising from various stakeholder groups in the social sphere. As such, we extend Rodrigue's article “Contrasting Realities: Corporate Environmental Disclosure and Stakeholder-Released Information” (2014. Accounting, Auditing and Accountability Journal 27 [1]: 119–149) by conducting a case study to examine and compare corporate and stakeholder communications made over a three-year period. We specifically focus on the volume and tone of environmental disclosures. We explore the topics emphasised by each party in their respective disclosures and analyse their tone by categorising the information as positive, negative or neutral. Through a detailed quantitative analysis, we highlight the differences in how various actors portray the organisation and its activities. We argue that this is of relevance since neither corporate nor stakeholder disclosures are merely passive reflections of reality, but rather, they play a part in constructing, f...

---

Automatic Essay Assessment
10.1080/0969594032000148154
Abstract Computational techniques for scoring essays have recently come into use. Their bases and development methods raise both old and new measurement issues. However, coming principally from computer and cognitive sciences, they have received little attention from the educational measurement community. We briefly survey the state of the technology, then describe one such system, the Intelligent Essay Assessor (IEA). IEA is based largely on Latent Semantic Analysis (LSA), a machine-learning model that induces the semantic similarity of words and passages by analysis of large bodies of domain-relevant text. IEA's dominant variables are computed from comparisons with pre-scored essays of highly similar content as measured by LSA. Over many validation studies with a wide variety of topics and test-takers, IEA correlated with human graders as well as they correlated with each other. The technique also supports other educational applications. Critical measurement questions are posed and discussed.

---

Framing fracking: scale-shifting and greenwashing risk in the oil and gas industry
10.1080/13549839.2017.1345877
ABSTRACT In this paper, I examine corporate environmental communication on hydraulic fracturing, or fracking, and industry efforts to shape public perception of resource extraction and its impacts on the environment. I look at how the oil and gas industry (OGI) frames fracking to ease public fear by downplaying risk and shifting its scale with rhetoric presenting the benefits of this emergent technology. Contrasting and building on ecological modernisation versus risk society ideas, I use OGI print advertising supplemented by corporate social responsibility statements and other online material to critically evaluate framing in light of the practice of corporate greenwashing. Findings reveal OGI efforts to positively portray fracking in the interest of unfettered resource extraction and profits from energy production. Several themes emerge in OGI framing rhetoric, starting with the establishment of trust through education and claims of transparency and continuing with ideas touting safety and responsibility, scientific progress, economic benefits and jobs, energy security, environmental protection, and sustainability. On the whole, such rhetoric reflects ecological modernisation ideas that shift the perception of risk and its consequences, framing fracking in a way that obscures the negative impacts of dependency on a fossil fuel-based economy.

---

Using natural language processing to analyze unstructured patient-reported outcomes data derived from electronic health records for cancer populations: a systematic review
10.1080/14737167.2024.2322664
Introduction Patient-reported outcomes (PROs; symptoms, functional status, quality-of-life) expressed in the ‘free-text’ or ‘unstructured’ format within clinical notes from electronic health records (EHRs) offer valuable insights beyond biological and clinical data for medical decision-making. However, the comprehensive assessment for utilizing natural language processing (NLP) coupled with machine learning (ML) methods to analyze unstructured PROs and their clinical implementation for individuals affected by cancer remains lacking.

---

The Use of Assistive Technologies Including Generative AI by Test Takers in Language Assessment: A Debate of Theory and Practice
10.1080/15434303.2023.2288256
ABSTRACT Assistive writing tools that provide suggestions for word choice, sentence structure, and grammar correction are allowed as accommodations for students with learning disabilities on a case-by-case basis. These assistive technologies, including generative artificial intelligence (AI) tools, are increasingly accessible to more people than ever and are being utilized for second language classroom instruction and learning. In light of this trend, at a meeting of the Automated Language Assessment (ALA) SIG at the Language Testing Research Colloquium (LTRC) in New York City, a debate took place on the topic of allowing access to assistive technologies including generative AI in language assessment. This commentary, building and expanding on the debate that occurred between two opposing teams who argued for or against allowing students’ access to these assistive technologies during language assessment, extends the exchange of ideas in a written debate. The debate raises issues related to construct definition, scoring and rubric design, validity, fairness, equity, bias and copyright. The debate also speculates on the use of generative AI by test takers at different proficiency levels and different stakes (high vs. low) assessments. The debate ends with thoughts on AI’s impact on language teaching and learning and when access to such technologies might emerge in language assessment. The issues and questions raised in the debate forecast discussions regarding the feasibility of allowing test takers to use assistive technologies including generative AI during language assessment and the extent to which humans interact and collaborate with these new technologies.

---

Fake news detection for sustainable supply chain management using deceptive tones: an application of ML, NLP and Explainable AI
10.1080/17517575.2025.2510350
ABSTRACT This study develops a novel framework to detect deceptive tones related to sustainable supply chain (SSC) in annual disclosure. Using natural language processing, machine learning, and explainable AI, the model extracts 15 linguistic features from annual reports to compute a deceptive score related to SSC. The findings reveal a positive relationship between deceptive scores and increased word count, and therefore, suggest strategic manipulation in SSC narratives. In essence, the present paper develops a robust tool that can be integrated into ERP/CRM systems to enable automated detection of SSC-related misleading claims, making it highly valuable for investors, regulators, and enterprises.

---

Exploring zero-shot and joint training cross-lingual strategies for aspect-based sentiment analysis based on contextualized multilingual language models
10.1080/24751839.2023.2173843
ABSTRACT Aspect-based sentiment analysis (ABSA) has attracted many researchers' attention in recent years. However, the lack of benchmark datasets for specific languages is a common challenge because of the prohibitive cost of manual annotation. The zero-shot cross-lingual strategy can be applied to solve this gap in research. Moreover, previous works mainly focus on improving the performance of supervised ABSA with pre-trained languages. Therefore, there are few to no systematic comparisons of the benefits of multilingual models in zero-shot and joint training cross-lingual for the ABSA task. In this paper, we focus on the zero-shot and joint training cross-lingual transfer task for the ABSA. We fine-tune the latest pre-trained multilingual language models on the source language, and then it is directly predicted in the target language. For the joint learning scenario, the models are trained on the combination of multiple source languages. Our experimental results show that (1) fine-tuning multilingual models achieve promising performances in the zero-shot cross-lingual scenario; (2) fine-tuning models on the combination training data of multiple source languages outperforms monolingual data in the joint training scenario. Furthermore, the experimental results indicated that choosing other languages instead of English as the source language can give promising results in the low-resource languages scenario.

---

Automated Essay Grading using Machine Learning Algorithm
10.1088/1742-6596/1000/1/012030
Essays are paramount for of assessing the academic excellence along with linking the different ideas with the ability to recall but are notably time consuming when they are assessed manually. Manual grading takes significant amount of evaluator’s time and hence it is an expensive process. Automated grading if proven effective will not only reduce the time for assessment but comparing it with human scores will also make the score realistic. The project aims to develop an automated essay assessment system by use of machine learning techniques by classifying a corpus of textual entities into small number of discrete categories, corresponding to possible grades. Linear regression technique will be utilized for training the model along with making the use of various other classifications and clustering techniques. We intend to train classifiers on the training set, make it go through the downloaded dataset, and then measure performance our dataset by comparing the obtained values with the dataset values. We have implemented our model using java.

---

Large language models in climate and sustainability policy: limits and opportunities
10.1088/1748-9326/addd36
Abstract Accurate, reliable and updated information support effective decision-making by reducing uncertainty and enabling informed choices. Multiple crises threaten the sustainability of our societies and pose at risk the planetary boundaries, hence requiring usable and operational knowledge. Natural-language processing (NLP) tools facilitate data collection, extraction and analysis processes. They expand knowledge utilization capabilities by improving access to reliable sources in shorter time. They also identify patterns of similarities and contrasts across diverse contexts. We apply general and domain-specific large language models (LLMs) to two case studies and we document appropriate uses and shortcomings of these tools for two tasks: classification and sentiment analysis of climate and sustainability documents. We study both statistical and prompt-based methods. In the first case study, we use LLMs to assess whether climate pledges trigger cascade effects in other sustainability dimensions. In the second use case, we use LLMs to identify interactions between the Sustainable Development Goals and detects the direction of their links to frame meaningful policy implications. We find that LLMs are successful at processing, classifying and summarizing heterogeneous text-based data helping practitioners and researchers accessing. LLMs detect strong concerns from emerging economies in addressing food security, water security and urban challenges as primary issues. Developed economies, instead, focus their pledges on the energy transition and climate finance. We also detect and document four main limits along the knowledge production chain: interpretability, external validity, replicability and usability. These risks threaten the usability of findings and can lead to failures in the decision-making process. We recommend risk mitigation strategies to improve transparency and literacy on AI methods applied to complex policy problems. Our work presents a critical but empirically grounded application of LLMs to climate and sustainability questions and suggests avenues to further expand controlled and risk-aware artificial intelligence-powered computational social sciences.

---

Sustainability challenges confronting business
10.1093/hebz/9780199642984.003.0003
This chapter studies the sustainability challenges affecting business. It begins with the history of how attitudes to the environment have changed over time, and how sustainability management has shifted as a result. It then considers the idea of a resource-constrained economy (RCE) as something unique since the onset of industrialization, and something that has profound implications for economic prosperity and business success. The chapter then looks at what life in a RCE looks like with reference to energy, water, food, and poverty. It concludes by discussing how organization of a RCE could differ in terms of politics, the law, consumer behaviour, and the role of companies.

---

Automated Classification for Open-Ended Questions with BERT
10.1093/jssam/smad015
Abstract Manual coding of text data from open-ended questions into different categories is time consuming and expensive. Automated coding uses statistical/machine learning to train on a small subset of manually-coded text answers. Recently, pretraining a general language model on vast amounts of unrelated data and then adapting the model to the specific application has proven effective in natural language processing. Using two data sets, we empirically investigate whether BERT, the currently dominant pretrained language model, is more effective at automated coding of answers to open-ended questions than other non-pretrained statistical learning approaches. We found fine-tuning the pretrained BERT parameters is essential as otherwise BERT is not competitive. Second, we found fine-tuned BERT barely beats the non-pretrained statistical learning approaches in terms of classification accuracy when trained on 100 manually coded observations. However, BERT’s relative advantage increases rapidly when more manually coded observations (e.g., 200–400) are available for training. We conclude that for automatically coding answers to open-ended questions BERT is preferable to non-pretrained models such as support vector machines and boosting.

---

Small and medium enterprises and industrial clusters in BRICS countries
10.1093/oso/9780198827535.003.0008
Industrial clusters have a prominent role in the innovation system and territorial development of many developed and emerging countries, including BRICS. They represent an important variable not only in terms of GDP growth but also in employment creation and poverty reduction, becoming potential drivers for the country’s development. Given the complexity of the process underlying cluster formation, this chapter analyses the evolution of industrial clusters of small and medium enterprises in BRICS countries in the last two decades. Particular focus is dedicated to the main endogenous and exogenous drivers of change and how these concurred in shaping different outcomes in each country, also in terms of sustainable human development.

---

The Legal Evolution of Corporate Sustainability Reporting: Enhancing Transparency and Reducing Information Asymmetry from the Non-Financial Reporting Directive to the Corporate Sustainability Reporting Directive
10.1093/yiel/yvaf004
Abstract This article explains the legal evolution of sustainability reporting in the European Union, tracing the journey from the Accounting Directive to the Corporate Sustainability Reporting Directive and the Non-Financial Reporting Directive. The article examines how each legal instrument has revolutionized corporate obligations and improved transparency on environmental and social governance. A specific focus is placed on the principle of double materiality, the role of the European Sustainability Reporting Standards in ensuring harmonization between member states, and the supervisory arrangements for ensuring effective application and accountability.

---

Mismatch between human early visual cortex and perception in spatial extent representation: Radial bias shapes cortical representation while co-axial bias shapes perception
10.1101/2023.02.16.528416
An object occupies an enclosed region in the visual field, which defines its spatial extent. Humans display exquisite finesse in spatial extent perception. Recent series of human neuroimaging and monkey single-cell studies suggest the spatial representation encoded in the early visual cortex (EVC) as the neural substrate of spatial extent estimation. Guided by this “EVC hypothesis” on spatial extent estimation, we predicted that human estimation of spatial extents would reflect the topographic biases known to exist in EVC’s spatial representation, the co-axial and radial biases. To test this prediction, we concurrently assessed those two spatial biases in both EVC’s and perceptual spatial representations by probing the anisotropy of EVC’s population receptive fields, on the one hand, and that of humans’ spatial extent estimation, on the other hand. To our surprise, we found a marked topographic mismatch between EVC’s and perceptual representations of oriented visual patterns, the radial bias in the former and the co-axial bias in the latter. Amid this topographic mismatch, the extent to which the anisotropy of spatial extents is modulated by stimulus orientation is correlated across individuals between EVC and perception. Our findings seem to require a revision of the current understanding of EVC’s functional architecture and contribution to visual perception: EVC’s spatial representation (i) is governed by the radial bias but only weakly modulated by the co-axial bias, and (ii) do contribute to spatial extent perception, but in a limited way where additional neural mechanisms are called in to counteract the radial bias in EVC. Significant statement Previous anatomical and functional studies suggest both radial and co-axial biases as topographic factors governing the spatial representation of the early visual cortex (EVC). On the other hand, EVC’s fine-grained spatial representation has been considered the most plausible neural substrate for exquisite human perception of spatial extents. Based on these suggestions, we reasoned that these two topographic biases are likely to be shared between EVC’s and perceptual representations of spatial extents. However, our neuroimaging and psychophysics experiments implicate a need for revising those two suggestions. Firstly, the co-axial bias seems to exert only a modulatory influence on EVC’s functional architecture. Secondly, human spatial extent perception requires further contribution from neural mechanisms that correct EVC’s spatial representation for its radial bias.

---

Data Extraction for Evidence Synthesis Using a Large Language Model: A Proof-of-Concept Study
10.1101/2023.10.02.23296415
Data extraction is a crucial, yet labor‐intensive and error‐prone part of evidence synthesis. To date, efforts to harness machine learning for enhancing efficiency of the data extraction process have fallen short of achieving sufficient accuracy and usability. With the release of large language models (LLMs), new possibilities have emerged to increase efficiency and accuracy of data extraction for evidence synthesis. The objective of this proof‐of‐concept study was to assess the performance of an LLM (Claude 2) in extracting data elements from published studies, compared with human data extraction as employed in systematic reviews. Our analysis utilized a convenience sample of 10 English‐language, open‐access publications of randomized controlled trials included in a single systematic review. We selected 16 distinct types of data, posing varying degrees of difficulty (160 data elements across 10 studies). We used the browser version of Claude 2 to upload the portable document format of each publication and then prompted the model for each data element. Across 160 data elements, Claude 2 demonstrated an overall accuracy of 96.3% with a high test–retest reliability (replication 1: 96.9%; replication 2: 95.0% accuracy). Overall, Claude 2 made 6 errors on 160 data items. The most common errors (n = 4) were missed data items. Importantly, Claude 2's ease of use was high; it required no technical expertise or labeled training data for effective operation (i.e., zero‐shot learning). Based on findings of our proof‐of‐concept study, leveraging LLMs has the potential to substantially enhance the efficiency and accuracy of data extraction for evidence syntheses.

---

Universality of representation in biological and artificial neural networks
10.1101/2024.12.26.629294
Many artificial neural networks (ANNs) trained with ecologically plausible objectives on naturalistic data align with behavior and neural representations in biological systems. Here, we show that this alignment is a consequence of convergence onto the same representations by high-performing ANNs and by brains. We developed a method to identify stimuli that systematically vary the degree of inter-model representation agreement. Across language and vision, we then showed that stimuli from high-and low-agreement sets predictably modulated model-to-brain alignment. We also examined which stimulus features distinguish high-from low-agreement sentences and images. Our results establish representation universality as a core component in the model-to-brain alignment and provide a new approach for using ANNs to uncover the structure of biological representations and computations.

---

A process-based operational framework for sustainability reporting in SMEs
10.1108/14626001211277460
Purpose – The paper aims to support small and medium enterprises (SMEs) in overcoming the difficulties they encounter in initiating sustainability reporting, proposing a “general” process for identifying a standard set of key sustainability indicators, that is specifically tailored on SMEs characteristics.Design/methodology/approach – The paper moves from the analysis of the existing international frameworks for sustainability reporting, and it discusses the main motivations whereby they are hardly applicable for SMEs. Then, it proposes a theoretical approach, which derives the set of key sustainability indicators starting from GRI, but explicitly considering SMEs specificities.Findings – The paper presents and discusses the application of the proposed approach to a network of Italian steel SMEs.Practical implications – The approach proposed can support SMEs to face the problems they face in implementing sustainability reporting (resource and capability constraints and lack of operative instruments to sup...

---

Sustainability reports as simulacra? A counter-account of A and A+ GRI reports
10.1108/aaaj-04-2012-00998
Purpose - – The purpose of this paper is to examine the extent to which sustainability reporting can be viewed as a simulacrum used to camouflage real sustainable-development problems and project an idealized view of the firms' situations. Design/methodology/approach - – The method was based on the content analysis and counter accounting of 23 sustainability reports from firms in the energy and mining sectors which had received application levels of A or A+ from the Global Reporting Initiative (GRI). The information disclosed in some 2,700 pages of reports was structured around 92 GRI indicators and compared with 116 significant news events that clearly addressed the responsibility of these firms in sustainable development problems. Moreover, the 1,258 pictures included in sustainability reports were categorized into recurring themes from an inductive perspective. Findings - – A total of 90 per cent of the significant negative events were not reported, contrary to the principles of balance, completeness and transparency of GRI reports. Moreover, the pictures included in these reports showcase various simulacra clearly disconnected with the impact of business activities. Originality/value - – The paper shows the relevance of the counter accounting approach in assessing the quality of sustainability reports and question the reliability of the GRI's A or A+ application levels. It contributes to debates concerning the transparency of sustainability reports in light of Debord's and Baudrillard's critical perspective. The paper reveals the underexplored role of images in the emergence of several types of simulacra.

---

Management by objectives and corporate social responsibility disclosure: First results from Italy
10.1108/aaaj-09-2013-1480
Purpose - – The purpose of this paper is to examine the relationship between remuneration for the achievement of objectives and sustainability, and – more specifically – the amount of attention that listed companies in Italy devote to defining, and consequently to communicating externally, sustainability as a criterion in establishing the wage levels of managers and directors. Design/methodology/approach - – It was decided to ascertain whether the quality of information regarding sustainability provided in connection with the remuneration policies of listed companies tallies with the general quality of information regarding sustainability provided through companies’ main (obligatory and voluntary) reporting procedures. Findings - – The results of this research show that the inconsistency between the information provided in voluntary and obligatory reports (between reports on sustainability and remuneration reports) extends to the levels of information provided in the two types of obligatory report (the reports on remuneration and on management); there is also a discrepancy between the levels of information provided in these reports and the evaluation of that information by an external assessor. Research limitations/implications - – One of the limitations of this research is that as the data examined were gleaned from public documents, it is not necessarily an accurate reflection of all the information that firms have at their disposal on questions of sustainability and remuneration policies. The existence of internal documents containing other information, and therefore leading to different results, cannot be ruled out. Originality/value - – This study is the first in Italy to examine the question of how limited companies report issues relating to management by objectives-corporate social responsibility. It does this through the introduction of a mixed system for ESG information, which counteracts the subjective limitations of the internal evaluation provided by the research group by adding in the authoritative evaluations of an external assessor.

---

Renovation in environmental, social and governance (ESG) research: the application of machine learning
10.1108/ara-07-2023-0201
PurposeEnvironmental, social and governance (ESG) factors have become increasingly important in investment decisions, leading to a surge in ESG investing and the rise of sustainable investment assets. Nevertheless, challenges in ESG disclosure, such as quantifying unstructured data, lack of guidelines and comparability, rampantly exist. ESG rating agencies play a crucial role in assessing corporate ESG performance, but concerns over their credibility and reliability persist. To address these issues, researchers are increasingly utilizing machine learning (ML) tools to enhance ESG reporting and evaluation. By leveraging ML, accounting practitioners and researchers gain deeper insights into the relationship between ESG practices and financial performance, offering a more data-driven understanding of ESG impacts on business communities.Design/methodology/approachThe authors review the current research on ESG disclosure and ESG performance disagreement, followed by the review of current ESG research with ML tools in three areas: connecting ML with ESG disclosures, integrating ML with ESG rating disagreement and employing ML with ESG in other settings. By comparing different research's ML applications in ESG research, the authors conclude the positive and negative sides of those research studies.FindingsThe practice of ESG reporting and assurance is on the rise, but still in its technical infancy. ML methods offer advantages over traditional approaches in accounting, efficiently handling large, unstructured data and capturing complex patterns, contributing to their superiority. ML methods excel in prediction accuracy, making them ideal for tasks like fraud detection and financial forecasting. Their adaptability and feature interaction capabilities make them well-suited for addressing diverse and evolving accounting problems, surpassing traditional methods in accuracy and insight.Originality/valueThe authors broadly review the accounting research with the ML method in ESG-related issues. By emphasizing the advantages of ML compared to traditional methods, the authors offer suggestions for future research in ML applications in ESG-related fields.

---

An English-for-specific-purposes motivated analysis of corporate sustainability reports: An analysis of text and context
10.1108/ccij-10-2018-0111
The purpose of this paper is to shed light on corporate sustainability reports (CSRs). In doing so, a number of rhetorical moves were identified in such reports and the rhetorical purposes of those moves were investigated. The findings helped understand the corporations’ eco-ideologies and priorities.,A total of 12 CSRs from laptop-manufacturing companies were chosen. The CSRs then underwent data coding which led to the identification of rhetorical moves and sub-moves. The identified moves were then analyzed contextually to interpret the corporations’ larger-scale ideologies.,The findings identified a number of rhetorical moves. For instance, the corporations were shown to stress issues, such as resource management and waste management, in the CSRs. In addition, linguistic analysis of the CSRs indicated that the companies accepted their share in environmental issues and aimed to address such issues.,The present study is the first known attempt at analyzing the CSRs issues by laptop manufacturers. While some previous studies (Mason and Mason, 2012) have investigated the CSRs issued by a wide array of companies, no existing study has focused on tech companies.

---

Public attention and “Environmental Disclosure Greenwashing”: pressure from oversight or incentive from legitimacy
10.1108/cfri-02-2024-0079
PurposeThis study aims to investigate whether public attention influences corporate decisions on environmental disclosure, thereby revealing how society perceives and understands environmental issues and how corporations respond to these expectations.Design/methodology/approachWe selected publicly listed Chinese firms as our sample. An “Environmental Disclosure Greenwashing” (EDG) Index was developed through textual analysis of their annual reports using natural language processing. Financial data were obtained from the CSMAR database, and multivariate regression was used for analysis.FindingsThe impact of public attention on EDG primarily manifests as an oversight pressure effect rather than a legitimacy incentive effect. As public attention intensifies, firms tend to adopt more substantial environmental actions instead of merely symbolic environmental disclosures. Formal regulatory frameworks might inadvertently trigger corporate EDG, but public attention can correct the adverse effects possibly introduced by formal regulations. Notably, in firms facing lower institutional pressure, the influence of public attention is more pronounced.Practical implicationsThe evidence suggests that public attention reduces corporate EDG. These findings have significant implications for the regulation of environmental disclosures among firms in emerging economies.Originality/valueThe study integrates research in environmental disclosure with the concept of “greenwashing”, unveiling the limitations of the “disclosure as governance” viewpoint. It elucidates the impact of an informal external oversight mechanism (i.e. public attention) on complex corporate environmental disclosure decisions.

---

Interests of Third Parties: the extent to which government should keep information secret for the sake of third parties
10.1108/eb027049
We are all third parties, at least in the sense that government rarely justifies keeping information secret for reasons other than our sake. Although most of the third parties to be considered in this session are those who have supplied information to government, it should be remembered that there are other third parties with interests to be considered. Some of these are people (and companies) about whom information is held, rather than from whom information is received.

---

An integrated framework for aspect category-based sentiment analysis using adaptive feature selection and category-aware decision fusion strategies
10.1108/el-09-2024-0270
Purpose Aspect category-based sentiment analysis (ACSA) has been widely used in consumer preference mining and marketing strategy formulation. However, existing studies ignore the variability in features and the intrinsic correlation among diverse aspect categories in ACSA tasks. To address these problems, this paper aims to propose a novel integrated framework. Design/methodology/approach The integrated framework consists of three modules: text feature extraction and fusion, adaptive feature selection and category-aware decision fusion. First, text features from global and local views are extracted and fused to comprehensively capture the potential information in the different dimensions of the review text. Then, an adaptive feature selection strategy is devised for each aspect category to determine the optimal feature set. Finally, considering the intrinsic associations between aspect categories, a category-aware decision fusion strategy is constructed to enhance the performance of ACSA tasks. Findings Comparative experimental results demonstrate that the integrated framework can effectively detect aspect categories and their corresponding sentiment polarities from review texts, achieving a macroaveraged F1 score (Fmacro) of 72.38% and a weighted F1 score (F1) of 79.39%, with absolute gains of 2.93% to 27.36% and 4.35% to 20.36%, respectively, compared to the baselines. Originality/value This framework can simultaneously detect aspect categories and corresponding sentiment polarities from review texts, thereby assisting e-commerce enterprises in gaining insights into consumer preferences, prioritizing product improvements, and adjusting marketing strategies.

---

Audit quality, media coverage, environmental, social, and governance disclosure and firm investment efficiency
10.1108/ijaim-03-2019-0041
Purpose - The purpose of this study is twofold: first, to introduce two determinants of environmental, social and governance (ESG) disclosure transparency, namely, audit quality and public media exposure; and second, to investigate the impact of ESG transparency on firm-level investment efficiency. Design/methodology/approach - Ordinary least square (OLS) regressions are applied to explore the relationship between the two variables of interest (audit quality and public media exposure) and ESG transparency on a sample of publicly listed Canadian firms during the period 2008 to 2017. Then, an econometric model is used to investigate the association between ESG transparency and investment efficiency under two identified scenarios, under-investment and over-investment. Findings - Results show that audit quality and public media exposure are two main drivers of ESG transparency, hence, commitment to high-quality audits and exposure to high public media coverage drive firms to disclose more extensive and transparent ESG information. The authors also find a negative association between ESG transparency and firm-level investment inefficiency. Thus, ESG transparency generates influential incremental information that helps mitigate the information asymmetry between firms and stakeholders while fostering better resource allocation through investment efficiency. Originality/value - This study contributes to the corporate social responsibility (CSR) and ESG literature by identifying audit quality and public media exposure as two determinants of ESG transparency; and by noting that higher ESG transparency has a significant economic effect on capital investment decisions through higher firm-level investment efficiency.

---

Environmental-specific servant leadership as a strategic tool to accomplish environmental performance: a case of China
10.1108/ijm-07-2020-0350
PurposeThe Chinese firms are keenly focused on reducing their environmental footprints as part of the competitive strategy. Within the context of sustainable organizations in China, we test a multilevel framework that examined the impact of environmental-specific servant leadership on the green individual (pro-environmental behavior) and team (project green performance) outcomes within projects. Using social identity theory, we theorize and test the mediating role of green self-identity (individual level) and team green identification (team level) in the relationships between environmental-specific servant leadership, pro-environmental behavior and project green performance.Design/methodology/approachWe used survey questionnaires to collect multi-level and multi-wave data from 42 ongoing project-based sustainable organisations in China. The multilevel team to individual-level hypothesis were analyzed using multilevel-modeling via Mplus, while team level hypotheses were tested using ordinary least squares regression.FindingsThe multilevel regression analysis showed that environmental-specific servant leadership has a trickle-down effect of green self-identity, which subsequently predicts pro-environmental behavior. The ordinary least squares regression results demonstrated that environmental-specific servant leadership predicts project green performance via team green identification. Also, environmental-specific servant leadership has a positive and direct impact on pro-environmental behavior and project green performance.Research limitations/implicationsWe offer community and service dimension of leadership as a determinant of environmental performance at multiple levels. We provide managerial and policy implications to Chinese organizations striving to reposition themselves as eco-friendly organizations both nationally and globally.Originality/valueThe study is among the first to understand the role of environmental-specific servant leadership in predicting individual-level and team-level environment-related mediator and outcomes simultaneously.

---

Governance of ESG implementations: governance dimensions and their structural implementation
10.1108/ijmpb-05-2024-0107
PurposeThis study identifies the various governance dimensions for environmental, social and governance (ESG) implementations, including reporting. Subsequently, it investigates the governance structures in place to steer these dimensions in project-based and project-oriented organizations.Design/methodology/approachA systematic literature review identifies 11 organizational governance dimensions for ESG implementations, followed by a conceptual mapping of these dimensions to the most likely governance structures being set up for their implementation (i.e. single-level, multi-level and polycentric governance).FindingsEleven governance dimensions are identified and categorized under (1) organizational settings, (2) ESG strategy and (3) implementation. The conceptual mapping of these dimensions against the governance structures for their implementation identifies an inverse relationship between the governance level in the organizational hierarchy and the complexity of governance structures needed for steering these dimensions. The paper suggests a variety of context-dependent governance structures and contributes to the governance literature on the interface between projects and their parent organizations.Research limitations/implicationsAcademics benefit from an organization-wide model and the first taxonomy on the relevant governance dimensions for ESG implementation and reporting projects, thus a first approach to theorizing the governance of ESG implementations.Practical implicationsThe results are of value for practitioners by allowing them to understand the diversity of dimensions and the structural implementation of ESG and its reporting.Social implicationsOne of the first studies to address governance of ESG implementation and reporting across intra-organizational boundaries between the permanent and the project-based parts of the organization. This provides for organization-wide improvements in the governance toward the UN Sustainability Goals.Originality/valueThe paper investigates the under-researched link of governance implementations from the corporate level to individual projects in the context of ESG implementations, including reporting.

---

Do environment, social and governance performance impact credit ratings: a study from India
10.1108/ijoes-09-2018-0130

Purpose
The purpose of this study is to determine the impact of environment, social and governance (ESG) disclosure on credit ratings of companies in India.


Design/methodology/approach
Firms under study are listed on the Bombay Stock Exchange (BSE) 500 and represent almost 93 per cent of the total market capitalization on BSE. This study considers a sample of 122 firms from a population of 500 to examine the relationship between ESG scores and Credit Rating. The scope of this study is confined to those firms listed on the S&P BSE 500 which have made ESG disclosures and were rated by various credit rating agencies like Crisil, ICRA and CARE. Data were sourced from Bloomberg. Ratings were given in ascending order. In the first model, credit rating was used as predicted variable; ESG score as predictor variable and market capitalization, net debt to equity, and total debt to asset as control considering the ordered nature of dependent variable in the study, ordered logistic regression was applied. It was repeated taking individual scores on environment rating, social rating and governance rating as predictors. The authors further segregated the 122 selected firms into large, medium and low capital firms and assessed separate logistic regression models taking credit rating as the predicted variable and overall ESG score as the predictor.


Findings
It was found that overall ESG performance and performance of individual components (environment, social and financial variables such as market capitalization, and debt to equity ratio) had significant positive indicators of creditworthiness as measured through credit rating. Governance score had a positive and insignificant relation with credit rating. Market capitalization was observed to have significant direct relationship with credit worthiness. On the other hand, number of independent directors in companies showed significant inverse relationship with creditworthiness. ESG significantly impacted credit rating in the desired direction only for small- and middle-level firms; for large firms which already had higher credit rating, ESG showed no effect. It was also found that credit rating itself determined significantly the extent of overall ESG reporting and disclosure of its components.


Originality/value
This is unique study that covers the aspects of ESG reports and its impact on credit rating.


---

Metrics for measuring industrial sustainability performance in small and medium-sized enterprises
10.1108/ijppm-04-2022-0200
PurposeThis paper aims to identify and empirically analyze useful and applicable metrics for measuring and managing the sustainability performance of small and medium-sized enterprises (SMEs).Design/methodology/approachTo achieve the objective of the paper, potential metrics were adopted from previous research related to industrial sustainability and an empirical analysis was carried to assess the applicability of the metrics by collecting empirical data from Italian footwear SMEs using a structured questionnaire. The SMEs were selected using a convenience sampling method.FindingsThe results of the within-case analysis and the cross-case analysis indicate that the majority of the metrics were found to be useful and applicable to each of the SMEs and across the SMEs, respectively. These metrics emphasized measuring industrial sustainability performance related to financial benefits, costs and market competitiveness for the economic sustainability dimension; resources for the environmental sustainability dimension; and customers, employees and the community for the social sustainability dimension.Research limitations/implicationsApart from the within-case analysis and cross-case analysis, it was not possible to conduct statistical analysis since a small number of SMEs were accessible to collect empirical data.Originality/valueThe findings of the paper have considerable academic, managerial and policy implications and will provide a theoretical basis for future research on measuring and managing industrial sustainability performance. By providing a set of empirically supported metrics based on the triple bottom line approach (i.e. economic, environmental and social metrics), this paper contributes to the existing knowledge in the field of industrial sustainability performance measurement.

---

Small business social responsibility: the CSR4UTOOL web application
10.1108/jaar-11-2014-0122

Purpose
Small and medium-sized enterprises (SMEs) positively represent the backbone of the European economic system and, negatively, they make a significant contribution to environmental pollution and social impacts. The purpose of this paper is to discuss how a new tool for supporting the adoption of small business social responsibility (SBSR) can be tailored in order to be more effective.


Design/methodology/approach
Theoretical framework – a literature analysis of how SMEs address sustainability issues will be discussed, especially the need for SME tools with the following features: quality, materiality, free of charge, anonymous, easy-to-use and easy to understand. This need has been confirmed by the literature of SBSR and also social accounting doctrine. Methodology – this paper analyzes CSR4UTOOL, an applied research project of a web-based application dedicated to SMEs in order to provide a self-assessment on sustainability performances. A technical paper aims to describe a specific development, technique or procedure that is capable of presenting a software tool or an experimental or computational method that should be well-tested and should preferably provide a solution to a problem and have some demonstrable practical values.


Findings
The recent studies over SBSR sustain the need of sustainability management tool for SMEs. A web-based tool co-designed and co-created with SMEs reveals the importance of new tools to support SBSR, especially for the global process of CSR downstreaming using a bottom-up perspective.


Research limitations/implications
Alongside the development of tailored, efficient tools to implement, manage, control and account for such social responsibilities, several authors have described organizational implications, managerial adaptations and practical features of such tools. As a technical paper, the aim of this research is to shed light on the new technology that has been adopted to solve a gap in both the literature and the real economy, and it answers to the specific call for SBSR tools.


Practical implications
CSR4UTOOL is based on a decision tree structure presenting several questions and indicators given by a profound benchmark of the most internationally well-known accountability standards. It translates accounting and non-accounting data as well as qualitative and quantitative data to a powerful source of information for entrepreneurs regarding their ethical and moral managerial conduct. A report on the performance of the company will be generated at the end of each session.


Social implications
This study has a practical social implication as it may concretely help entrepreneurs to self-assess their SBSR. IT technologies may represent a useful way to overcome the specific constraints on SMEs that when they are on track, they may also be more aware of the existent power of formalization in their social and environmental management practices. This kind of tool may help small entrepreneurs to be more aware of the fact that formalization may represent a strategic asset rather than a guarantee of morality.


Originality/value
This paper contributes to the literature by demonstrating how data collection over SBSR may be engineered and tailored using web application and ICT, even in a context of small businesses. This paper further responds to a call from Spence (2016) and Johnson et al. (2016) for interdisciplinary approaches to investigating sustainability in SMEs, the validation and application of existent theories on CSR within the context of SMEs, through foregrounding use of innovative software and web applications.


---

The impact of social, environmental and corporate governance disclosures on firm value: Evidence from Egypt
10.1108/jaee-08-2017-0079
The purpose of this paper is to examine the impact of environmental, social, and governance (ESG) practices disclosure and firm value in the Egyptian context. This is done through investigating the influence of being listed and ranked in the Egyptian Corporate Responsibility Index on firm value during the period starting from 2007 to 2016.,Using univariate and multivariate analyses, the findings support the economic benefits of ESG disclosures.,The authors find that firms listed in the ESG index have higher firm value, and that there is a positive association between firms’ higher rankings in the index and firm value, as measured by Tobin’s q.,The findings provide feedback to regulators and standard-setters in the developing countries, and more specifically the Egyptian regulators, on the benefits associated with the introduction of the sustainability index (Standard & Poor’s (S&P)/EGX ESG index). This, in turn, clarifies how the government’s efforts to promote ESG provide benefits to publicly traded firms.,By linking ESG to firm value, the ESG index will enable investors to take a leading role in inducing firms to enhance transparency and disclosure, and hence, improving their reporting standards. This, in turn, will ultimately result in improving sustainability and governance practices in Egypt.,The reported positive market reactions to social and governance practices disclosures can motivate firms to improve their social and governance performance.,The study contributes to the literature by addressing the combined economic effects of social and governance disclosures on firm value, and by investigating the economic effects of such disclosures on firm value in an emerging market.

---

Navigating the ESG seascape: media sentiment toward ESG and corporate strategies
10.1108/jal-04-2024-0057
PurposeThis study investigates how corporations navigate the increasingly prominent field of environmental, social and governance (ESG) through the lens of resource dependence theory (RDT). It aims to elucidate the strategic responses of companies to media-driven public sentiment on ESG, examining the alignment of their operations and competitive strategies – specifically differentiation and cost leadership – to the external resource of media ESG sentiment.Design/methodology/approachEmploying Python software, this research extracted over two million ESG-related news articles from Baidu News. Using machine learning and text analysis, the study assesses the media ESG sentiment and its correlation with the competitive strategies of China’s A-share listed companies over a period from 2007 to 2022. The approach leverages RDT to understand how firms adjust their strategies in response to media-driven public sentiment on ESG.FindingsThe findings indicate that positive media ESG sentiment acts as a crucial external resource, significantly influencing firms’ strategic alignment toward minimizing ESG public sentiment risks and enhancing competitive positioning, especially in the social (S) and governance (G) domains. This alignment is evident in firms’ adoption of differentiation and cost leadership strategies, affirming the study’s theoretical prediction within the RDT framework.Originality/valueThis paper provides a novel contribution by integrating RDT with the analysis of media-driven ESG sentiment to explore corporate strategic adjustments. It offers empirical evidence on the theory’s applicability in contemporary strategic corporate management, particularly in the context of ESG challenges. The research deepens the understanding of the interplay between media ESG sentiment and corporate strategy, highlighting the strategic importance of positive media sentiment in the ESG landscape.

---

Is corporate disclosure of environmental performance indicators reliable or biased information? A look at the underlying drivers
10.1108/jfra-02-2020-0027

Purpose
This study aims to examine the disclosure determinants of environmental performance indicators (EPIs) for a sample of US firms to understand if these disclosures are reliable or whether they are biased towards the reporting of positive information.


Design/methodology/approach
The study uses a panel data analysis to examine the association between firms’ EPIs disclosures and their environmental performances, and other economic and legitimacy factors.


Findings
The results show that firms’ disclosures are not associated with the level of environmental performance and that firms continue to provide EPI information even if they witness a decline in their environmental performance. The evidence suggests that firms’ environmental disclosures are reliable and indicative of their environmental performance.


Practical implications
The findings suggest that mandating EPI disclosures may increase the level of the information reported and reduce firms’ discretion over the disclosure of such information.


Originality/value
Reporting of EPIs is directly linked to firms’ environmental performances. By examining the association between EPI disclosures and environmental performance, the study contributes to the ongoing debate about firms’ reporting and whether it is informative to its stakeholders or whether firms use this type of information to legitimize their operations and portray it in a positive light.


---

Do media coverage and audit quality of US companies affect their Environmental, Social and Governance transparency?
10.1108/jfra-09-2022-0353

Purpose
The purpose of this paper was to study the direct impact of audit quality on environmental, social and governance (ESG) transparency. It aimed also to investigate the moderating effect of media coverage on the relationship between audit quality and ESG transparency in the USA.


Design/methodology/approach
The sample consisted of US companies listed in the Standard and Poor’s 500 Stock Index between 2010 and 2019. The Thomson Reuters database was used to collect ESG disclosure scores and governance information. The authors applied multiple panel data regressions.


Findings
The results showed that audit quality has a direct positive effect on ESG transparency. The findings also showed that the high exposure to public media by firms, the more they commit to high audit quality leading to disclose more transparent ESG information.


Research limitations/implications
The results illustrated the significance of an external audit on an organization’s ESG report. Second, improving data quality has significant consequences not only for rating agencies but also for investors, businesses and researchers. These steps are required to increase the information content of ESG ratings.


Originality/value
The findings demonstrated that third-party external verification improves the dependability of nonfinancial reporting, hence bridging the confidence gap between corporations and the market regarding sustainability reporting.


---

Corporate responses to sustainability issues: are they rhetorical?
10.1108/jfra-11-2024-0845
Purpose This study aims to examine the rhetorical strategies used by the top 50 large-cap companies on the Indonesia Stock Exchange in their voluntary sustainability reports (2006–2023), focusing on their alignment with institutional theory’s concept of decoupling. Design/methodology/approach An index adapted from the frameworks of Benoit (1995), Bolino and Turnley (2003) and Shrives and Brennan (2017) was used in a content analysis to evaluate corporate rhetorical strategies. The thematic analysis further explored the perspectives of sustainability reporting managers through semi-structured interviews. Findings Content analysis of 144 sustainability reports reveals that bolstering is the most prevalent rhetorical strategy, while simple denial is the least used. Thematic analysis of interviews with sustainability reporting managers confirms that these rhetorical responses are primarily symbolic, strategically used to protect and enhance corporate reputation. Research limitations/implications The study is limited to large-cap Indonesian companies and relies on publicly disclosed reports and interviews. Practical implications The findings provide insights for companies to enhance transparency in sustainability communication, assisting stakeholders in critically evaluating corporate sustainability claims to foster trust and accountability. Social implications This study helps stakeholders critically assess corporate sustainability commitments, highlighting the need for stronger oversight and genuine accountability. Originality/value The novelty lies in reclassifying rhetorical strategies from Benoit (1995), Bolino and Turnley (2003) and Shrives and Brennan (2017) into a unified framework, offering fresh insights into symbolic sustainability communication.

---

Corporate Sustainability Reporting Directive – engagement of SMEs into the EU multilevel sustainability enforcement system
10.1108/jgr-04-2025-0126

 
 This study aims to explore the role of small and medium-sized enterprises (SMEs) in the evolving landscape of sustainability reporting within the European Union (EU). It focuses on the indirect but significant implications of the Corporate Sustainability Reporting Directive (CSRD) for SMEs, particularly through their integration into value chains and the multilevel sustainability enforcement system established by the EU.
 
 
 
 This study uses a doctrinal legal analysis of the CSRD and related EU regulations, combined with a systematic review of current academic and regulatory literature. Particular attention is given to the proportionality principle and the role of digital technologies in facilitating or hindering compliance by resource-constrained SMEs.
 
 
 
 This research finds that while the CSRD offers SMEs potential benefits, such as enhanced competitiveness, investment readiness and alignment with future regulatory expectations, it also presents significant challenges. These include limited ESG expertise, data collection burdens and difficulties in aligning with evolving reporting standards. This paper highlights how digitalisation, though promising, requires tailored implementation to support SME capacities. It also emphasizes the enabling role of public authorities, larger corporations and industry associations in fostering SME compliance.
 
 
 
 This paper contributes to emerging scholarship by reframing SMEs as critical actors in EU sustainability governance. It offers original insights into how legal, technological and policy mechanisms intersect to shape SME reporting practices, and it proposes specific recommendations for ensuring that proportionality and digital enablement are integrated into future sustainability reporting frameworks.


---

From review filtering to topic modeling: a deep learning-based approach for analyzing negative online reviews of five-star hotels
10.1108/jhtt-04-2025-0328
Purpose This study aims to develop a robust analytical framework for identifying and interpreting negative user-generated reviews of five-star hotels. It addresses the limitations of traditional review filtering based solely on star ratings and the manual subjectivity of topic modeling, offering a deep learning–based solution aligned with national hospitality standards. Design/methodology/approach Using a data set of 124,381 user reviews from 70 five-star hotels in Jiangsu Province, collected via the Ctrip platform, this study applies a fine-tuned Chinese bidirectional encoder representations from transformers (BERT) model to detect negative reviews with high semantic accuracy. BERTopic is then used for topic modeling. To enhance domain relevance and interpretability, a high-quality semantic vocabulary is constructed based on the national standard Classification and Accreditation for Star-Rated Tourist Hotels (GB/T 14308–2023). The extracted topics are mapped to this vocabulary to establish structured semantic alignment between customer feedback and industry evaluation dimensions. Findings The BERT model identified 18,578 negative reviews, a figure significantly exceeding the number captured by rating-based filters alone. Among these, 437 topic clusters were extracted via BERTopic, with 388 successfully mapped to a standardized topic vocabulary. Results highlight that negative feedback is concentrated in key service areas such as room facilities, cleanliness, staff responsiveness and safety assurance. Notably, approximately 13% of high-rated (4–5 stars) reviews also contained negative sentiment, exposing service blind spots hidden beneath favorable scores. Research limitations/implications This study focuses on Chinese-language five-star hotel reviews and applies a national standard (GB/T 14308–2023) for topic alignment, which may limit cross-regional generalizability. The reliance on full-review classification, rather than sentence-level sentiment separation, may overlook mixed-opinion nuances. Furthermore, the exclusion of reviews with model disagreement might introduce selection bias. Lastly, while ChatGPT and DeepSeek enhance topic validation, the lack of human adjudication may affect interpretive accuracy. Future research could adopt multilingual data sets, cross-standard mapping and hybrid annotation methods to improve adaptability and robustness. Originality/value This research pioneers the integration of deep semantic modeling (BERT and BERTopic) with standardized industry lexicons in the context of Chinese-language user reviews, offering a reproducible, interpretable and domain-aligned approach to analyzing hotel reviews. Beyond the luxury hospitality sector, the framework’s combination of deep learning classification, BERTopic clustering and semantic mapping to industry standards can be adapted to other service industries – such as healthcare, retail or transportation – where customer experience data is text-rich and domain-specific. The study introduces a dual-model cross-validation mechanism to ensure semantic rigor and presents a semantic mapping framework that bridges user sentiment data with operational evaluation systems, providing a scalable methodology for intelligent service optimization across diverse high-contact service environments.

---

Artificial intelligence in supply chain decision-making: an environmental, social, and governance triggering and technological inhibiting protocol
10.1108/jm2-01-2023-0009

Purpose
Decision-making, reinforced by artificial intelligence (AI), is predicted to become potent tool within the domain of supply chain management. Considering the importance of this subject, the purpose of this study is to explore the triggers and technological inhibitors affecting the adoption of AI. This study also aims to identify three-dimensional triggers, notably those linked to environmental, social, and governance (ESG), as well as technological inhibitors.


Design/methodology/approach
Drawing upon a six-step systematic review following the preferred reporting items for systematic reviews and meta analysis (PRISMA) guidelines, a broad range of journal publications was recognized, with a thematic analysis under the lens of the ESG framework, offering a unique perspective on factors triggering and inhibiting AI adoption in the supply chain.


Findings
In the environmental dimension, triggers include product waste reduction and greenhouse gas emissions reduction, highlighting the potential of AI in promoting sustainability and environmental responsibility. In the social dimension, triggers encompass product security and quality, as well as social well-being, indicating how AI can contribute to ensuring safe and high-quality products and enhancing societal welfare. In the governance dimension, triggers involve agile and lean practices, cost reduction, sustainable supplier selection, circular economy initiatives, supply chain risk management, knowledge sharing and the synergy between supply and demand. The inhibitors in the technological category present challenges, encompassing the lack of regulations and rules, data security and privacy concerns, responsible and ethical AI considerations, performance and ethical assessment difficulties, poor data quality, group bias and the need to achieve synergy between AI and human decision-makers.


Research limitations/implications
Despite the use of PRISMA guidelines to ensure a comprehensive search and screening process, it is possible that some relevant studies in other databases and industry reports may have been missed. In light of this, the selected studies may not have fully captured the diversity of triggers and technological inhibitors. The extraction of themes from the selected papers is subjective in nature and relies on the interpretation of researchers, which may introduce bias.


Originality/value
The research contributes to the field by conducting a comprehensive analysis of the diverse factors that trigger or inhibit AI adoption, providing valuable insights into their impact. By incorporating the ESG protocol, the study offers a holistic evaluation of the dimensions associated with AI adoption in the supply chain, presenting valuable implications for both industry professionals and researchers. The originality lies in its in-depth examination of the multifaceted aspects of AI adoption, making it a valuable resource for advancing knowledge in this area.


---

Sustainable Production Indicators at Factory Level
10.1108/jmtm-04-2016-0054
Purpose - Sustainable production (SP) is a very broad area and the awareness and communication of the concept differ between varying levels in a company. The supposition is that the awareness and improvement of sustainability on shop floor level would improve, if a suitable set of indicators for measuring sustainability was available. The purpose of this paper is therefore to identify a list of performance indicators relevant for a production manager. Design/methodology/approach - This paper presents a two-step analysis, where the first step is a literature review with the purpose of compiling a gross list of sustainability indicators relevant on shop floor level. In the second phase, the relevance of this list for production managers in Swedish small- And medium-sized enterprises (SMEs) is tested in a questionnaire survey. Findings - The conclusion from the survey is that 27 out of 52 proposed indicators were relevant with statistical significance and that another 20 indicators were supported by at least 50 percent of the respondents. The respondents found the economic indicators to be most relevant for their purpose. However, the economic field seems to need more indicators in order to be more useful for daily operation. Practical implications - This set of indicators may be beneficial for companies seeking relevant indicators to drive sustainability improvements. Originality/value - This paper takes a new perspective on SP, as it focusses on shop floor production, which is possible to influence for a production manager.

---

Environmental, social and governance-type investing: a multi-stakeholder machine learning analysis
10.1108/md-04-2024-0930
Purpose This research delves into the determinants influencing the adoption of environmental, social and governance (ESG) investing through an analysis of social media dialogs using the uses and gratification theory. Design/methodology/approach This study employs a mixed-methods approach, integrating sentiment analysis, topic modeling, clustering, causal loop analysis and ethnography to examine ESG-related content on social media. Analyzing social media data, study identified key themes and derived ten propositions about ESG investing. Industry professionals, financial advisors and investors further validated these findings through expert interviews. Combining data-driven analysis and qualitative insights provides a comprehensive understanding of how social media shapes investor preferences and decision-making in the ESG domain. Findings Environmental aspects, such as conservation, preservation of natural resources, renewable and clean energy, biodiversity, restoration and eco-friendly products and technologies, shape attitudes toward ESG investing. Social considerations, including inclusivity, diversity, social justice, human rights, stakeholder engagement, transparency, community development and philanthropy, significantly influence ESG investing sentiments. Governance elements such as transparency, accountability, ethical governance, compliance, risk management, regulatory compliance and responsible leadership also play a pivotal role in shaping ESG investing opinions. Practical implications This study presents actionable insights for policymakers and organizations by identifying key constructs in ESG investing and proposing an integrated framework that includes mediating factors like resource efficiency and stakeholder engagement alongside moderating elements such as regulatory environment and investor preferences. Policymakers should establish standardized ESG reporting frameworks, incentivize sustainable practices and use social media data for regulatory purposes. For businesses, integrating social media insights into decision-making can enhance ESG communication strategies and accountability. These measures will foster greater transparency, strengthen investor relations and contribute to a more sustainable and inclusive global economy. Originality/value To the authors' best knowledge, this is the first study to investigate improving ESG investing preferences based on big data mined from social media platforms.

---

The impact of the EU nonfinancial information directive on environmental disclosure: evidence from Italian environmentally sensitive industries
10.1108/medar-03-2021-1247

Purpose
To determine whether to entrust the European Union (EU) to create a new nonfinancial reporting framework or endorse the extant reporting framework developed by the Global Reporting Initiative (GRI), this study aims to explore whether the mandatory implementation of the EU Directive positively impacted the GRI-based environmental disclosure.


Design/methodology/approach
The authors compared the pre- and post-EU Directive environmental disclosure of 16 Italian environmentally sensitive companies. The authors used an extended coding scheme and developed a unique scoring system to compare the quantitative and qualitative changes in environmental disclosure.


Findings
The analysis showed that the quantity of environmental disclosure increased after the mandatory EU Directive adoption. The most significant change was observed regarding the disclosure topics explicitly required by the Italian legislature. Additionally, disclosure of soft information continued to prevail over that of hard information in the post-Directive period. While the Directive boosted the level of adherence to GRI standards, Italian companies disclosed information that could be easily mimicked (soft) instead of objective measures that could be verified (hard). In light of this evidence, the endorsement of extant GRI standards could be a valuable option for enhancing the comparability and transparency of environmental disclosure.


Originality/value
This study used an original extended coding system and proposed related environmental disclosure indexes that allow monitoring changes in environmental disclosure over time. To the authors’ best knowledge, this study is one of the few that justifies the significant impact of regulation (here the EU Directive) on the increase in environmental disclosure and that uses hard and soft information typology to examine the quality of environmental disclosure.


---

Environmental disclosure and sentiment analysis: state of the art and opportunities for public-sector organisations
10.1108/medar-09-2019-0563
Purpose Because of the expansion of the internet and Web 2.0 phenomenon, new challenges are emerging in the disclosure practises adopted by organisations in the public-sector. This study aims to examine local governments’ (LGOs) use of social media (SM) in disclosing environmental actions/plans/information as a new way to improve accountability to citizens to obtain organisational legitimacy and the related sentiment of citizens’ judgements. Design/methodology/approach This paper analyses the content of 39 Italian LGOs’ public pages on Facebook. After the distinction between five classes of environmental issues (air, water, energy, waste and territory), an initial study is performed to detect possible sub-topics applying latent Dirichlet allocation. Having a list of posts related to specific environmental themes, the researchers computed the sentiment of citizens’ comments. To measure sentiment, two different approaches were implemented: one based on a lexicon dictionary and the other based on convolutional neural networks. Findings Facebook is used by LGOs to disclose environmental issues, focussing on their main interest in obtaining organisational legitimacy, and the analysis shows an increasing impact of Web 2.0 in the direct interaction of LGOs with citizens. On the other hand, there is a clear divergence of interest on environmental topics between LGOs and citizens in a dialogic accountability framework. Practical implications Sentiment analysis (SA) could be used by politicians, but also by managers/entrepreneurs in the business sector, to analyse stakeholders’ judgements of their communications/actions and plans on corporate social responsibility. This tool gives a result on time (i.e. not months or years after, as for the reporting system). It is cheaper than a survey and allows a first “photograph” of stakeholders’ sentiment. It can also be a useful tool for supporting, developing and improving environmental reporting. Originality/value To the best of the authors’ knowledge, this paper is one of the first to apply SA to environmental disclosure via SM in the public sphere. The study links modern techniques in natural language processing and machine learning with the important aspects of environmental communication between LGOs and citizens.

---

Decoding mood of the Twitterverse on ESG investing: opinion mining and key themes using machine learning
10.1108/mrr-07-2023-0526
Purpose
Grounded in the stakeholder theory and signaling theory, this study aims to broaden the research agenda on environmental, social and governance (ESG) investing by uncovering public sentiments and key themes using Twitter data spanning from 2009 to 2022.

Design/methodology/approach
Using various machine learning models for text tonality analysis and topic modeling, this research scrutinizes 1,842,985 Twitter texts to extract prevalent ESG investing trends and gauge their sentiment.

Findings
Gibbs Sampling Dirichlet Multinomial Mixture emerges as the optimal topic modeling method, unveiling significant topics such as “Physical risk of climate change,” “Employee Health, Safety and well-being” and “Water management and Scarcity.” RoBERTa, an attention-based model, outperforms other machine learning models in sentiment analysis, revealing a predominantly positive shift in public sentiment toward ESG investing over the past five years.

Research limitations/implications
This study establishes a framework for sentiment analysis and topic modeling on alternative data, offering a foundation for future research. Prospective studies can enhance insights by incorporating data from additional social media platforms like LinkedIn and Facebook.

Practical implications
Leveraging unstructured data on ESG from platforms like Twitter provides a novel avenue to capture company-related information, supplementing traditional self-reported sustainability disclosures. This approach opens new possibilities for understanding a company’s ESG standing.

Social implications
By shedding light on public perceptions of ESG investing, this research uncovers influential factors that often elude traditional corporate reporting. The findings empower both investors and the general public, aiding managers in refining ESG and management strategies.

Originality/value
This study marks a groundbreaking contribution to scholarly exploration, to the best of the authors’ knowledge, by being the first to analyze unstructured Twitter data in the context of ESG investing, offering unique insights and advancing the understanding of this emerging field.


---

Ensuring trust in sustainability financial reports: the role of AI and blockchain in metadata standardization
10.1108/msar-02-2025-0073

 
 This paper addresses metadata incoherence of corporate sustainability reporting in Saudi Arabia and UAE. The study examines how blockchain and artificial intelligence technologies can promote standardization, reliability and transparency of sustainability metadata, particularly in increasingly digitally maturing economies.
 
 
 
 According to technology–organization–environment (TOE) and institutional theory, this paper examines a conceptual model that includes seven hypotheses regarding metadata consistency, digital governance, transparency of AI, interaction with stakeholders, and sustainability reporting quality. Using Partial Least Squares Structural Equation Modeling (PLS-SEM) and multi-group analysis (MGA), this paper examines regulatory arrangements, organizational readiness, and relations with stakeholders to see how metadata outcomes are affected within industries.
 
 
 
 Findings conclude that metadata optimization with AI significantly enhances consistency and interoperability but is reliant upon digital capacity and governance maturity. Heterogeneity of sectoral practices, regulatory landscapes and adoption of AI between the UAE and Saudi Arabia affects metadata quality. Mediation and moderation analysis also shows that governance mechanisms and stakeholder engagement are significant to realize sustainable reporting practices with AI and blockchain technologies.
 
 
 
 The report calls for industry-specific regulatory policy and digital government mechanisms for facilitating AI integration in corporate sustainability reporting. Policymaking, business leadership and technology entrepreneurship have a role in collaborating on efforts to develop industry-specific AI policy strategies for staying in compliance, enhancing data integrity and constructing digital reporting ecosystems on a foundation of trust.
 
 
 
 This regionally focused, evidence-informed study offers practical suggestions for balancing global sustainable development targets with digital reporting systems. The study gives a contextualized examination of Gulf economies' adoption of AI and blockchain to encourage sustainability governance.


---

Influence of sustainability report disclosure and assurance on environmental, social and governance performance persistence: a survival analysis
10.1108/par-11-2024-0305

 
 This study aims to use a time-to-event framework (Cox survival analysis) to provide duration-based evidence on how sustainability report disclosure and sustainability assurance relate to environmental, social and governance (ESG) performance persistence (defined as the time until an ESG rating declines), a method rarely applied in ESG research.
 
 
 
 Using 2016–2024 data for publicly listed companies in Taiwan, this study uses a Cox time-dependent survival model to estimate the time to an ESG rating decline. The analysis distinguishes disclosure types (voluntary, mandatory, non-disclosure) and assurance types (accounting-firm assurance, non-accounting-firm assurance), further separating Big4 from non-Big4 accounting firms and identifying whether the same accounting firm provides both the statutory financial statement audit and the sustainability assurance (dual assurance).
 
 
 
 Voluntary disclosure is associated with longer ESG performance persistence relative to non-disclosure. Mandatory disclosure is associated with longer persistence only for firms subject to higher capital-based thresholds. Non-accounting-firm assurance is associated with longer persistence, whereas accounting-firm assurance shows no significant association, irrespective of Big4 status or dual assurance.
 
 
 
 This study applies a survival-analytic perspective, which is underutilized in ESG research, to capture the temporal dynamics of ESG performance persistence and to appropriately treat right-censored outcomes. By focusing on time to rating decline rather than point-in-time averages, this study provides duration-based evidence on corporate sustainability practices.


---

Environmental, social and governance (ESG) performance and abnormal positive tone
10.1108/sampj-01-2024-0045

Purpose
The purpose of this study is to explore the relationship between environmental, social and governance (ESG) performance and tone management in the annual report. This is based on the notion that managers, driven by personal interests, may use their ESG accomplishments by using an abnormal positive tone to enhance their reputation or career prospects.


Design/methodology/approach
Using panel data from Chinese listed companies from 2010 to 2022, this study first investigates the relationship between ESG performance and abnormal tone management. The study then uncovers this relationship is mediated through the mechanisms of equity-based incentive and analyst coverage. The conclusions of this paper hold even after a series of robustness tests, such as propensity score matching, Heckman two-stage method and two-stage least squares with instrumental variables.


Findings
This study finds a positive correlation between ESG performance and the presence of abnormal positive tone in annual reports. Furthermore, the mechanistic analysis reveals that managers in companies with strong ESG performance are motivated to use an overly positive tone, largely due to their vested interests in equity-based compensation. Moreover, in an effort to alleviate the pressure stemming from heightened financial analyst coverage and enhance the impression conveyed through analysts' reports, managers with superior ESG performance also tend to inflate the tone within their annual reports.


Practical implications
This study provides significant insights into the ongoing dialogue surrounding ESG-related equity incentives, which incentivize managerial manipulation of stock prices through the use of abnormal positive tone. The findings call upon investors to exercise greater vigilance in examining narrative information in annual reports, as abnormally positive tones may not always faithfully represent performance but rather reflect managerial self-interest.


Social implications
There is an emphasis on the importance of robust oversight mechanisms within corporate governance bodies to curb the manipulation of tone for managers’ personal gain.


Originality/value
This study enhances the theoretical foundation of ESG studies, offering a holistic perspective on the intricate interplay among ESG performance, managerial behavior and financial markets, with potential implications for researchers, investors and regulators.


---

A comparison of analysts’ and investors’ information efficiency of corporate social responsibility activities
10.1108/sampj-02-2023-0079

Purpose
This paper aims to examine the extent to which sell-side analysts efficiently incorporate firms’ corporate social responsibility (CSR) activities into their earnings forecasts. In addition, this paper also investigate the CSR information efficiency of analysts vis-à-vis that of investors.


Design/methodology/approach
This paper measures CSR activities by using CSR strength and CSR concern scores from the Morgan Stanley Capital International Environmental, Social and Governance database. This paper uses analysts’ earnings forecast errors and dispersion as proxies for their information efficiency. To compare the CSR information efficiency of analysts to that of investors, this paper uses the Vt/Pt ratio, which is the equity value estimates inferred from analysts’ earnings forecasts (a proxy for analysts’ CSR information efficiency) to the stock price of the focal company (a proxy for investors’ CSR information efficiency).


Findings
The regression analysis indicates that analysts’ earnings forecasts are optimistically biased and more dispersed for firms with positive CSR activities. The paper also finds that analysts’ forecasts are more optimistically biased than investors in interpreting CSR activities.


Practical implications
The lack of standardized protocols in CSR reporting and activities has raised the risk of mispricing by analysts, threatening the stability of sustainable investments. This paper suggests that regulators and standard-setters should establish a uniform framework governing firms’ CSR activities, along with their reporting and measurement, to ensure more consistent and reliable evaluations of CSR practices.


Social implications
Analysts’ mispricing of CSR activities may distort sustainable investing, as it can overly focus on the positive impacts of stakeholder theory, overlooking agency theory’s warnings about managerial self-interest. Investors need to assess CSR efforts with a dual perspective, acknowledging their societal value but also examining their alignment with shareholder interests.


Originality/value
To the best of the authors’ knowledge, this research is the first to assess the efficiency of analysts versus investors in processing CSR information amidst growing sustainable investment interests. Furthermore, building on Dhaliwal et al. (2012), which found that voluntary CSR disclosures correlate with more accurate analyst forecasts, this research provides fresh perspectives on the evolving nature of how analysts assimilate CSR information over time.


---

How will AI text generation and processing impact sustainability reporting? Critical analysis, a conceptual framework and avenues for future research
10.1108/sampj-02-2023-0097

Purpose
The ability of generative artificial intelligence (AI) tools such as ChatGPT to produce convincing, human-like text has major implications for the future of corporate reporting, including sustainability reporting. As the importance of sustainability reporting continues to grow, this study aims to critically analyse the benefits and pitfalls of automated text generation and processing.


Design/methodology/approach
This study develops a conceptual framework to delineate the field, assess the implications and form the basis for the generation of research questions. This study uses Alvesson and Deetz’s critical framework, considering insight (a review of literature and practice in the field), critique (consideration of the influences on the production and use of non-financial information and the implications for assurers of such information) and transformative redefinition (considering the implications of generative AI for sustainability reporting and proposing a research agenda).


Findings
This study highlights the implications of generative AI for sustainability accounting, reporting, assurance and report usage, including the risk of AI facilitating greenwashing, and the importance of more research on the use of AI for these matters.


Practical implications
The paper highlights to stakeholders the implications of AI for all aspects of sustainability reporting, including accounting, reporting, assurance and usage of reports.


Social implications
The implications of AI need to be understood in society, which this paper facilitates.


Originality/value
This study critically analyses the potential use of AI for sustainability reporting, construct a conceptual framework to delineate the field and develop a research agenda.


---

Verbal tones in sustainability assurance statements: An empirical exploration of explanatory factors
10.1108/sampj-06-2017-0051
This paper aims to examine, through the lens of language expectancy theory (LET), how sustainability assurors use optimism and certainty in possible persuasion attempts. The paper also explores a number of explanatory variables that could offer insights into the use of these verbal tones in sustainability assurance reports.,First, the paper relies on DICTION standard normalised optimism and certainty ranges in conjunction with descriptive statistics to analyse how sustainability assurors use optimism and certainty. Second, the paper uses quantile regression with robust standard errors to investigate the association between the measures of verbal tone used in this study and several possible explanatory variables.,Consistent with LET, the study documents that sustainability assurors exercise caution in using both certainty and optimism in persuasion attempts. The paper also finds that possible explanatory variables significantly associated with optimism include praise, assurance level, legal system and report location. However, reference to sustainability management control (SMC), status of assurance providers, praise, legal system and financial performance appear to explain the use of certainty.,The paper does not, at this stage, claim causality between the two measures of verbal tone, on the one hand, and the possible explanatory variables explored, on the other hand. It rather reports their possible associations. Furthermore, the study only measures reference to management control system and not reliance on it.,The main findings of this study imply that the use of optimism and certainty exhibits likely cautious practice by assurors. Nevertheless, assurors are more likely to use certainty more flexibly and appear more discreet when using optimism.,The findings of this paper also indicate how societal expectations play an important role in ensuring cautious persuasive behaviour by sustainability assurors in using verbal tones within sustainability assurance statements. This suggests that stakeholders may place reliance on attestations expressed in these statements.,The paper represents the first attempt to test LET in sustainability accounting by analysing verbal tones used by sustainability assurance providers. It contributes to the sustainability assurance literature in that it empirically demonstrates how sustainability assurors, as expert communicators, use optimistic tone and verbal certainty in careful persuasion attempts.

---

Evaluating the outcome effectiveness of the global reporting initiative transitions
10.1108/sampj-07-2022-0365

Purpose
This paper aims to evaluate the outcome effectiveness of the global reporting initiatives (GRI) transitions by understanding how companies have responded to the changes from G3.1 to G4 and finally to the GRI Standards.


Design/methodology/approach
A quality disclosure score is developed that incorporates assessments of both the quality of disclosures and the materiality of Australian companies. To analyse materiality, survey data were collected from 187 companies. Disclosure scores are based on a content analysis of the sustainability reports of 12 mining and metals companies and 12 financial services companies that used the GRI Standards from 2011 to 2019 (a total of 213 reports).


Findings
The study found that the GRI transitions have not led to companies improving the quality of their disclosures on areas considered important for them to achieve their social and environmental goals. Instead, the companies tended to use a greenwashing strategy, where the quality of disclosure of material issues declined or fluctuated over time.


Practical implications
From a practical perspective, the disclosure score developed in this paper enables managers of companies to recognize a threshold of completeness and to summarize the areas that are not materially relevant to their business.


Social implications
The results are potentially helpful for investors, shareholders and other stakeholders, enabling them to better understand sustainability reports.


Originality/value
This study contributes to the body of research in sustainability reporting by providing evidence on the outcome effectiveness of the latest updates in the GRI framework.


---

The greenwashing triangle: adapting tools from fraud to improve CSR reporting
10.1108/sampj-10-2018-0272
The purpose of this paper is to show a significant overlap in the models accounting research uses for fraud and the models other research disciplines use for greenwashing, and show how researchers and policymakers interested in the application of effective sustainability policy can draw from fraud accounting literature to better understand, and therefore, combat greenwashing. This is illustrated by showing multi-actor information-asymmetry models from other branches of accounting literature and synthesizing them with the fraud triangle model to suggest new avenues for reducing greenwashing and strengthening corporate social responsibility (CSR).,This paper reviews the current literature surrounding the greenwashing aspect of corporate camouflage compares the legal and technical definitions of fraud and synthesizes a new variant fraud triangle that more usefully describes greenwashing.,This paper is able to show that other areas of accounting research in North America have already tackled similar systems of multiple actors in an information-asymmetric environment and that a recurring trait is the emergence of a more robust reporting system. CSR reporting is currently in the process of emerging and could develop more swiftly by copying extant fraud-fighting tools. This is particularly salient given the increasing amount of liability legal regimes are giving to both sustainability activities and sustainability reporting from firms, as evidenced in both guidelines and scandals over the past decade.,Sustainability reporting is not unique in comprising a large number of interrelated entities with non-financial information asymmetry between actors. Previous researchers have encountered similar situations in government accounting and public administration and developed network models to study these relationships as a result. In government accounting, this led to the development both of better diagnostic tools for further research and better models for local governments to use to prevent fraud and malfeasance. This paper suggests that using such research methods in the area of CSR will allow for the development of similarly-useful tools and models.,Visualizing greenwashing as a form of fraud allows policymakers to use tools from the fraud-fighting literature to improve CSR reporting and produce a more robust regime in the future. As governments increasingly seek to respond effectively to material misstatements with an intent to deceive in sustainability reports, understanding the underlying information asymmetry as it is found in other private-public interfaces is critical. Similarly, researchers can analyze CSR reporting through the lens of fraud researchers to gain novel insights into how information asymmetry in CSR reporting works.,Greenwashing is not traditionally seen as a form of fraudulent reporting, even though it often meets the same technical test used to determine fraudulent reporting. The realization that the two are structurally similar allows the authors to better understand how CSR reporting works and how CSR reporting can be falsified. By understanding the latter, governments, firms and non-governmental organizations (NGOs) can develop tools to prevent CSR reporting from being falsified.,This paper suggests a new suite of tools with which to study greenwashing, and with which to fight greenwashing in a sustainability accounting context.

---

Simplified rating tool to evaluate sustainable practices of small-scale infrastructure projects in Australia – a comparative review
10.1108/sasbe-05-2022-0089
PurposeThe present study aims to inform the requirements for developing a sustainable rating tool for small-scale infrastructure projects (SSIPs) through research findings.Design/methodology/approachA review-based comparative study of existing infrastructure sustainability (IS) rating tools for assessment of SSIPs is presented. Key stakeholder participants of the existing IS rating tools, are interviewed to identify existing barriers and requirements for sustainability rating. The study further presents possible rating tool options to optimise the sustainable performance evaluation of SSIPs.FindingsFindings of this study indicated that prevalent IS rating tools are majorly applied to large-scale infrastructure projects and sustainability of SSIPs are seldom assessed. Based on a literature review and series of interviews, it was found that user friendliness, efficient structure, training and technical support, cost effectiveness and stakeholder recognition are the five key requirements of a sustainability rating tool for SSIPs. Additionally, six sustainability assessment options were proposed for SSIPs which range from pathways for existing tools through to new, customisable tools. Upon comparison, a new modified tool with verification process and revised tool with defined grouping of sustainable criteria was more effective for evaluation of SSIPs.Research limitations/implicationsUse of case specific information for validation and framework development may lack generalisation. However, methodology can be used for future decision-making by making necessary adjustments to suit different local regional requirements.Originality/valueDespite lack of generalisation, the findings can lead to future general studies on sustainability of SSIPs. Findings of the study provide foundation knowledge and awareness for sustainability evaluation of SSIPs.

---

Lifting the veil on environment-social-governance rating methods
10.1108/srj-03-2012-0035
Purpose - – There is growing recognition that numerous business drivers contribute to financial performance and investment returns but they are not included in a company's profit and loss statements. In the investment industry, these wider sets of value drivers are known as environment-social- governance (ESG) factors. A small number of specialized ESG rating agencies provide information to investors about the extent to which firms' behaviors are socially responsible. However, a major criticism of these rating agencies is the lack of transparency in their methods. This paper aims to examine the issues of subjectivity, transparency and uniformity of ESG ratings by exploring the methods used to assess ethics performance by an Australian rating agency. Design/methodology/approach - – A case study was conducted on an Australian ESG rating provider, Regnan. The data for the analysis were sourced from internal Regnan documents. Findings - – The paper found that a level of subjectivity is inevitable in ESG ratings and the call for uniformity may inhibit innovation, but these issues can be addressed by increased transparency of the rating methods. Research limitations/implications - – Further research is required to understand what level and, combination of, uniformity and transparency is sufficient to satisfy stakeholder requirements for ESG information. Practical implications - – The discussion of the factors underlying the ethics performance rating may prompt more open and transparent debate on how to assess ethical performance of companies, and increase investor confidence in ESG ratings. It may also provide more direction to companies on how to strengthen their ethical performance. Originality/value - – There is growing recognition that numerous business drivers contribute to financial performance and investment returns but they are not included in a company's profit and loss statements. These “ESG” factors can account for up to 66 percent of the market value of globally listed companies. In response to calls for more transparency on how ESG factors are assessed, and how ethical performance is appraised, this paper attempts to lift the veil on ESG rating methods.

---

Sustainable investment evaluation by means of life cycle assessment
10.1108/srj-03-2018-0054

Purpose
Sustainability investors are in need of updated standards, indexes and in general better tools and instruments to facilitate company information on its impacts on people, planet and profit. Such instruments to reveal reliable, independent metrics and indicators to evaluate companies’ performances on sustainability exist, however, in research fields that previously have not been used extensively, for instance, life cycle assessments (LCAs). ISO 14001:2015 has implemented life cycle perspective, however, without being explicitly clear on which methodology is preferred. This paper aims to investigate LCA as to improve companies’ transparency towards sustainability investors through a literature review on sustainable investment evaluation.


Design/methodology/approach
The literature review is conducted through the search engine Google Scholar, which to date hosts the most comprehensive academic database across other databases such as Scopus, ISI Web of Knowledge, Science Direct, etc. Search words such as “Sustainable finance”, “Sustainable Investments”, “Performance metrics”, “Life cycle assessment”, “LCA”, “Environmental Management Systems”, “EMS” and “Environmental Profit and Loss Account” were used. Special journals that publish research on LCA such as International Journal of Life Cycle Assessment, Journal of Cleaner Production and Journal of Industrial Ecology were also investigated in-depth.


Findings
The combination of using LCA in, for instance, environmental profit and loss accounts studied in this paper shows a comprehensive and reliable tool for sustainability investors, as well as for social responsibility standards such as ISO 14001, ISO 26000, UN Global Compact, GIIN, IRIS and GRI to incorporate. With a LCA-based hybrid input-output account, both upstream and downstream’s impact on the environment and society can be assessed by companies to attract more funding from sustainability investors such as shareholders, governments and intergovernmental bodies.


Research limitations/implications
The literature review is based on publicly disclosed academic papers as well as five displayed company Environmental Profit and Loss accounts from the Kering Group, PUMA, Stella McCartney company, Novo Nordisk and Arla Group. Other company experiences with integration of LCA as a reporting tool have not been found, yet it is not to conclude that these five companies are the only ones to work extensively with LCA.


Practical implications
The paper may contribute to the clarification of LCA-thinking and perspective implementation in both ISO 14001 and ISO 26000, as well as in other social responsibility standards such as the UN Global Compact, the Global Impact Investing Networks, IRIS performance metrics, the Global Reporting Initiative and others.


Originality/value
The paper is one of the first that evaluates LCA and environmental profit and loss accounts for sustainability investors, as well as for consideration of implementation in social responsibility standards such as the ISO 14001 and ISO 26000, as well as in other social responsibility standards such as the UN Global Compact, the Global Impact Investing Networks, IRIS performance metrics and the Global Reporting Initiative.


---

The debate on automated essay grading
10.1109/5254.889104
We look at a controversy: the use of computers for automated and semiautomated grading of exams. K. Kukich, the director of the Natural Language Processing group at Educational Testing Service, provides an insider's view of the history of the field of automated essay grading and describes how ETS is currently using computer programs to supplement human judges in the grading process. T. Landauer, D. Laham, and P. Foltz describe the use of latent semantic analysis in a commercial essay-scoring system called IEA. They also address important ethical questions. L. Hirschman, E. Breck, J. Burger, and L. Ferro report on MITRE's current efforts towards automated grading of short-answer questions and discuss the ramifications for the design of general question answering systems. Finally R. Calfee places these developments in the framework of current educational theory and practice.

---

EECSA: Enhancing Textual Sentiment Analysis through Emotion-Enriched Contextual Integration and Self-Adaptive Prompting Optimization
10.1109/acait63902.2024.11022234
Textual sentiment analysis remains a crucial task in natural language processing (NLP) that focuses on identifying sentiments towards specific entities within a t ext. Recently, large language models (LLMs) have demonstrated remarkable capabilities in understanding semantics and logical reasoning. However, LLMs often face challenges such as hallucinations, which can lead to inaccurate sentiment determination for target entities. Additionally, current methods frequently fail to fully exploit the extensive prior knowledge embedded within LLMs, resulting in suboptimal sentiment classification. To address these challenges, we propose the Emotion-Enriched Contextual Sentiment Analysis (EECSA) framework. EECSA integrates unique emotion-enriched constraints, backgrounds, and analogical reasoning to mitigate LLM hallucinations and employs self-adaptive bootstrap instructions optimization to further enhance LLM predictions. Specifically, constraints are constructed through a linguistic consistency mechanism and multiple inquiry rounds; background descriptions are incorporated as prompts for LLM inference; and analogies are generated by LLMs to enhance textual sentiment analysis. EECSA further refines initial LLM instructions through adaptive iterative optimization using a random search bootstrap algorithm, maximizing the effectiveness of LLM prompting. Extensive experiments of zero-shot and few-shot learning with GPT-3.5-turbo across six public datasets validate the superiority of our EECSA framework, demonstrating performance that in some cases surpasses GPT-4.0, the most advanced large language model in the world. Code is available at: https://github.com/albert-jin/EECSA.

---

Deep Learning for Aspect-Level Sentiment Classification: Survey, Vision, and Challenges
10.1109/access.2019.2920075
This survey focuses on deep learning-based aspect-level sentiment classification (ASC), which aims to decide the sentiment polarity for an aspect mentioned within the document. Along with the success of applying deep learning in many applications, deep learning-based ASC has attracted a lot of interest from both academia and industry in recent years. However, there still lack a systematic taxonomy of existing approaches and comparison of their performance, which are the gaps that our survey aims to fill. Furthermore, to quantitatively evaluate the performance of various approaches, the standardization of the evaluation methodology and shared datasets is necessary. In this paper, an in-depth overview of the current state-of-the-art deep learning-based methods is given, showing the tremendous progress that has already been made in ASC. In particular, first, a comprehensive review of recent research efforts on deep learning-based ASC is provided. More concretely, we design a taxonomy of deep learning-based ASC and provide a comprehensive summary of the state-of-the-art methods. Then, we collect all benchmark ASC datasets for researchers to study and conduct extensive experiments over five public standard datasets with various commonly used evaluation measures. Finally, we discuss some of the most challenging open problems and point out promising future research directions in this field.

---

Aspect Based Sentiment Analysis With Feature Enhanced Attention CNN-BiLSTM
10.1109/access.2019.2952888
Previous work has recognized the importance of using the attention mechanism to obtain the interaction between aspect words and contexts for sentiment analysis. However, for the most attention mechanisms, it is unrigorous to use the average vector of the aspect words to calculate the context attention. Besides, the feature extraction ability of the model is also essential for effective analysis, the combination of CNN and LSTM can enhance the feature extraction ability and semantic expression ability of the model, which is also a popular research trend. This paper introduces an aspect level neural network for sentiment analysis named Feature Enhanced Attention CNN-BiLSTM (FEA-NN). Our method is to extract a higher-level phrase representation sequence from the embedding layer by using CNN, which provides effective support for subsequent coding tasks. In order to improve the quality of context encoding and preserve semantic information, we use BiLSTM to capture both local features of phrases as well as global and temporal sentence semantics. Besides, we add an attention mechanism to model interaction relationships between aspect words and sentences to focus on those keywords of targets to learn more effective context representation. We evaluate the proposed model on three datasets: Restaurant, Laptop, and Twitter. Extensive experiments show that the effectivess of FEA-NN.

---

LISA: Language-Independent Method for Aspect-Based Sentiment Analysis
10.1109/access.2020.2973587
Understanding “what others think” is one of the most eminent pieces of knowledge in the decision-making process required in a wide spectrum of applications. The procedure of obtaining knowledge from each aspect (property) of users’ opinions is called aspect-based sentiment analysis which consists of three core sub-tasks: aspect extraction, aspect and opinion-words separation, and aspect-level polarity classification. Most successful approaches proposed in this area require a set of primary training or extensive linguistic resources, which makes them relatively costly and time consuming in different languages. To overcome the aforementioned challenges, we propose an unsupervised paradigm for aspect-based sentiment analysis, which is not only simple to use in different languages, but also holistically performs the subtasks for aspect-based sentiment analysis. Our methodology relies on three coarse-grained phases which are partitioned to manifold fine-grained operations. The first phase extracts the prior domain knowledge from dataset through selecting the preliminary polarity lexicon and aspect word sets, as representative of aspects. These two resources, as primitive knowledge, are assigned to an expectation-maximization algorithm to identify the probability of any word based on the aspect and sentiment. To determine the polarity of any aspect in the final phase, the document is firstly broken down to its constituting aspects and the probability of each aspect/polarity based on the document is calculated. To evaluate this method, two datasets in the English and Persian languages are used and the results are compared with various baselines. The experimental results show that the proposed method outperforms the baselines in terms of aspect, opinion-word extraction and aspect-level polarity classification.

---

Enhancing BERT Representation With Context-Aware Embedding for Aspect-Based Sentiment Analysis
10.1109/access.2020.2978511
Aspect-based sentiment analysis, which aims to predict the sentiment polarities for the given aspects or targets, is a broad-spectrum and challenging research area. Recently, pre-trained models, such as BERT, have been used in aspect-based sentiment analysis. This fine-grained task needs auxiliary information to distinguish each aspect. But the input form of BERT is only a words sequence which can not provide extra contextual information. To address this problem, we introduce a new method named GBCN which uses a gating mechanism with context-aware aspect embeddings to enhance and control the BERT representation for aspect-based sentiment analysis. Firstly, the input texts are fed into BERT and context-aware embedding layer to generate BERT representation and refined context-aware embeddings separately. These refined embeddings contain the most correlated information selected in the context. Then, we employ a gating mechanism to control the propagation of sentiment features from BERT output with context-aware embeddings. The experiments of our model obtain new state-of-the-art results on the SentiHood and SemEval-2014 datasets, achieving a test F1 of 88.0 and 92.9 respectively.

---

A Systematic Review on Implicit and Explicit Aspect Extraction in Sentiment Analysis
10.1109/access.2020.3031217
Aspect-based sentiment analysis (ABSA) is currently among the most vigorous areas in natural language processing (NLP). Individuals, private and government institutions are increasingly using media sources for decision making. In the last decade, aspect extraction has been the most essential phase of sentiment analysis (SA) to conduct an abridged sentiment classification. However, previous studies on sentiment analysis mostly focused on explicit aspects extraction with limited work on implicit aspects. To the best of our knowledge, this is the first systematic review that covers implicit, explicit, and the combination of both implicit and explicit aspect extractions. Therefore, this systematic review has been conducted to, 1) identify techniques used for extracting implicit, explicit, or both implicit and explicit aspects; 2) analyze the various evaluation metrics, data domains, and languages involved in the implicit and explicit aspect extraction in sentiment analysis from years 2008 to 2019; 3) identify the key challenges associated with the techniques based on the result of a comprehensive comparative analysis; and finally, 4) highlight the feasible opportunities for future research directions. This review can be used to assist novice and prominent researchers to understand the concept of both implicit and explicit aspect extractions in aspect-based sentiment analysis domain.

---

Transformer Based Multi-Grained Attention Network for Aspect-Based Sentiment Analysis
10.1109/access.2020.3039470
Aspect-based sentiment analysis aims to predict sentiment polarity for every aspect in a sentence review. Most existing approaches are based on the sequence models, which may superimpose the emotional semantics of different tendencies and lack syntactic structure information. And most models adopt coarse-grained attention mechanism which still face the issues of weakness interaction between aspect and context. In this paper, we propose a transformer based multi-grained attention network (T-MGAN), which utilizes the Transformer module to learn the word-level representations of aspects and context respectively, and further utilizes the Tree Transformer module to obtain the phrase-level representations of contexts. It is capable of extracting the syntactic structure features and syntax information of aspect and context. In addition, we adopt dual-pooling method and multi-grained attention network to extract high quality aspect-context interactive representations. We evaluate the proposed model on three datasets and prove the effectiveness of the proposed model.

---

Multi-Head Self-Attention Transformation Networks for Aspect-Based Sentiment Analysis
10.1109/access.2021.3049294
Aspect-based sentiment analysis (ABSA) aims to analyze the sentiment polarity of an input sentence in a certain aspect. Many existing methods of ABSA employ long short-term memory (LSTM) networks and attention mechanism. However, the attention mechanism only models the local certain dependencies of the input information, which fails to capture the global dependence of the inputs. Simply improving the attention mechanism fails to solve the issue of target-sensitive sentiment expression, which has been proven to degrade the prediction effectiveness. In this work, we propose the multi-head self-attention transformation (MSAT) networks for ABSA tasks, which conducts more effective sentiment analysis with target specific self-attention and dynamic target representation. Given a set of review sentences, MSAT applies multi-head target specific self-attention to better capture the global dependence and introduces target-sensitive transformation to effectively tackle the problem of target-sensitive sentiment at first. Second, the part-of-speech (POS) features are integrated into MSAT to capture the grammatical features of sentences. A series of experiments carried on the SemEval 2014 and Twitter datasets show that the proposed model achieves better effectiveness compared with several state-of-the-art methods.

---

Explainable Artificial Intelligence for Tabular Data: A Survey
10.1109/access.2021.3116481
Machine learning techniques are increasingly gaining attention due to their widespread use in various disciplines across academia and industry. Despite their tremendous success, many such techniques suffer from the “black-box” problem, which refers to situations where the data analyst is unable to explain why such techniques arrive at certain decisions. This problem has fuelled interest in Explainable Artificial Intelligence (XAI), which refers to techniques that can easily be interpreted by humans. Unfortunately, many of these techniques are not suitable for tabular data, which is surprising given the importance and widespread use of tabular data in critical applications such as finance, healthcare, and criminal justice. Also surprising is the fact that, despite the vast literature on XAI, there are still no survey articles to date that focus on tabular data. Consequently, despite the existing survey articles that cover a wide range of XAI techniques, it remains challenging for researchers working on tabular data to go through all of these surveys and extract the techniques that are suitable for their analysis. Our article fills this gap by providing a comprehensive and up-to-date survey of the XAI techniques that are relevant to tabular data. Furthermore, we categorize the references covered in our survey, indicating the type of the model being explained, the approach being used to provide the explanation, and the XAI problem being addressed. Our article is the first to provide researchers with a map that helps them navigate the XAI literature in the context of tabular data.

---

Unrestricted Attention may not Be All You Need - Masked Attention Mechanism Focuses Better on Relevant Parts in Aspect-Based Sentiment Analysis
10.1109/access.2022.3142178
Aspect-Based Sentiment Analysis (ABSA) is one of the highly challenging tasks in natural language processing. It extracts fine-grained sentiment information in user-generated reviews, as it aims at predicting the polarities towards predefined aspect categories or relevant entities in free text. Previous deep learning approaches usually rely on large-scale pre-trained language models and the attention mechanism, which applies the complete computed attention weights and does not place any restriction on the attention assignment. We argue that the original attention mechanism is not the ideal configuration for ABSA, as for most of the time only a small portion of terms are strongly related to the sentiment polarity of an aspect or entity. In this paper, we propose a masked attention mechanism customized for ABSA, with two different approaches to generate the mask. The first method sets an attention weight threshold that is determined by the maximum of all weights, and keeps only attention scores above the threshold. The second selects the top words with the highest weights. Both remove the lower score parts that are assumed to be less relevant to the aspect of focus. By ignoring part of input that is claimed irrelevant, a large proportion of input noise is removed, keeping the downstream model more focused and reducing calculation cost. Experiments on the Multi-Aspect Multi-Sentiment (MAMS) and SemEval-2014 datasets show significant improvements over state-of-the-art pre-trained language models with full attention, which displays the value of the masked attention mechanism. Recent work shows that simple self-attention in Transformer quickly degenerates to a rank-1 matrix, and masked attention may be another cure for that trend.

---

A Local and Global Context Focus Multilingual Learning Model for Aspect-Based Sentiment Analysis
10.1109/access.2022.3197218
Aspect-Based Sentiment Analysis (ABSA) aims to predict the sentiment polarity of different aspects in a sentence or document, which is a fine-grained task of natural language processing. Most of the existing work focuses on the correlation between aspect sentiment polarity and local context. The important deep correlations between global context and aspect sentiment polarity have not received enough attention. Besides, there are few studies on Chinese ABSA tasks and multilingual ABSA tasks. Based on the local context focus mechanism, we propose a multilingual learning model based on the interactive learning of local and global context focus, namely LGCF. Compared with the existing models, this model can effectively learn the correlation between local context and aspect words and the correlation between global context and aspect words simultaneously. In addition, the model can effectively analyze comments in Chinese and English simultaneously. Experiments conducted on three Chinese benchmark datasets(Camera, Phone and Car) and six English benchmark datasets(Laptop, Restaurant14, Restaurant16, Twitter, Tshirt and Television) demonstrate that LGCF has achieved compelling performance and efficiency improvements compared with several existing state-of-the-art models. Moreover, the ablation experiment results also verify the effectiveness of each part in LGCF.

---

Detecting Documents With Inconsistent Context
10.1109/access.2022.3204151
Extremely large volumes of documents are available from online news platforms and social media. While the quantity of these documents have grown exponentially, the majority lack their quality, which can cause digital fatigue or promote misinformation. To this end, we propose a novel framework that can evaluate the quality of documents in terms of consistency. We model low-quality document detection as a binary classification task, which is able to measure how the documents have consistent contents. Specifically, we relax the problem by considering each sentence or paragraph as node. A given document is then considered as a network of nodes. We show how we define the supernode in a network and show how it is informative enough to detect whether the document is consistent or not. We believe this scheme can be applied to various applications including fake news detection, and document screening with qualitative evaluations. We achieve the state-of-the-art on existing tasks using the NELA17 dataset, and YH-News dataset which we release in this paper.

---

Attention-Based Multi-Channel Gated Recurrent Neural Networks: A Novel Feature-Centric Approach for Aspect-Based Sentiment Classification
10.1109/access.2023.3281889
Sentiment analysis is an active research domain of the current era, thanks to its vast applications. The main objective is to classify the polarity of the given text as negative, positive, or neutral. Thus, researchers’ focus shifted towards the aspect or feature-based sentiment analysis because overall polarity does not determine the people’s views towards certain features. Therefore, Aspect-Based Sentiment Analysis (ABSA) helps us to identify the sentiments about various aspects of different services and products. However, their accurate identification and extraction are still challenging for the research community due to the complex nature of natural languages. This paper presents a method named Attention-based Multi-Channel Gated Recurrent Neural Network (Att-MC-GRU), which extracts aspects and analyzes textual reviews to predict or classify their sentiments. It introduces the hybrid approach by combining word embedding, part of speech (POS) tags, and contextual position information. The main novelty lies in proposal of a Multi-Channel Gated Recurrent Neural Network (MC-GRU), in contrast to the existing studies that consider Recurrent Neural Networks (RNN) comprising only a single input channel. In addition, word embedding, POS tags, and contextual position information collectively improve the identification and prediction accuracy of aspects and their associated sentiments. Due to the application of the filtering by the attention mechanism that figured out first the significant words, which helps to determine entities’ aspects related to the sentiment expressed. The empirical analysis proves the proposed approach’s effectiveness compared to the existing techniques in the relevant literature using standard datasets. According to the empirical analysis, the proposed model performs better in the F1-measure, with an overall achievement of 94% in the task of aspect extraction and 93% in the classification of sentiment.

---

A Survey on Multimodal Aspect-Based Sentiment Analysis
10.1109/access.2024.3354844
Multimodal Aspect-Based Sentiment Analysis (MABSA), as an emerging task in the field of sentiment analysis, has recently received widespread attention. Its aim is to combine relevant multimodal data to determine the sentiment polarity of a given aspect in text. Researchers have surveyed both aspect-based sentiment analysis and multimodal sentiment analysis, but, to the best of our knowledge, there is no survey on MABSA. Therefore, in order to assist related researchers to know MABSA better, we surveyed the research work on MABSA in recent years. Firstly, the relevant concepts of MABSA were introduced. Secondly, the existing research methods for the two subtasks of MABSA research (that is, multimodal aspect sentiment classification and aspect sentiment pairs extraction) were summarized and analyzed, and the advantages and disadvantages of each type of method were analyzed. Once again, the commonly used evaluation corpus and indicators for MABSA were summarized, and the evaluation results of existing research methods on the corpus were also compared. Finally, the possible research trends for MABSA were envisioned.

---

Challenges and Opportunities of Text-Based Emotion Detection: A Survey
10.1109/access.2024.3356357
Emotion detection has become an intriguing issue for researchers owing to its psychological, social, and commercial significance. People express their feelings directly or indirectly through facial expressions, language, writing, or behavior. An emotion detection tool is a critical and practical way to recognize and categorize moods in various applications. Artificial intelligence (AI) is often used to identify emotions. Machine learning and deep learning algorithms produce high-quality solutions for diagnosing emotional diseases among social media users. Numerous studies and survey articles have been published on emotion detection using textual data. However, most of these studies did not comprehensively address the emerging architectures and performance analyses in emotion detection. This study provides an extensive survey of state-of-the-art systems, techniques, and datasets for textual emotion recognition. Another goal of this study is to emphasize the limitations and provide up-and-coming research directions to fill these gaps in this rapidly evolving field. This survey paper investigated the concepts and performance of different categories of textual emotion detection models, approaches, and methodologies.

---

Comparative Analysis of Deep Natural Networks and Large Language Models for Aspect-Based Sentiment Analysis
10.1109/access.2024.3386969
Sentiment analysis is essential for comprehending public opinion, particularly when considering e-commerce and the expansion of online businesses. Early approaches treated sentiment analysis as a document or sentence-level classification problem, lacking the ability to capture nuanced opinions about specific aspects. This limitation was addressed by the development of aspect-based sentiment analysis (ABSA), which links sentiment to specific aspects that are mentioned explicitly or implicitly in the review. ABSA is relatively a recent field of sentiment analysis and the existing models for ABSA face three main challenges, including domain-specificity, reliance on labeled data, and a lack of exploration into the potential of newer large language models (LLMs) such as GPT, PaLM, and T5. Leveraging a diverse set of datasets, including DOTSA, MAMS, and SemEval16, we evaluate the performance of prominent models such as ATAE-LSTM, flan-t5-large-absa, DeBERTa, PaLM, and GPT-3.5-Turbo. Our findings reveal nuanced strengths and weaknesses of these models across different domains, with DeBERTa emerging as consistently high-performing and PaLM demonstrating remarkable competitiveness for aspect term sentiment analysis (ATSA) tasks. In addition, the PaLM demonstrates competitive performance for all the domains that were used in the experiments including the restaurant, hotel, books, clothing, and laptop reviews. Notably, the analysis underscores the models’ domain sensitivity, shedding light on their varying efficacy for both ATSA and ACSA tasks. These insights contribute to a deeper understanding of model applicability and highlight potential areas for improvement in ABSA research and development.

---

Sentiment Analysis in Low-Resource Settings: A Comprehensive Review of Approaches, Languages, and Data Sources
10.1109/access.2024.3398635
The field of low-resource sentiment analysis has seen significant developments in recent years. This research review SLR evaluates the approaches and data sources utilized in low-resource sentiment analysis by deep learning. The primary aim is to discover suitable approaches for future sentiment analysis in low-resource. Our studies explore various languages, models, and data sources expressing a desire to create effective approaches. Our emphasis lies in the critical evaluation of the approaches and the datasets utilized, to identify areas where further research is needed. Our analysis study adds to the existing body of literature reviews, encompassing multilingual low-resource sentiment analysis research spanning from 2018 to 2023. The findings indicate that the transfer learning approach is the most frequently used, followed by word embedding learning and machine translation systems. Additionally, the study shows that social media is the most used platform for data collection, followed by product reviews, movies, and hotels. There has been a significant surge in the adoption of pre-trained transformers, indicating a growing interest in exploring the potential of these models for low-resource languages within the natural language processing (NLP) community. This trend is largely attributed to the novel nature of these models and their feature of being non-labour intensive. However, the scarcity of annotated datasets for such languages remains a major hurdle. finally, these research findings are relevant and informative for any researcher working in the field of low-resource multilingual sentiment analysis. The study introduces a conceptual framework for performing sentiment analysis in low-resource. The study provides a valuable resource for future researchers.

---

Uncovering Concerns of Citizens Through Machine Learning and Social Network Sentiment Analysis
10.1109/access.2024.3426329
Artificial Intelligence and Machine Learning (AI/ML) as analytical tools can be applied across multiple social domains. Thus, these tools are being deployed in several ways to address societal issues and concerns for “social good”. For instance, AI/ML has applicable use cases for crisis response, economic empowerment, educational demands, environmental challenges, equality and inclusion, health and hunger, and security and justice. In this work, we seek to explore the power and capability of AI/ML in understanding citizens’ engagement, which can improve governance and smart city deployment. Specifically, we studied the views expressed by online users about the city of Saskatoon in Canada. The analyzed views have become a value chain that community leaders can use to improve the governance structure of the city. In the study, we extracted 114,390 comments from Reddit (i.e., Saskatoon subreddit posts) between January 1, 2019, and September 20, 2023, to discover topics to highlight citizens’ concerns. We compare the performance of three major topic models, namely, Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and BERTopic with a K-means clustering algorithm in the discovery of topics from the collected Reddit comments. The BERTopic with the K-means clustering algorithm achieved the highest coherence score of approximately 0.64 in the extraction of 25 topics from the dataset. Our findings showed that BERTopic can discover coherent and diverse topics compared to LDA and NMF. We found 12 underlying themes by merging related topics. Also, we leveraged SiEBERT (a pre-trained transformer model), 4 supervised ML models, and VADER (a lexical sentiment analysis classifier) to identify the sentiments expressed in each theme. The SiEBERT model outperformed the other sentiment classifiers with an accuracy of 89% in the prediction of sentiments. The research discovered factors for smart city engagement such as Housing and Facilities, Education, Downtown Development, Tourism and Entertainment, Policing, Healthcare, Online Community, and Cost.

---

Aspect Based Sentiment Analysis for Service Industry
10.1109/access.2024.3440357
In today's digital age, customer feedback, particularly gathered from various sources like mobile application reviews, has emerged as a critical resource for service-providing organizations to gain valuable insights into their customers' experiences. As the key objective of service-providing organizations is to facilitate their customers with better services, customer feedback or opinion is a vital resource for such organizations to improve and enhance their services for the betterment of their customers. Explicitly mentioned opinions have been widely studied in research, while a significant gap exists in addressing implicitly described views. Furthermore, most existing research focuses on product-oriented corpora, emphasizing specific product aspects and features. This article presents a novel study on performing end-to-end aspect-based sentiment analysis (ABSA) by extracting implicit opinion terms, categorizing them, and assigning polarity to each term from mobile app reviews in English. Through this study, we developed a domain-specific, service-oriented, and aspect-based annotated dataset and introduced a novel two-step hybrid approach. The first step involves extracting multiple opinion terms using a rule-based approach. The second step employs machine learning and deep learning algorithms to classify the extracted opinion terms into general aspect categories. This two-step approach effectively addresses the double-implicit problem commonly encountered in the previous work on implicit aspects and opinion mining. In addition to traditional machine learning and deep learning models, we fine-tuned BERT to carry out the ABSA task. This approach utilized a pipeline method, where each task's output serves as the subsequent task's input, ensuring a seamless flow of information and improved performance. This multi-step pipeline begins with the extracted opinion terms classification into aspect categories and ends with the assignment of sentiment polarity. Experiments with a hold-out test set for the first step (opinion term extraction using a rule-based approach) achieved an accuracy and precision score of 81.4% and an F1 score of 0.99 %, outperforming several baselines. Further experiments with a range of machine learning and deep learning algorithms for classifying extracted opinion terms into general aspect categories yielded accuracy scores ranging from 0.68% to 0.74 % and F1 scores ranging from 0.23% to 0.28%. Experiments on sentiment classification using various machine learning algorithms showed accuracy ranges from 0.58% to 0.68% and F1 scores from 0.47% to 0.49%. This two-step approach for implicit opinion term, aspect extraction, and classification outperforms many baseline systems. By leveraging BERT's contextual understanding and fine-tuning it for our specific domain, we significantly improved the accuracy and robustness of our aspect-based sentiment analysis. This approach effectively captured both explicit and implicit opinions from mobile app reviews. Specifically, our method achieved an accuracy of 0.80% and an F1 score of 0.78% for aspect categorization, and an accuracy of 0.79% and an F1 score of 0.70% for sentiment classification, demonstrating substantial improvements over traditional methods.

---

A Reinforcement Learning Module for Reducing Curve Trajectory Error in End-to-End Autonomous Driving
10.1109/acdsa65407.2025.11165918
This paper proposes an approach that integrates a reinforcement learning module into a supervised learning-based end-to-end (E2E) autonomous driving system to address the trajectory error problem that occurs during curve driving in Target Point-based E2E models. The representative Target Point-based autonomous driving algorithm, Trajectory-guided Control Prediction (TCP), suffers from trajectory deviation issues in curved paths due to the shortcut problem. The proposed model minimizes trajectory errors in real time through a Soft Actor-Critic (SAC) reinforcement learning-based adaptive control correction module that utilizes the image features and control outputs of TCP, and employs a reward function specifically designed to enhance stability in curve driving scenarios. Experimental results using a proprietary driving dataset collected in the MORAI simulator demonstrate that the proposed model reduces trajectory errors by approximately 80% compared to the conventional TCP in curve situations and proves its robustness in various driving environments.

---

Sentence Pair Classification Fine-Tuning of Bidirectional Encoding Representation from Transformer (BERT) for Aspect-Based Sentiment Analysis (ABSA) on IMDb Movie Reviews
10.1109/amlds63918.2025.11159419
Aspect Based Sentiment Analysis (ABSA) provides a more detailed sentiment analysis towards aspects on domains like movie reviews, which often discussing various aspects of a movie in a review. However, previous models often struggle with handling sarcasm and other complex language nature. This study explores the fine-tuning of Bidirectional Encoder Representations from Transformers (BERT) for ABSA on IMDb movie reviews by implementing sentence pair classification that incorporates auxiliary sentences representing predefined aspects such as acting, plot, directing, visual, and audio. The dataset consists of 2,000 unique reviews expanded into 10,000 sentence pair sequences. Two models were developed: one utilizing the base BERT model directly trained on the IMDb dataset, and the other fine-tuned on SemEval 2014 dataset before application to IMDb reviews. The fine-tuned BERT model on the SemEval 2014 dataset outperformed the baseline model in low training data scenarios, achieving an F1-score of 0.88 for the aspect detection task and sentiment classification accuracy of 80%, 82%, and 89% for 4-way, 3-way, and binary classification settings respectively. he fine-tuned BERT model on the SemEval 2014 dataset also achieves more stable performance across low-resources scenarios.

---

Detecting Greenwashing in Sustainability Disclosures: A Prediction Model for KOSPI 200 Enterprises using ESG-BERT
10.1109/bigdata59044.2023.10386798
This research is centered on the development of a BERT-based metric for greenwashing, designed to address the existing limitations inherent in ESG assessment methodologies. Unlike standard assessments, ESG-BERT considers real-time policy details and reduces the risk of inaccurate evaluations and greenwashing. We employed ESG-BERT along with financial and environmental data to predict greenwashing among Korean KOSPI200 companies. By using advanced machine learning models like ANN, LR, RF, and XGB, the study found that XGB performs best in predicting greenwashing. Furthermore, the study compares greenwashing predictions between companies with top5 and bottom5 ESG ratings. The results showed better performance for the top 5 companies compared to the bottom 5 companies.

---

A Comprehensive Categorization and Comparative Analysis of Methodologies in Aspect-Based Sentiment Analysis
10.1109/cai64502.2025.00028
The increasing number of user-generated reviews on the web has forced businesses to want to know more about the sentiment of customers, which has led to the need for improved techniques in the field of sentiment analysis. Aspect-based sentiment Analysis (ABSA) addresses this need by extracting detailed sentiment information linked to specific aspects or categories in text. This survey presents an overview of the current achievements in ABSA, which suggests a new taxonomy that is used to categorize ABSA methods into three main groups: machine learning and deep learning-based models, attention-based models, and transformer-based models. The taxonomy is focused on how the four key components of ABSA (aspect, sentiment, category, and opinion) have interacted. Key models are explained with technical details and in simple and intuitive descriptions, along with a comparison of their performance. It also outlines the current challenges and emerging trends to offer practical guidelines that can guide the direction of future research and applications in ABSA.

---

An ontology-driven clustering method for supporting gene expression analysis
10.1109/cbms.2005.29
The gene ontology (GO) is an important knowledge resource for biologists and bioinformaticians. This paper explores the integration of similarity information derived from GO into clustering-based gene expression analysis. A system that integrates GO annotations, similarity patterns and expression data in yeast is assessed. In comparison with a clustering model based only on expression data correlation, the proposed framework not only produces consistent results, but also it offers alternative, potentially meaningful views of the biological problem under study. Moreover, it provides the basis for developing other automated, knowledge-driven data mining systems in this and related application areas.

---

Personalised track design in car racing games
10.1109/cig.2016.7860435
Real-time adaptation of computer games' content to the users' skills and abilities can enhance the player's engagement and immersion. Understanding of the user's potential while playing is of high importance in order to allow the successful procedural generation of user-tailored content. We investigate how player models can be created in car racing games. Our user model uses a combination of data from unobtrusive sensors, while the user is playing a car racing simulator. It extracts features through machine learning techniques, which are then used to comprehend the user's gameplay, by utilising the educational theoretical frameworks of the Concept of Flow and Zone of Proximal Development. The end result is to provide at a next stage a new track that fits to the user needs, which aids both the training of the driver and their engagement in the game. In order to validate that the system is designing personalised tracks, we associated the average performance from 41 users that played the game, with the difficulty factor of the generated track. In addition, the variation in paths of the implemented tracks between users provides a good indicator for the suitability of the system.

---

A Prototype System Using Causal Inference and Machine Learning Models to Estimate ESG Ratings for SMEs
10.1109/cinti63048.2024.10830823
Although numerous environmental, social, and governance (ESG) rating agencies exist globally, different organizations may employ distinct evaluation frameworks and methods, potentially leading to inconsistent or misleading ratings for a given company. Moreover, these rating agencies primarily focus on evaluating publicly listed large-cap companies, rendering their assessment approaches unsuitable for small and medium-sized enterprises (SMEs) that lack sufficient resources to support ESG evaluations. This study aims to develop an SME-specific ESG rating system that integrates both indirect and direct approaches. To facilitate responsible and reliable ESG evaluations, the indirect approach employs causal inference technique to identify the most relevant variables affecting ESG ratings in large listed companies. Subsequently, three machine learning-based predictive models are trained using this data, which are then applied to SMEs with a bridging strategy to derive their potential ESG ratings. The direct approach categorizes SMEs by industry and applies customized ESG performance indicators to assess their actual ESG ratings. Finally, the two rating results are integrated using a weighted formula, accompanied by the implementation of Microsoft Power BI's visualized and interactive features, to establish a transparent, explainable, and interpretable SME-specific ESG rating system.

---

Machine-Human Boundary: A Systematic Survey of Machine-Generated Text Detection
10.1109/cisce65916.2025.11065851
The rapid development of Large Language Models (LLMs) has blurred the line between machine-generated text (MGT) and human-written text (HWT), raising ethical concerns. This paper reviews existing research on MGT detection. It introduces three types of detectors, manual, metric-based, and model-based, and analyzes their characteristics and representative cases. Three categories of benchmarks for evaluating detectors, namely dichotomy, demarcation point, and Mixtext, are also discussed. Current detectors have flaws, such as poor robustness and limited language support. Future research should prioritize the development of more robust and adaptable detectors and benchmarks, with an emphasis on leveraging textual correlation features to improve detection accuracy across diverse scenarios. This study summarizes the current state of MGT detection, providing a basis for future research in this field.

---

A Vision-Language Pre-training model based on Cross Attention for Multimodal Aspect-based Sentiment Analysis
10.1109/cvidl62147.2024.10603872
Multimodal aspect-based sentiment analysis (MABSA) focuses on extracting unique aspect term within multimodal informations and subsequently analysising sentiment associated with this aspect. Existing studies have typically used either a separate pipeline method or a uniform transformer. However, these approaches are inadequate to clearly and efficiently integrate the alignment between distinct modes. To resolve these constraints, we suggest a Vision-Language Pre-training model based on cross attention for MABSA named VLPCA, which applies a novel Multi-head cross attention to capture textual and visual features for better representation of visual-language interactions. Furthermore, we design two subtasks and propose a novel unsupervised joint training approach based on contrastive learning to enhance the effectiveness of the proposed model. Extensive experiments and ablation studies show that our model consistently performs better than the current methods.

---

Aspect-Based Sentiment Analysis Using Local Context Focus Mechanism with DeBERTa
10.1109/docs60977.2023.10294548
Text sentiment analysis, often termed as opinion mining, delves into quantifying individuals' opinions, evaluatio ns, attitudes, and emotions conveyed about entities. Sentiment a nalysis of text can be categorized into text-level, sentence-level, and aspect-level analyses. Aspect-Based Sentiment Analysis (A BSA) represents a detailed sub-discipline within sentiment anal ysis, with its primary goal being to ascertain the sentiment pola rity of specific aspects. The research of pre-training neural mod el has significantly improved the performance of many natural language processing tasks. In recent years, pre training model (PTM) has been applied in ABSA. Therefore, there has been a q uestion, which is whether PTMs contain sufficient syntactic info rmation for ABSA. In this paper, we explored the recent DeBE RTa model (Decoding-enhanced BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis problem. DeBE RTa is a kind of neural language model based on transformer, which uses self-supervised learning to pre-train on a large num ber of original text corpora. Based on the Local Context Focus (LCF) mechanism, by integrating DeBERTa model, we purpos e a multi-task learning model for aspect-based sentiment analys is. The experiments result on the most commonly used the lapto p and restaurant datasets of SemEval-2014 and the ACL twitte r dataset show that LCF mechanism with DeBERTa has signifi cant improvement.

---

Fine-Grained Sentiment Analysis for Enhanced Financial Distress Prediction
10.1109/eebda60612.2024.10485942
Sentiment analysis aims to identify the sentiment polarity of specific aspects within given sentences or comments, and aspect-based sentiment analysis is considered a fundamental task in sentiment analysis. With practical applications in areas such as product reviews, food delivery evaluations, and public opinion monitoring, sentiment analysis plays a crucial role. This paper focuses on the application of fine-grained sentiment analysis in financial distress prediction (FDP) to enhance early warnings of the management status of companies. In previous studies, there has been a narrow emphasis on using document-level sentiment analysis to extract overall sentiment from text, overlooking the semantic nuances conveyed by sentiments. Therefore, this paper aims to extract fine-grained sentiments from the Management Discussion & Analysis (MD&A) of Chinese listed companies. The proposed model is based on a two-step framework, consisting of an unsupervised aspect-level financial sentiment extraction phase and a model validation phase. Specifically, the former is built on a deep learning model with an attention mechanism, conducting unsupervised aspect extraction, aspect identification, and aspect-level sentiment classification in a sequential manner to obtain fine-grained sentiments. The latter is responsible for evaluating the effectiveness of the newly acquired features on benchmark machine learning models, including SVM, DT, LR, CNN, and DNN. Experimental results reveal that MD&A predominantly covers eight types of aspects, including ownership, business scope, development, capital, sales, management, prizes, and probability. Additionally, it has been observed that fine-grained sentiment features can enhance the performance of FDP. This study represents a significant innovation in existing literature, being the first to introduce aspect-level financial sentiment analysis into the realm of FDP.

---

A Text Mining Analysis of Social Media Criticisms on Tuition Fee Policies at State Universities in Indonesia
10.1109/eecsi63442.2024.10776274
This study aims to identify public criticism on social media related to the residential tuition policy imposed in state universities in Indonesia. Knowing the public's criticism of the single tuition policy on social media can identify critical issues, understand the public sentiment, and provide insights for policy improvement. The benefits of the process can be used to improve the quality of policies, strengthen relations with the public, increase student satisfaction and welfare, and contribute to academic knowledge about policy analysis using social media. The data came from public comments about tuition fees on Twitter, You'Tube, Instagram, and TikTok platforms. The dataset was labeled as neutral, negative, and positive sentiment in the first step using IndoBERT's pre-trained model. The result of a negative sentiment of 62.26% from the dataset was then used for topic analysis using the Latent Dirichlet Allocation algorithm, and a different number of topics was obtained on each platform with topic compositions for Twitter, YouTube, Instagram, and TikTok of 10, 5, 2, 10 topics respectively. The results of the synthesis from all platforms concluded that seven themes are the main issues discussed in public criticism related to tuition fees, where the top three themes are criticism of government policies that increase tuition fees (48,60/0), corruption cases occur in state universities (20,3 %) and students' concerns about continuing their studies when there is an increase in tuition fees (12,5%).

---

A Type-2 Fuzzy Logic Approach to Explainable AI for regulatory compliance, fair customer outcomes and market stability in the Global Financial Sector
10.1109/fuzz48607.2020.9177542
The field of Artificial Intelligence (AI) is enjoying unprecedented success and is dramatically transforming the landscape of the financial services industry. However, there is a strong need to develop an accountability and explainability framework for AI in financial services, based on a risk-based assessment of appropriate explainability levels and techniques by use case and domain.This paper proposes a risk management framework for the implementation of AI in banking with consideration of explainability and outlines the implementation requirements to enable AI to achieve positive outcomes for financial institutions and the customers, markets and societies they serve. The work presents the evaluation of three algorithmic approaches (Neural Networks, Logistic Regression and Type 2 Fuzzy Logic with evolutionary optimisation) for nine banking use cases. We review the emerging regulatory and industry guidance on ethical and safe adoption of AI from key markets worldwide and compare leading AI explainability techniques.We will show that the Type-2 Fuzzy Logic models deliver very good performance which is comparable to or lagging marginally behind the Neural Network models in terms of accuracy, but outperform all models for explainability, thus they are recommended as a suitable machine learning approach for use cases in financial services from an explainability perspective. This research is important for several reasons: (i) there is limited knowledge and understanding of the potential for Type-2 Fuzzy Logic as a highly adaptable, high performing, explainable AI technique; (ii) there is limited cross discipline understanding between financial services and AI expertise and this work aims to bridge that gap; (iii) regulatory thinking is evolving with limited guidance worldwide and this work aims to support that thinking; (iv) it is important that banks retain customer trust and maintain market stability as adoption of AI increases.

---

Aspect-Based Subjectivity Analysis Using a BERT-Based Approach
10.1109/iaict62357.2024.10617758
Aspect-based subjectivity analysis stands as an important task in natural language processing, seeking to identify the subjectivity of various aspects or features within a text. A new method for aspect-based subjectivity analysis using BERT is introduced in this paper. BERT has demonstrated impressive performance across various NLP tasks, and its capabilities are utilized to accurately ascertain the subjectivity of specific aspects within a given text. The approach involves fine-tuning BERT on a sizable dataset annotated with aspect-level subjectivity labels, enabling the model to grasp the subtleties of aspect-based subjectivity analysis. Extensive experiments on benchmark datasets are conducted to showcase the effectiveness of this approach and compare it with existing methods. The results reveal that this proposed approach surpasses state-of-the-art techniques in aspect-based subjectivity analysis, underscoring the potential of leveraging BERT for such purposes.

---

Aspect Detection and Sentiment Classification Using Deep Neural Network for Indonesian Aspect-Based Sentiment Analysis
10.1109/ialp.2018.8629181
Sentiment analysis can categorize an overall opinion from a sentence or a document. However, there are sentences with more than one opinion in a single sentence statement. This problem is solved by aspect-based sentiment analysis. We conduct experiments on this problem using Indonesian dataset with 2-step process: aspect detection and sentiment classification. On aspect detection, we compare two deep neural network models with different input vector and topology: word embedding vector which is processed using gated recurrent unit (GRU), and bag-of-words vector which is processed using fully-connected layer. On sentiment classification, we also compare two approaches of deep neural network. The first approach uses word embedding, sentiment lexicon and POS tags as the input vector, with bi-GRU based as the topology. The second one uses aspect matrix to rescale the word embedding vector as the input vector and convolutional neural network (CNN)based as the topology. Our work is compared to a baseline framework which uses different model for each aspect. The dataset has approximately 9800 reviews collected from various categories on popular online marketplaces in Indonesia. Our models generalize well over all aspects and achieve state-of-the-art performance on 4 out of 7 aspects compared to the baseline framework.

---

End-to-End Deep Learning for Autonomous Steering Angle Prediction: A Dataset-Driven Approach
10.1109/iatmsi64286.2025.10985690
Trained on a dataset of 8,036 samples from several camera viewpoints, this work proposes an end-to-end deep learning method for guiding angle prediction in autonomous driving. With an eye toward forward-driving situations, the dataset comprises steering, throttle, and braking settings. Using Mean Squared Error (MSE), model performance was assessed, a low validation MSE of 0.0107 indicated reliable steering predictions. With kurtosis and skewness assessments stressing rare but important driving circumstances, dataset analysis revealed a balanced distribution of steering values and skewed throttle values. The model’s minimal overfitting and consistent training progress highlight its possible practical application. Future developments will try to increase resilience and lower sensitivity to outliers, hence supporting safer and more flexible autonomous driving systems.

---

Formula One: Racing Intelligence in the Age of AI
10.1109/icaeca63854.2025.11012647
This study proposes an all-inclusive, data-driven system to optimize the performance of Formula One racing by incorporating state-of-the-art technology such as blockchain, edge computing, artificial intelligence, and real-time telemetry. The current systems of Formula 1 depend heavily on manual analysis and traditional data collection, thereby limiting the ability to make decisions in real time. The proposed system, however, incorporates high-resolution telemetry sensors with Nvidia Jetson edge computing devices, AI-driven prediction models, and secure blockchain protocols so that performance improvement during a race can be achieved even faster and more accurately. The key components are in the form of telemetry sensors for real-time data acquisition, edge computing for real-time processing, blockchain technology for secure data exchange and artificial intelligence models for predictive analytics. This method improves vehicle performance, adjusts to the race condition changes, and enhances general decision-making with an efficient, transparent, and scalable solution. The system architecture also accommodates future technological development as well as cooperation within the motorsport industry. The implications of the findings spread wider to driverless car industries and other data-dependent sectors, showing how blockchain and AI can change the game plan of racing and corporate formations.

---

Optimizing Route Efficiency in Formula One (F1) Vehicles Using Reinforcement Learning Algorithms
10.1109/icaic63015.2025.10848611
This research explores the application of reinforcement learning (RL) to enhance route efficiency and performance of a Formula One (F1) car within a simulation environment. The simulation is implemented using Python, NEAT (NeuroEvolution of Augmenting Topologies), and PyGame to create a dynamic system where neural networks control the car’s navigation. RL enables the F1 car, acting as an agent, to learn optimal decisions through a fitness-based reward mechanism by interacting with its environment. Equipped with radar sensors to detect obstacles and measure distances, the virtual car adjusts its speed and steering to avoid collisions and optimize movement. Over successive generations, the RL algorithm refines the car’s driving ability, improving speed and directional control to maximize distance covered and minimize lap times. A fitness-based evaluation system tracks progress, providing metrics such as best and average fitness scores, which highlight the car’s evolving performance. Results demonstrate the effectiveness of RL in enhancing autonomous driving capabilities, enabling the car to navigate complex environments and improve decision-making across generations.

---

Detecting Factual Hallucinations in LLMs: A Cross-Domain Dynamic Validation Framework for Fact-Consistency
10.1109/icaide65466.2025.11189340
Although generative pre-training models improve the quality of text generation for large language models, factual error detection becomes more difficult due to three fundamental problems. First, deploying large models increases the risk of errors during task processing. Second, the granularity of factual statements in long texts generated by existing large models is unclear, and the definition of individual facts lacks nuance. Third, the fact-checking process lacks traceable sources of evidence. A large amount of computational resources are utilized to solve these problems, but these resources are insufficient for efficiently solving complex, multi-hop problems. The proposed solution is an adaptive fact-detection framework called FactChecker. It dynamically selects the most appropriate strategy, ranging from the simplest to the most complex large language model (LLM), based on the complexity of the problem. The framework uses small LLMs as classifiers to predict the complexity of the verification process. Thus, FactChecker can identify factual inaccuracies in texts generated by large language models (e.g., ChatGPT). Experiments on three different tasks demonstrate the effectiveness of this approach.

---

MSACC: A Unified Multimodal Sentiment Analysis Framework for High Interpretability and Zero-shot Performance
10.1109/icassp49660.2025.11099014
Compared to large language models, traditional multimodal sentiment analysis frameworks are constrained by their classification heads, resulting in poor performance on zero-shot tasks. Moreover, due to limitations in visual encoders and multimodal fusion modules, most existing frameworks can only process a small number of images, leading to a loss of visual information. In light of these issues, this paper proposes a new framework, MSACC. This framework enhances the model’s zero-shot performance by adopting a contrastive classification method and reduces the loss of visual information through visual relation extraction and three-dimensional sentiment analysis. We conducted extensive experiments on the Yelp dataset. The experimental results show that MSACC outperforms models of the same category in zero-shot MSA tasks, achieving a 48% performance improvement. Furthermore, compared to the large language model ChatGLM2-6B, MSACC still achieved a 7% performance increase while saving 90% of the model size. In addition, in supervised tasks, MSACC also achieved a 3.27% performance improvement compared to the baseline model.

---

Aspect-Based Sentiment Analysis: An Extensive Study of Techniques, Challenges, and Applications
10.1109/iccebs58601.2023.10449111
Aspect-based Sentiment Analysis(ABSA) has gained significant attention in recent years because of its ability to provide more fine-grained insights into customer preferences. ABSA, an NLP-based data mining technique, focuses on identi-fying user sentiments related to different aspects of a product or service. This paper presents a comprehensive overview of ABSA including aspect extraction, sentiment analysis, and aspect-based summarization techniques, and discusses its challenges, applications, and future scope. In addition, the paper presents a thorough comparative analysis of deep learning-based models for ABSA and also discusses the commonly used datasets. Deep learning-based techniques have produced better outcomes than the traditional ABSA methods.

---

A Sentiment Analysis Model Based on Text Generation-OpnionSpanT5
10.1109/iccect60629.2024.10546142
ABSA is a type of fine-grained sentiment analysis, aiming to analyze the various sentiment types of a given sentence in more detail. Currently, more and more solutions adopt Seq2Seq-based architecture to perform sentiment analysis by outputting a sentence containing the required sentiment tuples. However, there is usually a problem, that is, when capturing opinion items, there may be a problem of missing extraction when facing opinion items with span. This paper proposes a model based on the paraphrase generation method, that is, before generating the quadruple, to extract opinion items separately. A BiLSTM-CRF layer is added in front of the PARA model to extract opinion items separately, and then the opinion items and the original sentences are input into the subsequent T5 model to generate emotional interpretations. We call it OpinionSpanT5, which is effective Fixed an error that occurred when generating opinion items.

---

Application of unstructured data processing and analyzing base on chinese in digital data evidence collecting
10.1109/iccet.2010.5485757
The proportion of unstructured data in the total number of information is much larger that the proportion of structured data. But the research on the processing and analyzing mode is not wider and deeper than the structured data. Based on illustrating the importance of the unstructured data research, this article expounds the key techniques in its processing and analyzing mode, such as entity recognition, relation extraction, etc. It describes the specific application of the unstructured data processing and analyzing in the digital data evidence collecting, combining the independent research and development of “Mass Case Information Intelligence Analyzing System” and applied in handling the online ball gambling case.

---

Sentiment Analysis of TikTok Comments on Indonesian Presidential Elections Using IndoBERT
10.1109/iccit62134.2024.10701256
TikTok, a popular social media platform, hosts numerous trending topics, including discussions about the Indonesian presidential and vice-presidential elections (Pilpres). Netizens frequently share their views during this period, resulting in a mix of positive, neutral, and negative comments that can significantly influence public opinion. This research aims to develop a model for classifying Indonesian comments on TikTok using the pre-trained IndoBERT model. The dataset comprises 36,991 comments: 10,600 positive, 10,136 neutral, and 16,255 negative. The research process involves data preprocessing, labeling, training, validation, and testing. The developed model, named Indonesia-Pemilu-Sentiment-Classification, is hosted on Hugging Face. Testing results show an accuracy of 92.1%, a precision of 92.3%, a recall of 92.1%, and an F1-score of 92.1%, indicating the model's high accuracy in classifying sentiment in comments. The model demonstrates high performance and reliability in sentiment classification, contributing to a better understanding of public opinion during the Indonesian presidential elections.

---

AI as a Catalyst for Expanding CSR: The Case of Moroccan Entreprises
10.1109/iccsc66714.2025.11135146
At first glance, the concepts of corporate social responsibility (CSR) and artificial intelligence (AI) may seem contradictory; one is centered on automation and performance, while the other focuses on ethical governance and sustainable development. However, the union of these two paradigms holds revolutionary potential to reshape the way companies communicate with their stakeholders and their environment. Corporate Social Responsibility (CSR) embodies an organization's incorporation of social, environmental, and economic factors into its operations and strategic direction. With its capabilities in machine learning, natural language processing, and big data analytics, artificial intelligence helps companies improve their performance, strengthen their accountability, and anticipate stakeholder demands. This article examines how Moroccan companies, which are driving a constantly modernizing economy, could leverage AI to consolidate and expand their Corporate Social Responsibility (CSR) initiatives. Based on a critical literature review, contextual analysis, and concrete applications, this study offers a conceptual and practical framework for incorporating AI into CSR strategies. It highlights opportunities, ethical risks, and governance guidance for ensuring AI serves as a tool for inclusion and sustainable development.

---

DocFormer: End-to-End Transformer for Document Understanding
10.1109/iccv48922.2021.00103
We present DocFormer - a multi-modal transformer based architecture for the task of Visual Document Understanding (VDU). VDU is a challenging problem which aims to understand documents in their varied formats (forms, receipts etc.) and layouts. In addition, DocFormer is pre-trained in an unsupervised fashion using carefully designed tasks which encourage multi-modal interaction. DocFormer uses text, vision and spatial features and combines them using a novel multi-modal self-attention layer. DocFormer also shares learned spatial embeddings across modalities which makes it easy for the model to correlate text to visual tokens and vice versa. DocFormer is evaluated on 4 different datasets each with strong baselines. DocFormer achieves state-of-the-art results on all of them, sometimes beating models 4x its size (in no. of parameters).

---

Predicting Environmental, Social, and Governance Scores with Machine Learning: A Systematic Literature Review
10.1109/icdabi63787.2024.10800444
The paramount factors in assessing sustainability and the social responsibility of business are environmental, social, and governance (ESG). Over the last few years, leveraging machine learning (ML) methods in predicting ESG scores has preserved significant recognition due to its possibility of enhancing prediction accuracy, investment practices, and decision-making processes by stakeholders, policymakers, and investors. Our research uses ML methods to synthesize recent findings from scholarly articles on ESG score prediction. The systematic literature review (SLR) elucidates diverse areas of previous literature, such as datasets, methodology, metrics, and findings. The finding of SLR illustrates the significant application of ML to predict ESG scores, especially by Random Forest. The research paves the way for practical implications for stakeholders seeking long-term investment practices. However, it also underscores the pressing need for further studies to explore the capabilities of other ML models and enhance their capabilities through integrating hybrid models or feature engineering, thereby ensuring the continued improvement and relevance of ESG score prediction.

---

Aspect-based Sentiment Analysis (ABSA) using Machine Learning Algorithms
10.1109/icdcece60827.2024.10549140
Aspect-Based Sentiment Analysis (ABSA) is a Natural Language Processing task that aims to identify and extract the sentiment of specific aspects or components of a product or service. ABSA typically involves a multi-step process that begins with identifying the aspects or features of the product or service that are being discussed in the text. This is followed by sentiment analysis, where the sentiment polarity (positive, negative, or neutral) is assigned to each aspect based on the context of the sentence or document. Finally, the results are aggregated to provide an overall sentiment for each aspect. The process involves training machine learning models to classify text sentiment (positive, negative, or neutral). First, we transform text data using Term Frequency-Inverse Document Frequency (TF-IDF), which assigns weights to words based on their importance within a document collection. This emphasizes informative terms. Then, these TF-IDF features are fed into both SVM and Logistic Regression models. SVM find a hyper plane that best separates sentiment classes, while Logistic Regression calculates the probability of a text belonging to a specific sentiment class. Extensive experiments have been conducted on datasets of covid vaccinations dataset and results show that the support vector machine model achieves excellent performance in terms of aspect extraction and sentiment classification. Sentiment on Twitter can be imbalanced, with more positive or negative tweets depending on the topic. This can affect the training process. Techniques like oversampling or undersampling the minority class might be necessary. This work investigates the performance of machine learning algorithms for a specific classification task. Support Vector Machine (SVM) and Logistic Regression (LR) were compared. The results indicate that Support Vector Machine achieved superior accuracy (87.34%) compared to Logistic Regression (84.64%), suggesting Support Vector Machine as a more suitable option for this classification task.

---

Towards Transparent AI-Powered Cybersecurity in Financial Systems: The Deployment of Federated Learning and Explainable AI in the CaixaBank pilot
10.1109/icdmw65004.2024.00041
In the domain of financial cybersecurity, where trust and reliability is paramount, the advent of Artificial Intelligence is bringing novel tools for network intrusion detection. This paper introduces AI4FIDS, a novel AI-powered Intrusion Detection System leveraging Federated Learning (FL) to enhance data privacy while enabling decentralized model training across multiple financial entities. Concurrently, we present TRUST4AI.xAI, an explainability module designed to render AI decision-making transparent and interpretable, thereby aligning with the critical need for model accountability in financial applications. Our experimental results, conducted in the framework of the AI4CYBER project’s financial sector pilot, demonstrate in detecting network intrusions in financial infrastructure while maintaining user privacy, while increasing trustworthiness via explain-ability methods. The integration of these technologies addresses the dual challenges of effective threat detection and regulatory compliance, offering a scalable solution for modern financial institutions. This work contributes to the ongoing dialogue on leveraging AI for financial security and sets a benchmark for the development of privacy-preserving, interpretable AI models in this sector.

---

Neurosymbolic AI for Mining Key Aspects of Socially Responsible Investing
10.1109/icdmw65004.2024.00070
Environmental, Social, and Governance (ESG) factors have become critical for assessing corporate sustainability and ethical responsibility. However, the vast volume of unstructured data available across corporate reports, social media, and news sources poses a challenge for systematic ESG analysis. This paper explores the application of neurosymbolic AI, which combines neural networks’ pattern recognition capabilities with the structured reasoning of symbolic AI, to mine key aspects of ESG from large-scale, diverse data sources. By leveraging SenticNet for concept parsing and deep learning for sentiment analysis, we extract relevant ESG metrics, classify corporate practices, and identify trends. This hybrid approach enhances both the interpretability and scalability of ESG analysis, providing more accurate insights into corporate behaviors and their impact on sustainability goals. Results demonstrate that neurosymbolic AI not only improves the extraction of meaningful ESG aspects but also enables real-time monitoring, supporting data-driven decision-making for investors, regulators, and stakeholders.

---

A New Feature-Centered Method for Classifying Emotions Based on Context
10.1109/icdsns58469.2023.10245944
Automated sentiment evaluation techniques are in great demand because of the rising tide of reviews on the internet as well as additional text with emotional nuance. Automatic, granular collection of emotion from written words or phrases is made possible by aspect-based categorization of sentiment (ABSC) owing to the complex structure of natural tongues, the study industry continues to encounter difficulties in accurately identifying and extracting them. In order to anticipate or categorize reviewers' emotions, the authors of this research provide a technique called Attention-based Gated Recurrent Neural Network (A-GRU). It presents the hybrid strategy by integrating word incorporation, POS tagging, & positional contextual data. In contrast to previous research, which has focused on single-channel Recurrent Neural Networks (RNNs), we propose a Multi-Channel Gated Recurrent Neural Networks (MC-GRU). Word insertion, POS tags, & context location information all work together to boost the precision with which aspects & their related attitudes may be identified & predicted. As a result of the filtration applied by the system of attention which initially identified the key phrases, this in turn aids in determining the attributes of entities connected to the stated attitude. Using industry-standard datasets, the empirical study demonstrates the efficacy of the suggested strategy in comparison to the state-of-the-art methods in the aforementioned literature. The experimental findings show that the suggested model outperforms the state-of-the-art methods for the F1-measure, having a total score of 94% on the component extraction test & 98% on the emotional categorization job.

---

Automated Essay Scoring in Education
10.1109/icet62460.2024.10868446
As artificial intelligence advances rapidly, Automated Essay Scoring (AES) technology has made significant strides, effectively addressing the challenges of high labor intensity and slow feedback encountered by teachers during essay grading. In recent years, numerous innovative research findings and methodologies have emerged in this field. We explore in this paper the mainstream studies in AES and categorize them into two groups: single-prompt AES and cross-prompt AES. Single-prompt AES involves scoring scenarios where training and test essays are written for the same prompt, comprising both feature engineering and deep learning-based approaches. On the other hand, cross-prompt AES entails scoring scenarios where training and test essays are written for different prompts, often implemented within the frameworks of contrast learning, transfer learning, or multi-task learning. Drawing from existing research, we analyze the primary issues and challenges in AES and proposes potential directions for future research.

---

Automated Grading of Handwritten Essays
10.1109/icfhr-2018.2018.00056
Automatic grading of handwritten essays is vital in evaluating the performance of students in educational settings, particularly in situations where language experts are rare. We build a system capable of taking the input as handwritten essays in image format and outputs the grading on the scale of 0-5; 0 being the worst and 5 being the best. The overall system integrates Optical Handwriting Recognition (OHR) and Automated Essay Scoring (AES)/grading. The handwritten essay is transcribed using a network composed of Multi-Dimensional Long Short Term Memory (MDLSTM) and convolution layers. The loss function is Connectionist Temporal Classification (CTC). The AES model is a 2-layer artificial neural network with a feature set based on pretrained GloVe word vectors. The results of grading of essays are compared for transcriptions of essays received from OHR system and transcriptions of essays done manually. The mutual agreement between the two shows a Quadratic Weighted Kappa score of 0.88. The results indicate that though the current OHR systems have transcription errors but as a whole can perform well for an application like AES.

---

Aspect-Based Sentiment Analysis on Social Media X for Electric Vehicles (EV) in Indonesia Using IndoBERT and Machine Learning
10.1109/icic64337.2024.10956679
A countries around the world rallies in pushing more sustainable and more-environmentally-friendly approach in transportation vehicle development, the Ministry of Industry of Indonesia is stepping forward to be one the leader in transition towards environmentally friendly industries through the adoption of Electric Vehicles (EVs), EVs have become a prominent topic of discussion in Indonesia, particularly on social media platform X.Therefore, this study utilizes data from social media X to leverage Natural Language Processing (NLP) technology, focusing on sentiment analysis based on various aspects of EVs using the IndoBERT method. Previous research has found IndoBERT to be more effective compared to other Machine Learning methods. The use of NLP and IndoBERT is expected to provide a deep understanding of Indonesian public sentiment towards EVs, with analysis results compared across several Machine Learning and Deep Learning techniques. Model performance is evaluated using a confusion matrix, which provides metrics such as Accuracy, Precision, Recall, and F1 Score. The implementation of sentiment analysis based on multiple EV aspects is deployed through a website dashboard, facilitating easier access for users to gain insights and visualize sentiment-related data on EVs in Indonesia. Thus, this research not only contributes to enhancing public and industrial understanding of EVs but also opens up further potential for the development of Machine Learning and Deep Learning technologies for aspect-based sentiment analysis. From the research findings, the use of NLP and IndoBERT demonstrated superior performance compared to other methods, achieving a sentiment accuracy of 0,82 and aspect accuracy of 0,85. Negative sentiments predominated over positive ones, with negative sentiments dominating throughout the years 2021-2024, particularly concerning infrastructure aspects, and in 2024 regarding cost aspects.

---

Hybrid Models for Recognizing Indonesian Textual Entailment
10.1109/icicos62600.2024.10636863
A paragraph raises pertinent issues about linguistic disparities in understanding the semantic relationships between sentences within the natural language processing domain (NLP) Addressing this challenge necessitates the development of robust NLP models, with a specific emphasis on text entailment. This research endeavors to enhance the performance of Indonesian NLP benchmarks in text entailment by harnessing a variant of the Bidirectional Encoder Representations from Transformer (BERT) model tailored explicitly for the Indonesian language, known as IndoBERT. The approach in this study adopts a hybrid methodology, amalgamating the IndoBERT layer modification model by summing the last four hidden layers with neural networks. The performance evaluation of the hybrid models, combining the IndoBERT layer modification model with the Bidirectional Gated Recurrent Unit (BiGRU) model and the IndoBERT layer modification model with the Bidirectional Long Short-Term Memory (BiLSTM) model, yields the F1-scores of 0.87 and 0.84, respectively.

---

Analyzing Public Perception using Aspect Based Sentiment Analysis: Case Study of Capital Relocation Planning of Indonesia
10.1109/icicos62600.2024.10636903
The use of machine learning to analyze public perception has become a prominent theme in governance services, particularly in relation to Indonesia’s capital relocation planning. This focus arises due to the substantial budget allocated for the development of the new capital (IKN) from the state expenditure. Consequently, various public responses have emerged regarding this phenomenon, prompting an analysis to understand public perception of the capital city’s relocation. This study will employ machine learning technologies for sentiment analysis, specifically focusing on Aspect-Based Sentiment Analysis (ABSA) using public responses about the IKN relocation obtained directly from the social media platform X. ABSA is different with a traditional sentiment analysis method cause focused on each aspect and context. The proposed method of this research involves using Support Vector Machine (SVM) and Decision Tree algorithms to identify each aspect. The results of this study indicate that the SVM algorithm achieved an optimal testing accuracy of $85.78 \%$, with the majority of public discussion showing support for the IKN project. The sentiment analysis revealed several points conveyed by the public, including the perception that the development of the IKN project is necessary given the difficult economic conditions and its impact on the people and the environment.

---

BERT for Natural Language Processing in Bahasa Indonesia
10.1109/icicyta57421.2022.10038230
Indonesian is the national language of Indonesia. Besides Indonesian, there are 700 foreign and local languages used to communicate in Indonesia. Even though it is used by more than 275.7 million people, Indonesian and local Indonesian are still not getting more attention in the Natural Language Processing (NLP) community. Currently the Bidirectional Encoder Representations from Transformers or BERT model is a state of the art performance in NLP. This article aims to conduct a review of the BERT model in Indonesian and local Indonesian language. Some of the findings in this article can be used as ideas for developing NLP using the BERT model in Indonesian. From the search, found 7 pretrained BERT models in Indonesian and local Indonesian language. 5 of them are monolingual BERT Model in Bahasa Indonesia and 1 monolingual in Local Indonesian Language (Sundanese). Meanwhile, only 1 multilingual BERT model in Indonesian-Javanese-Sundanese. The majority of monolingual BERT models are in Indonesian, while only 1 monolingual BERT model is in the local Indonesian language, Sundanese. The downstream task of the Indonesian BERT model and the Indonesian local BERT model are sentiment analysis, classification, and text summarization. There are 3 extrinsic evaluation benchmarks for Indonesian BERT, namely IndoNLU, IndoNLG, and IndoLEM.

---

SigmaLaw-ABSA: Dataset for Aspect-Based Sentiment Analysis in Legal Opinion Texts
10.1109/iciis51140.2020.9342650
Aspect-Based Sentiment Analysis (ABSA) has been prominent and ongoing research over many different domains, but it is not widely discussed in the legal domain. A number of publicly available datasets for a wide range of domains usually fulfill the needs of researchers to perform their studies in the field of ABSA. To the best of our knowledge, there is no publicly available dataset for the Aspect (Party) Based Sentiment Analysis for legal opinion texts. Therefore, creating a publicly available dataset for the research of ABSA for the legal domain can be considered as a task with significant importance. In this study, we introduce a manually annotated legal opinion text dataset (SigmaLaw-ABSA) intended towards facilitating researchers for ABSA tasks in the legal domain. SigmaLaw-ABSA consists of legal opinion texts in the English language which have been annotated by human judges. This study discusses the sub-tasks of ABSA relevant to the legal domain and how to use the dataset to perform them. This paper also describes the statistics of the dataset and as a baseline, we present some results on the performance of some existing deep learning based systems on the SigmaLaw-ABSA dataset.

---

Sentiment Analysis of the New Indonesian Government Policy (Omnibus Law) on Social Media Twitter
10.1109/icimcis51567.2020.9354287
In this era of modern technology, people are always connected to the internet. Twitter is one of the most developed social media technologies. Countries that adhere to democratic governments usually need opinions from various sources to determine the level of satisfaction and level of acceptance of policies for decision makers, one source that can be used is Twitter. The quality of community satisfaction and the level of acceptance of good policies carried out by the government are important and become benchmarks for maintaining the harmony of state life in Indonesia. In this research study, the level of satisfaction quality and level of acceptance of policies from public reviews will be measured using sentiment analysis, targeting people on Twitter who mention new government policies (omnibus law) in Indonesia. To determine the level of quality of satisfaction and level of acceptance, the Support Vector Machine (SVM) methodology and sentiment analysis were used to classify reviews for the following 8 policy topics in the omnibus law; Increase SMEs, Administration, Area and Land, Employment, Licensing and Investment, Punishment, Research and Innovation, and Taxation. The results showed that topics related to employment were the topics that received the most reviews and negative sentiment from the public, while research and innovation were the topics that were the least reviewed by the public.

---

Transformer-Based Indonesian Language Model for Emotion Classification and Sentiment Analysis
10.1109/icitcom60176.2023.10442970
The rapid development of social networks has made much user-generated data accessible for public evaluation. These data can be used for multiple purposes, such as textual analysis of comments and reviews. This article employs a variant of the bidirectional encoder representations from Transformer (BERT) model designed explicitly for Bahasa Indonesia (called IndoBERT model) to enhance the performance of the Indonesian natural language understanding benchmark in tasks, such as sentiment analysis and emotion classification. The two tasks were tested using a hybrid method, combining the IndoBERT model's last hidden layer summation with a neural network model. The performance of the resulting model was assessed using the F1-score metric. The experimental results show that the proposed model attains an accuracy of 0.92 and 0.76 for sentiment analysis and emotion classification, respectively.

---

Exploring Youth Perspectives on Environmental Sustainability in Social Media Campaigns Using K-Means Clustering and LDA
10.1109/icocseti63724.2025.11019074
Indonesia is facing significant environmental challenges, including deforestation, which exacerbates climate change. To address this issues, non-governmental organizations (NGOs) have launched social media campaigns encouraging youth to adopt sustainable lifestyles. Despite the widespread awareness of environmental issues through these campaigns, there remains a disconnect between awareness and actual sustainable actions. This research investigates Indonesian youth’s perspectives on environmental sustainability campaigns by analyzing social media data. Using sentiment analysis techniques—including word embedding with IndoBERT, K-Means clustering, and Latent Dirichlet Allocation (LDA) topic modeling—the study identifies key themes in youth discourse, such as legal concerns, governance, and activism. The findings aim to provide insights into leveraging social media more effectively to bridge the gap between awareness and real-world sustainable practices.

---

Aspect-Based Sentiment Analysis of Consumer Reviews on Eco-Friendly Straws: Insights for Product Improvement and Sustainable Marketing
10.1109/icodsa67155.2025.11157042
This study takes a closer look at how consumers feel about eco-friendly straw products by applying Aspect-Based Sentiment Analysis (ABSA) to real customer reviews collected from several popular e-commerce platforms. The reviews were first cleaned and prepared through a series of text preprocessing steps using the NLTK and spaCy libraries, which helped standardize the language and make it easier to analyze. Through aspect extraction, the study identified key themes that consumers tend to focus on, such as the material and type of straw, its strength and durability, and even sensory aspects like flavor and mouthfeel. Sentiment analysis was then carried out using the VADER model, providing a clearer picture of how people evaluate these different features. Interestingly, while the overall sentiment leaned positive, especially regarding the durability of eco-friendly straws, there were notable concerns about other factors. Many users expressed dissatisfaction with the price point, describing the products as expensive compared to conventional options. Others criticized the texture and subtle taste that some materials left behind, which affected their enjoyment. Drawing from these insights, the study offers several recommendations for businesses hoping to increase the appeal of eco-friendly alternatives. These include improving product quality to address sensory complaints, refining pricing strategies to make the products more competitive, enhancing the user experience, and strengthening marketing efforts that emphasize the environmental advantages of choosing sustainable straws.

---

Sentiment analysis using Latent Dirichlet Allocation and topic polarity wordcloud visualization
10.1109/icoict.2017.8074651
Sentiment analysis is a field of study that analyzes sentiment. One method for doing sentiment analysis is Latent Dirichlet Allocation (LDA) that extracts the topic of documents where the topic is represented as the appearance of the words with different topic probability. Therefore, we need data representation in visual form that is easier to understand than text and tables. One form of data visualization is wordcloud that provides a visual representation of words frequency. This research will perform sentiment analysis from the students' comments toward a university, in this case the Universitas Diponegoro, using LDA and topic polarity wordcloud visualization. The purpose of this study is to generate the topic polarity wordcloud of the students' comments by using the best combination of parameters. The best combination is the parameter with the value of alpha 0.1, value of beta 0.1, number of topics 9, threshold 10−7, and perplexity values 8.07. Such parameter combination produces 3 topics as positive sentiment and 6 topics as negative sentiment. In addition, we also compare the proposed method to several algorithms such as Naïve Bayes and Logistic Regression. The final result shows that the proposed method outperforms the Naïve Bayes and Logistic Regression in terms of F-Measure by 61%, 54%, and 56%, respectively.

---

Exploring Patterns and Groupings in Environmental, Social, and Governance Practices: A Mapping and Clustering Analysis
10.1109/icscai61790.2024.10866773
Thus, it is critical to grasp and unravel the linkages and dynamics in the sphere of Environmental, Social and Governance (ESG) practices, primarily in the contemporary world where environmental sustainability, social contributions, and good governance are crucial for organizations regardless of industries. In the current research, the form of mapping and clustering analysis is incorporated for a review of these practices. Based on the heterogeneous dataset, we analyze ESG practices with the help of analytical methods such as; qualitative coding through NVivo and, applying cluster analysis to detect latent structures inherent in our data. Therefore, the following research analysis is initiated by dissecting ESG practices into environmental, social, and governance subcategories. Using advanced data analytics and data visualization, which undertake the complex and multilayered ESG issues, we are then able to see the relationships and patterns between different practices. By clustering the organizations, are grouped based on their perceived similarity in ESG profiles, thereby informing the researcher of emergent trends and typologies. Further, we also explore the spatial distribution of ESG practices, focusing on differentiation based on regions as well as the location of concentrations of innovation and compliance. By applying a geospatial analysis coupled with network mapping, the role of local contexts, regulatory frameworks and SEMs and their interactions on ESG outcomes are explained. This study helps to enrich the knowledge about the nature of ESG aspects and offer information for practice, which is of interest to investors, the government, and managers. Overall, this research provides a set of guidelines for strengthening the implementation of ESG practices in world enterprises by identifying and describing their relationships and clusters.

---

Improving Sentiment Analysis of Social Media Captions Through Advancements in NLP
10.1109/icscss60660.2024.10624998
This research study focused on the development and implementation of advanced natural language processing techniques and machine learning models to perform a sentiment analysis on social media captions from Twitter and Facebook. An open-source repository was used to gather data, and the compiled dataset of 1,500 records covered a variety of human emotions. The data was preprocessed to remove noise, normalize, and reduce the dimensionality using PCA. The three models, Support Vector Machine, Naive Bayes, and K-Nearest Neighbors, were trained on 70% of the dataset and tested on the remaining 30% in order to evaluate their performance to classify content into specific emotions. The results demonstrated the efficiency of all three models, with the SVM achieving the highest accuracy of 96.57% and superior precision, recall, and F1 of 0.97, 0.97, and 0.98, respectively. This machine learning model appeared to be the most efficient and powerful for high-dimensional data and multidimensional feature space, showing a 2D representation of the dataset points in 99% information retention. The NB, with an accuracy of 91.20%, demonstrated and confirmed the appropriateness of the probability approach to make accurate predictions despite the assumption of independence of features. This model used such features as 0.5, 1.2, 1.7, 1.8 to classify specific emotions into neutral or other. The final accuracy of the KNN was 88.70%, and its special features allowed clustering and classification of the data in lower dimensions using the PCA information. Thus, the three models demonstrated competitive results, but the SVM was identified as the most efficient when dealing with the data and complex pattern text classification. The confusion matrix also proved the minimum misclassification rate of SVM.

---

Aspect Based Sentiment Analysis with Self-Attention and Gated Convolutional Networks
10.1109/icsess49938.2020.9237640
Aspect based sentiment analysis (ABSA) is a fine-grained sentiment analysis task, whose main goal is to identify the sentiment polarity of an aspect in a sentence. A sentence may contain many different aspects, each of which may have different sentiment polarities. Based on the current researches in this area, ABSA can be divided into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). In the past, more commonly used method is to adopt the time serial algorithm such as Long Short-Term Memory (LSTM) or Recurrent Neural Network (RNN), which usually needs more training time and has complex structures. Moreover, many previous models lack the abilities to effectively learn the internal structure features of sentences. For ABSA, sometimes the sentence structure may significantly affect the final classification results. However, we found the excellent performance of self-attention algorithm and gating mechanism in some other related researches. Therefore, to solve the problems above, we build a new model based on gating mechanism, combined with convolutional neural networks (CNN) and self-attention mechanism. First, we use self-attention to extract the structural feature of the input, and integrate it with the features of the original sentence extracted by CNN. On such basis, we further combine the aspect-category or aspect-term of the input sentence to form the final sentiment feature. Experiments on SemEval datasets show the performance of our models and the effectiveness of the model is proved.

---

A Conceptual Hybrid Model for Fake Review Detection Using Implicit ABSA and Imbalanced Data
10.1109/icspc63060.2024.10862280
Detecting fake reviews is crucial for maintaining trust on e-commerce platforms, protecting consumers from deception, and promoting fair competition. Sentiment Analysis (SA) is widely used to analyze the tone and sentiment of reviews. However, traditional methods that analyze full review texts can be inefficient, increasing computation time and energy usage while missing key signs like lack of detail, repetitive language, or exaggerated emotions. Additionally, the imbalance between fake and genuine reviews (ranging from $\mathbf{9 0 \%}$ to $\mathbf{0. 1 \%}$ in real datasets) makes accurate detection difficult. To address these issues, Aspect-Based Sentiment Analysis (ABSA) provides a more targeted approach by focusing on specific product features. The objective of this research is to develop a hybrid model using Bidirectional Encoder Representations from Transformers (BERT) for implicit aspect extraction, Support Vector Machine (SVM) for sentiment classification, and Synthetic Minority Over-Sampling Technique (SMOTE) to address the imbalance between fake and genuine reviews. Key indicators include sentiment scores, cosine similarity, and word count. The research involved four phases: 1) problem assessment, 2) techniques for implicit ABSA, 3) addressing imbalanced datasets, and 4) developing the hybrid model. Preliminary findings suggest that the proposed model can significantly enhance fake review detection by effectively identifying implicit aspects and managing data imbalance. The model is expected to help improve detection accuracy compared to existing methods, leading to more reliable and efficient detection systems for e-commerce platforms.

---

Greenwashing Detection Mechanism Based on Multimodal Analysis and Social Intelligence
10.1109/iiai-aai63651.2024.00104
In recent years, there has been a growing public interest in sustainability issues, leading many companies to use social media to promote their sustainability efforts and enhance their public image. However, misleading and exaggerated greenwashing continues to emerge, which not only harms the rights and interests of consumers and investors, but also reduces market fairness and transparency. This study proposes an innovative mechanism that combines multimodal analysis with social intelligence to identify common traits among greenwashing claims. We have developed a classification model to assess the significance of image features and social interaction features in detecting greenwashing. Our research aims to guide regulatory agencies in effectively monitoring greenwashing and encourage companies to be more transparent in their sustainability claims. Through this work, we seek to raise awareness about greenwashing and support companies committed to genuine sustainability practices.

---

Novel AI Frameworks for ESG Performance Prediction: ESG-RAG and SSA-iLSTM
10.1109/ijcnn64981.2025.11227579
Environmental, Social, and Governance (ESG) issues are becoming a focal point in global investment decision-making. However, accurately analyzing and predicting a company’s ESG performance remains a complex challenge due to the fragmented and heterogeneous nature of ESG data. Traditional approaches often rely on a single predictive model and conventional time series techniques, which face limitations in handling low-frequency, discontinuous, and unstructured data while lacking standardized processing methods. To address these challenges, this study proposes the ESG-RAG (Retrieval-Augmented Generation) framework for in-depth information mining, enabling the extraction of implicit insights from ESG reports to alleviate data sparsity issues. Additionally, a novel temporal prediction framework, SSA-iLSTM (Singular Spectrum Analysis integrated with iTransformer and Long Short-Term Memory), is introduced. The framework processes the target variable’s temporal signals through the iTransformer module while decomposing covariates using SSA and processing them with the LSTM model. A cross-attention mechanism fuses the outputs from both modules, enabling dynamic feature integration across multiple variables. Experimental results using datasets from Wind (Chinese companies) and Refinitiv (US companies) demonstrate that ESG-RAG and SSA-iLSTM significantly outperform baseline models in predicting ESG performance. This approach not only enhances prediction accuracy but also provides investors with valuable ESG insights, supporting more informed investment decisions.

---

Trustworthy and Interpretable AI for Robust Fraud Detection in Financial Transactions
10.1109/incet64471.2025.11140975
Financial fraud poses a critical threat to global economies, necessitating robust and transparent detection mechanisms. Traditional machine learning models achieve high predictive accuracy but often lack interpretability, limiting their adoption in high-stakes financial decision-making. This work introduces a novel Interpretable AI (IAI) framework that enhances fraud detection by integrating Explainable Boosting Machines (EBMs), SHAP (SHapley Additive exPlanations), and causal inference models. Our approach ensures high detection performance while providing clear, human-understandable explanations for each decision, fostering regulatory compliance and trust among financial institutions. Secondly, we use Federated Learning (FL) to facilitate privacy-preserving multi-party collaborative fraud detection among institutions without exchanging sensitive information. Experimental testing on actual transaction datasets illustrates 20–30% better recall of fraud compared to traditional black-box models, with retaining computational efficiency. We also test the performance of our model using adversarial evaluation to ensure robustness against complex fraud patterns. The suggested IAI framework is compatible with new regulatory needs and ethical AI standards, making it an affordable and scalable solution for future-proofed fraud prevention systems.

---

Aspect-Based Sentiment Analysis of Job Reviews
10.1109/isas66241.2025.11101750
Online job reviews provide critical insights into employee experiences, aiding organizations in improving workplace dynamics and informing job seekers about aspects like worklife balance, career growth, compensation, and company culture. This study proposes an aspect-based sentiment analysis framework for Glassdoor. We preprocess a Kaggle dataset of 75,012 reviews, extract contextual embeddings with BERT, and capture sequential dependencies with LSTM to generate sentiment scores for four aspects. The hybrid BERT-LSTM model helped to achieve an accuracy of 93.18%. SARIMAX forecasting models predict sentiment trends over time, with a Mean Squared Error of 0.01-0.03. The developed web application includes various features such as company-wise aspect-based scores, spatial and temporal analysis to present aspect scores based on location and time. The web application also integrates job opening alerts, enhancing its utility for career exploration. This approach offers organizations actionable insights into employee feedback and supports job seekers in decision making.

---

UTtoKB: a Model for Semantic Relation Extraction from Unstructured Text
10.1109/ismsit52890.2021.9604538
In this paper, a model prototype called UTtoKB has been built. It extracts semantic relationships from an unstructured text based on ontology. The model is a pipeline steps based on natural language processing (NLP) tasks and tools like Coreference Resolution (CR), Named Entity Recognition (NER), Semantic Role Labeling (SRL), and Part of Speech (PoS) Tagging. WordNet is the tool used to measure similarities between entities to convert them into ontology concepts and properties. The model works fine in specific domains, while performance degrades in other domains due to the instability of WordNet performance in finding semantic similarities.

---

Mutual Attention Network for Multi-label Emotion Recognition with Graph-Structured Label Representations
10.1109/iucc65928.2024.00040
Multi-label emotion classification (MLEC) tasks have gained significant attention due to their closer alignment with real-life emotional expressions. However, previous studies mostly focused on exploring semantic information in texts, while often neglecting the emotional information encoded in the labels. To address this, an advanced framework called the Text-Label Mutual Attention Network(TLMAN) model is proposed to thoroughly investigate the complex interactions between text and labels. The model effectively combines a text representation learning module, a label representation learning module and a mutual attention module to delve into the intricate relationships between labels and textual content, focusing on a holistic understanding of emotional information integration. To enhance MLEC methods, this study integrates Graph Convolutional Networks (GCN) to reveal label dependencies. The TLMAN model achieved AP scores of 76.93% and 77.38% on the Ren-CECps and NLPCC2018 datasets, significantly improving the performance of MLEC tasks.

---

Digital Twin of a Driver-in-the-Loop Race Car Simulation With Contextual Reinforcement Learning
10.1109/lra.2023.3279618
In order to facilitate rapid prototyping and testing in the advanced motorsport industry, we consider the problem of imitating and outperforming professional race car drivers based on demonstrations collected on a high-fidelity Driver-in-the-Loop (DiL) hardware simulator. We formulate a contextual reinforcement learning problem to learn a human-like and stochastic policy with domain-informed choices for states, actions, and reward functions. To leverage very limited training data and build human-like diverse behavior, we fit a probabilistic model to the expert demonstrations called the reference distribution, draw samples out of it, and use them as context for the reinforcement learning agent with context-specific states and rewards. In contrast to the non-human-like stochasticity introduced by Gaussian noise, our method contributes to a more effective exploration, better performance and a policy with human-like variance in evaluation metrics. Compared to previous work using a behavioral cloning agent, which is unable to complete competitive laps robustly, our agent outperforms the professional driver used to collect the demonstrations by around 0.4 seconds per lap on average, which is the first time known to the authors that an autonomous agent has outperformed a top-class professional race driver in a state-of-the-art, high-fidelity simulation. Being robust and sensitive to vehicle setup changes, our agent is able to predict plausible lap time and other performance metrics. Furthermore, unlike traditional lap time calculation methods, our agent indicates not only the gain in performance but also the driveability when faced with modified car balance, facilitating the digital twin of the DiL simulation.

---

High-Speed Autonomous Racing Using Trajectory-Aided Deep Reinforcement Learning
10.1109/lra.2023.3295252
The classical method of autonomous racing uses real-time localisation to follow a precalculated optimal trajectory. In contrast, end-to-end deep reinforcement learning (DRL) can train agents to race using only raw LiDAR scans. While classical methods prioritise optimization for high-performance racing, DRL approaches have focused on low-performance contexts with little consideration of the speed profile. This work addresses the problem of using end-to-end DRL agents for high-speed autonomous racing. We present trajectory-aided learning (TAL) that trains DRL agents for high-performance racing by incorporating the optimal trajectory (racing line) into the learning formulation. Our method is evaluated using the TD3 algorithm on four maps in the open-source F1Tenth simulator. The results demonstrate that our method achieves a significantly higher lap completion rate at high speeds compared to the baseline. This is due to TAL training the agent to select a feasible speed profile of slowing down in the corners and roughly tracking the optimal trajectory.

---

Exploring Zero-shot Cross-lingual Aspect-based Sentiment Analysis using Pre-trained Multilingual Language Models
10.1109/mapr53640.2021.9585242
Aspect-based sentiment analysis (ABSA) has received much attention in the Natural Language Processing research community. Most of the proposed methods are conducted exclusively in English and high-resources languages. Leveraging resources available from English and transferring to low-resources languages seems to be an immediate solution. In this paper, we investigate the performance of zero-shot cross-lingual transfer learning based on pre-trained multilingual models (mBERT and XLM-R) for two main sub-tasks in the ABSA problem: Aspect Category Detection and Opinion Target Expression. We experiment on the benchmark data sets of six languages as English, Russian, Dutch, Spanish, Turkish, and French. The experimental results demonstrated that using the XLM-R model can yield relatively acceptable results for the zero-shot cross-lingual scenario.

---

Synergies Among Responsible Artificial Intelligence (RAI), Environmental, Social and Governance (ESG), and Sustainable Development Goals (SDGs)
10.1109/mci.2025.3593452
This paper explores the synergistic relationships among responsible artificial intelligence (RAI), environmental, social and governance (ESG), and sustainable development goals (SDGs), highlighting how their integration can create opportunities for organizations to enhance societal impact while increasing corporate value and fostering innovation. Through a systematic literature review, this study first provides foundational knowledge related to RAI, ESG, and SDGs, including their origins, core concepts, and main implementation challenges. Then, the relationships between RAI, ESG, and SDGs are examined and discussed through the analysis of connections and synergies among them. Based on this analysis, an integrated framework that synthesizes the relationships among RAI, ESG, and SDGs is proposed, demonstrating how they can be mutually reinforcing when implemented cohesively. Additionally, the paper discusses implications of this integrated approach for multiple stakeholders, while acknowledging implementation challenges and tensions. The study emphasizes the necessity of viewing RAI, ESG, and SDGs as interconnected frameworks rather than individual goals. Through this integrated lens, organizations can align RAI, ESG, and SDGs collectively in their strategic planning and operational practices, thereby promoting more effective and sustainable outcomes while addressing complex societal challenges in the AI era. The research contributes to both theoretical understanding and practical guidance for implementing holistic approaches that harness the transformative potential of AI technologies while ensuring that their development and deployment serve broader sustainability and social responsibility objectives.

---

New Avenues in Opinion Mining and Sentiment Analysis
10.1109/mis.2013.30
The Web holds valuable, vast, and unstructured information about public opinion. Here, the history, current use, and future of opinion mining and sentiment analysis are discussed, along with relevant techniques and tools.

---

Considering Sentiment Causes in In-Context Learning for Aspect-Based Sentiment Analysis
10.1109/mis.2025.3584862
Aspect-based sentiment analysis (ABSA) aims to identify aspect terms in texts and determine their sentiment polarities. The in-context learning paradigm, powered by large language models, has proven effective in low-resource scenarios, where the retrieval of effective demonstration examples is crucial. Existing retrieval methods prioritize semantic and syntactic similarities, overlooking the fact that sentiment is often driven by its underlying causes. Recognizing that similar causes tend to yield similar sentiments, we propose the semantic-causal contextual demonstration retrieval (SCCDR), a demonstration retriever that integrates semantic and syntactic information while explicitly modeling sentiment causes. SCCDR was trained using contrastive learning based on rich contextual signals, including semantics, aspect-sentiment relationships, syntactic structures, and sentiment causes. Experiments on four datasets show that SCCDR outperforms other retrieval methods, thereby effectively improving ABSA performance under the ICL paradigm.

---

From unstructured data to actionable intelligence
10.1109/mitp.2003.1254966
There's content everywhere, but not the information you need. Content analysis can organize a pile of text into a richly accessible repository. This article explains two key technologies for generating metadata about content - automatic categorization and information extraction. These technologies, and the applications that metadata makes possible, can transform an organization's reservoir of unstructured content into a well-organized repository of knowledge. With metadata available, a company's search system can move beyond simple dialogs to richer means of access that work in more situations. Information visualization, for example, uses metadata and our innate visual abilities to improve access. Besides better access, metadata enables intelligent switching in the content flows of various organizational processes - for example, making it possible to automatically route the right information to the right person. A third class of metadata applications involves mining text to extract features for analysis using the statistical approaches typically applied to structured data. For example, if you turn the text fields in a survey into data, you can then analyze the text along with other data fields. All these metadata-powered applications can improve your company's use of its information resources.

---

Deep learning for Aspect-based Sentiment Analysis
10.1109/mlise54096.2021.00056
With the popularity of the Internet and the amount of information people can get from the Internet increases exponentially, sentiment analysis depending on text information has become a rather important tool in daily life, which helps people improve the quality of products and their lifestyle. According to the different granularity of sentiment text, sentiment analysis can be divided into three levels: word, sentence, and document level. While coarse-grained sentiment analysis only focuses on the polarity of emotions and neglects the finer-grained tendency of emotions. Driven by this demand, some researchers put forward the aspect-based sentiment analysis. ABSA can be categorized into two basic steps: aspect extraction and sentiment classification, on which researchers are mainly focusing to improve its performance. Deep learning has gradually become a research hotspot in recent years and has been widely used in CV and NLP fields. Recently, some efforts have begun to apply deep learning to ABSA tasks. In this paper, we further summarize and analyze the recent achievements of ABSA in the application of different deep learning models in terms of their completeness, accuracy, and detail. We also summarize ABSA's current challenges and difficulties and look forward to the future development and improvement of ABSA based on the existing experience.

---

Environmental. Social and Governance (ESG) Scores Automation in Global Reporting Initiative (GRI) with Natural Language Processing
10.1109/netapps63333.2024.10823436
The increasing importance of Environmental, Social, and Governance (ESG) factors in making investment decisions has triggered a noteworthy transformation in the global investing scenario. This study investigates various distilled BERT models in Python, including all-MiniLM - L 12-v2, all-MiniLM-L6-v2, all-mpnet-base-v2, and paraphrase-MiniLM-L6v2, to improve the accuracy and efficiency of ESG scoring. The performance of these models will undertake a detailed evaluation employing the F1 score criteria, which will offer an understanding of the accuracy and consistency of the scoring outputs produced by each model. The reason for this detailed review is to pledge a strong valuation of the effectiveness of the automated scoring procedure. The study shows that utilising textual similarity for ESG rating considerably decreases the time needed in comparison to the conventional human scoring procedure. Throughout the time of this study, all-MiniLM-L6-v2 was used as the base comparison model, however, it was observed that the all-mpnet-base-v2 model provided a relatively better F1 score due to its value being determined by its 768-dimensional embeddings. Embeddings with better length possess the volume to cover more difficult and subtle semantic representations, therefore potentially yielding superior performance. In the result finding summary, all-mpnet-base-v2 has the highest F1 score compared with the experiment's sentence transformers.

---

Analyzing the Financial Impact of ESG News Sentiment on ESG Finance Trends
10.1109/platcon63925.2024.10830722
This paper examines the interconnections between environmental, social, and governance (ESG) financial trends and the sentiment analysis of ESG-related news from 2019 to 2022. A substantial corpus of news articles and ESG management data is subjected to analysis employing a range of techniques, including term frequency-inverse document frequency (TF-IDF), latent Dirichlet allocation (LDA) topic modeling, and sentiment analysis tools such as valence-aware dictionary and sentiment reasoner (VADER) and bidirectional encoder representations from transformers (BERT). This analysis identifies significant trends and relationships between sentiment direction, ESG ratings, and financial performance. The results demonstrate that while ESG ratings do not have a robust direct correlation with financial metrics, sentiment analysis of ESG news offers insights into the potential impact of ESG management. This study underscores the necessity for standardized ESG metrics to enhance the clarity and efficacy of measuring the financial impact of ESG management. These insights are intended to inform the formulation of policies and strategies for future ESG management.

---

Issues and Challenges of Aspect-based Sentiment Analysis: A Comprehensive Survey
10.1109/taffc.2020.2970399
The domain of Aspect-based Sentiment Analysis, in which aspects are extracted, their sentiments are analysed and sentiments are evolved over time, is getting much attention with increasing feedback of public and customers on social media. The immense advancements in this field urged the researchers to devise new techniques and approaches, each sermonizing a different research analysis/question, that cope with upcoming issues and complex scenarios of Aspect-based Sentiment Analysis. Therefore, this survey emphasized on the issues and challenges that are related to extraction of different aspects and their relevant sentiments, relational mapping between aspects, interactions, dependencies, and contextual-semantic relationships between different data objects for improved sentiment accuracy, and prediction of sentiment evolution dynamicity. A rigorous overview of the recent progress is summarized based on whether they contributed towards highlighting and mitigating the issue of Aspect Extraction, Aspect Sentiment Analysis or Sentiment Evolution. The reported performance for each scrutinized study of Aspect Extraction and Aspect Sentiment Analysis is also given, showing the quantitative evaluation of the proposed approach. Future research directions are proposed and discussed, by critically analysing the presented recent solutions, that will be helpful for researchers and beneficial for improving sentiment classification at aspect-level.

---

Hierarchical Interactive Multimodal Transformer for Aspect-Based Multimodal Sentiment Analysis
10.1109/taffc.2022.3171091
Aspect-based multimodal sentiment analysis (ABMSA) aims to determine the sentiment polarities of each aspect or entity mentioned in a multimodal post or review. Previous studies to ABMSA can be summarized into two subtasks: aspect-term based multimodal sentiment classification (ATMSC) and aspect-category based multimodal sentiment classification (ACMSC). However, these existing studies have three shortcomings: (1) ignoring the object-level semantics in images; (2) primarily focusing on aspect-text and aspect-image interactions; (3) failing to consider the semantic gap between text and image representations. To tackle these issues, we propose a general Hierarchical Interactive Multimodal Transformer (HIMT) model for ABMSA. Specifically, we extract salient features with semantic concepts from images via an object detection method, and then propose a hierarchical interaction module to first model the aspect-text and aspect-image interactions, followed by capturing the text-image interactions. Moreover, an auxiliary reconstruction module is devised to largely eliminate the semantic gap between text and image representations. Experimental results show that our HIMT model significantly outperforms the state-of-the-art methods on two benchmarks for ATMSC and one benchmark for ACMSC.

---

Mixture of Hybrid Prompts for Cross-Domain Aspect Sentiment Triplet Extraction
10.1109/taffc.2024.3487870
Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplets from the review of a target domain, utilizing knowledge from a source domain. As a newly proposed task, limited work has been devoted to it. Except for solving it in a zero-shot manner with in-domain models, recent work explores a bidirectional generative framework to generate pseudo-labeled target data. However, such a method suffers from low efficiency with two-stage training and unstable pseudo-label quality. In this paper, we propose a Hybrid Prompts Mixture (HiPM) method for cross-domain ASTE to fully utilize domain-independent knowledge. Within this method, given that syntax information is an essential linguistic feature for triplet extraction, we design a syntax-related hard prompt to transfer the structures. Additionally, aspects from different domains exhibit similarities in their respective categories. We take this shared information as the prototypes and enrich them through a warm-up step. The resulting prototypes then act as the source of soft prompts. We further mix the hard and soft prompts with the original sequence into a generative model to extract triplets. Experimental results show that our method outperforms baselines on twelve transfer pairs, and obtains a 1.48% average F1 score improvement over the state-of-the-art cross-domain ASTE model.

---

Using Natural Language Processing With Explainable AI Approach to Construct a Human-Centric Consumer Application for Financial Climate Disclosures
10.1109/tce.2023.3326953
Climate change is becoming an increasingly urgent issue. To encourage firms to include climate-related risk information in regular financial reports, the Task Force on Climate-related Financial Disclosures (TCFD) has developed a report format that provides detailed description of the risks and opportunities that enterprises will face due to climate change. Such information is of great concern for consumers, investors and regulators. However, manually accessing this information through individual financial reports is time-consuming. This research uses pre-trained models such as BERT, RoBERTa, and ClimateBERT to automate the detection and analysis of TCFD-related texts. The generative adversarial network (GAN) model is used to generate data with fewer labels, thereby improving classification performance as measured by accuracy, recall, precision, and F1-score. The detected texts are analyzed using explainable AI (XAI) to confirm which the text variables that will affect whether the paragraphs reflect internal support for climate change remediation efforts or lack of such support. In addition, the relationship between these variables and the final prediction results can be understood through the intensity value provided by XAI which reflects the degree of influence of each feature on the model detection results. The results show that the ClimateBERT model achieves a prediction accuracy rate of 90%, thus potentially helping consumers and investors better access important information to their consumption and investment decisions.

---

Aspect-Based Sentiment Analysis: A Survey of Deep Learning Methods
10.1109/tcss.2020.3033302
Sentiment analysis is a process of analyzing, processing, concluding, and inferencing subjective texts with the sentiment. Companies use sentiment analysis for understanding public opinion, performing market research, analyzing brand reputation, recognizing customer experiences, and studying social media influence. According to the different needs for aspect granularity, it can be divided into document, sentence, and aspect-based ones. This article summarizes the recently proposed methods to solve an aspect-based sentiment analysis problem. At present, there are three mainstream methods: lexicon-based, traditional machine learning, and deep learning methods. In this survey article, we provide a comparative review of state-of-the-art deep learning methods. Several commonly used benchmark data sets, evaluation metrics, and the performance of the existing deep learning methods are introduced. Finally, existing problems and some future research directions are presented and discussed.

---

Deep Learning Model for Interpretability and Explainability of Aspect-Level Sentiment Analysis Based on Social Media
10.1109/tcss.2023.3347664
The interactive attention graph convolution network (IAGCN), a novel model proposed in this article, will revolutionize aspect-level sentiment analysis (SA). IAGCN effectively addresses these key features, in contrast to prior research that ignored the meaning of aspect terms and their relationship with context. The model combines a modified dynamic weighting layer with bidirectional long short-term memory (BiLSTM) to accurately acquire context. It takes use of graph convolutional networks (GCNs) to encrypt syntactic information from the syntactic dependency tree. Furthermore, a method for interactive attention is employed to discover the intricate relationships between context and aspect terms, which results in the reconstruction of those terms' representations. Comparing the proposed IAGCN model to baseline models, impressive gains are made. Across five datasets, the model beats previous methods with an amazing improvement in F1 scores that ranges from 1.34% to 4.04% and an impressive improvement in accuracy that ranges from 0.56% to 1.75%. Additionally, the IAGCN model outperforms the global vectors (GloVe)-based strategy when the potent pretrained model bidirectional encoder representations from transformers (BERT) is included in the challenge, resulting in even greater improvements. The F1 score considerably increases from 2.59% to 7.55%, and accuracy increases from 1.47% to 3.95%, making the IAGCN model a standout performer in aspect-level SA.

---

Toward Knowledge Integration With Large Language Model for End-to-End Aspect-Based Sentiment Analysis in Social Multimedia
10.1109/tcss.2024.3484460
Aspect-based sentiment analysis (ABSA) aims to identify specific sentiment elements in social multimedia content. To address aspect extraction and sentiment prediction together, recent studies have utilized a sequence tagging approach, mainly leveraging pretrained language models (PLMs) with specific architecture and auxiliary subtasks. However, these approaches often overlook task-related knowledge and struggle to scale across different domains. With advances in large language models (LLMs), there is a rising trend in constructing generative ABSA models. Nevertheless, these techniques tend to emphasize specific frameworks and overlook comprehensive knowledge representation. To address these challenges while leveraging the advantages of LLM and PLM-based methods, we propose a hybrid knowledge integration framework (HFABGKI). It employs a parameter-efficient fine-tuning technique, allowing for plug-and-play integration with existing LLMs. To bridge the LLM and PLM-based models, HF-ABGKI incorporates a global label semantic representation for potential aspect tokens, in which a simplified gating mechanism is proposed to filter useful information. Experimental results from six public social multimedia datasets demonstrate that our approach can accurately extract aspect terms and predict their sentiment polarity, achieving state-of-the-art performance compared to existing ABSA methods.

---

Supervised and Unsupervised Aspect Category Detection for Sentiment Analysis with Co-occurrence Data
10.1109/tcyb.2017.2688801
Using online consumer reviews as electronic word of mouth to assist purchase-decision making has become increasingly popular. The Web provides an extensive source of consumer reviews, but one can hardly read all reviews to obtain a fair evaluation of a product or service. A text processing framework that can summarize reviews, would therefore be desirable. A subtask to be performed by such a framework would be to find the general aspect categories addressed in review sentences, for which this paper presents two methods. In contrast to most existing approaches, the first method presented is an unsupervised method that applies association rule mining on co-occurrence frequency data obtained from a corpus to find these aspect categories. While not on par with state-of-the-art supervised methods, the proposed unsupervised method performs better than several simple baselines, a similar but supervised method, and a supervised baseline, with an <inline-formula> <tex-math notation="LaTeX">$F_{1}$ </tex-math></inline-formula>-score of 67%. The second method is a supervised variant that outperforms existing methods with an <inline-formula> <tex-math notation="LaTeX">$F_{1}$ </tex-math></inline-formula>-score of 84%.

---

Learning Expert-Level Racing Strategies via Scheduled Cost Functions in Model Predictive Control
10.1109/tiv.2024.3465598
In racing sports, driving strategies necessitate meticulous control and optimal utilization of vehicle dynamics. Model predictive control (MPC) has emerged as an effective approach for imitating expert driving strategies. Traditional MPC methods typically rely on constant cost functions, which are not optimal in dynamic environments that require track-dependent strategies. This paper introduces a novel framework that enhances the imitation of expert strategies by incorporating a scheduled cost function into the MPC. We present an inverse model predictive control (iMPC) framework, equipped with a custom MPC formulation that adeptly integrates scheduled cost functions. By employing Gaussian process (GP) regression, our framework effectively maps the connection between trajectories and their respective scheduling costs, enabling dynamic adaptation of cost functions within MPC planning. Furthermore, we present a probabilistic modeling method that combines Bayesian optimization (BO) with GP. This method is designed to create datasets that closely mimic expert-level driving behaviors, enriching the data available for training and validating our iMPC approach. We evaluate our framework by emphasizing the goodness of fit and interpretability of the reconstructed cost functions and the resulting trajectories. Compared to standard imitation learning methods, our approach stands out in its ability to accurately restore trajectories. We validate our framework using human-in-the-loop expert data and demonstrate the superiority of our methodology by comparing it with a tracking MPC.

---

Survey on Aspect-Level Sentiment Analysis
10.1109/tkde.2015.2485209
The field of sentiment analysis, in which sentiment is gathered, analyzed, and aggregated from text, has seen a lot of attention in the last few years. The corresponding growth of the field has resulted in the emergence of various subareas, each addressing a different level of analysis or research question. This survey focuses on aspect-level sentiment analysis, where the goal is to find and aggregate sentiment on entities mentioned within documents or aspects of them. An in-depth overview of the current state-of-the-art is given, showing the tremendous progress that has already been made in finding both the target, which can be an entity as such, or some aspect of it, and the corresponding sentiment. Aspect-level sentiment analysis yields very fine-grained sentiment information which can be useful for applications in various domains. Current solutions are categorized based on whether they provide a method for aspect detection, sentiment analysis, or both. Furthermore, a breakdown based on the type of algorithm used is provided. For each discussed study, the reported performance is included. To facilitate the quantitative evaluation of the various proposed methods, a call is made for the standardization of the evaluation methodology that includes the use of shared data sets. Semanticallyrich concept-centric aspect-level sentiment analysis is discussed and identified as one of the most promising future research direction.

---

Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis
10.1109/tkde.2023.3250499
Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment analysis. To better comprehend long complicated sentences and obtain accurate aspect-specific information, linguistic and commonsense knowledge are generally required in this task. However, most current methods employ complicated and inefficient approaches to incorporate external knowledge, e.g., directly searching the graph nodes. Additionally, the complementarity between external knowledge and linguistic information has not been thoroughly studied. To this end, we propose a knowledge graph augmented network KGAN, which aims to effectively incorporate external knowledge with explicitly syntactic and contextual information. In particular, KGAN captures the sentiment feature representations from multiple different perspectives, i.e., context-, syntax- and knowledge-based. First, KGAN learns the contextual and syntactic representations in parallel to fully extract the semantic features. Then, KGAN integrates the knowledge graphs into the embedding space, based on which the aspect-specific knowledge representations are further obtained via an attention mechanism. Last, we propose a hierarchical fusion module to complement these multi-view representations in a local-to-global manner. Extensive experiments on five popular ABSA benchmarks demonstrate the effectiveness and robustness of our KGAN. Notably, with the help of the pretrained model of RoBERTa, KGAN achieves a new record of state-of-the-art performance among all datasets.

---

Neural Abstractive Summarization for Long Text and Multiple Tables
10.1109/tkde.2023.3324012
Abstractive summarization aims to generate a concise summary covering the input document's salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries’ informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and dozens of tables in each report document. To deal with the scarcity of available datasets, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, FINDSum has two subsets for summarizing each company's results of operations and liquidity. Besides, we present four types of summarization methods to jointly consider text and table content when summarizing reports. Additionally, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Our summarization methods significantly outperform advanced baselines, which verifies the necessity of incorporating textual and tabular data when summarizing report documents. We also conduct extensive comparative experiments to identify vital model components and configurations that can improve summarization results.

---

FeBT: A Feature Balancing Transformer for Corporate ESG Forecasting
10.1109/tkde.2025.3560137
Environmental, social, and governance (ESG) serves as a crucial indicator for evaluating firms in terms of sustainable development. However, the existing ESG evaluation systems suffer from limitations, such as narrow coverage, subjective bias, and lack of timeliness. Therefore, there is a pressing need to leverage machine learning methods to predict the ESG performance of firms using their publicly available data. Traditional machine learning models encounter the feature imbalance problem due to the heterogeneity in ESG-related features. Common approaches typically involve unfolding all features, thereby granting high-dimensional folding features greater exposure and accessibility to downstream models, which results in the neglect of low-dimensional features. To fill the research gap regarding fully using the heterogeneous features of enterprises to enhance AI-based ESG prediction performance, we propose the Feature Balancing Transformer (FeBT), a model based on autoencoders and Transformer blocks. FeBT incorporates a novel feature balancing technique that compresses and enhances high-dimensional features from imbalanced data into low-dimensional representations, thereby ensuring a more balanced impact of high-dimensional and low-dimensional features on the model’s performance in the downstream ESG forecasting module. Extensive experiments verified the superior performance of FeBT compared with state-of-the-art methods in real-world ESG-related datasets and evidenced that our feature balancing module provides significant insights from high-dimensional folding features.

---

Dual Causes Generation Assisted Model for Multimodal Aspect-Based Sentiment Classification.
10.1109/tnnls.2024.3415028
Multimodal aspect-based sentiment classification (MABSC) aims to identify the sentiment polarity toward specific aspects in multimodal data. It has gained significant attention with the increasing use of social media platforms. Existing approaches primarily focus on analyzing the content of posts to predict sentiment. However, they often struggle with limited contextual information inherent in social media posts, hindering accurate sentiment detection. To overcome this issue, we propose a novel multimodal dual cause analysis (MDCA) method to track the underlying causes behind expressed sentiments. MDCA can provide additional reasoning cause (RC) and direct cause (DC) to explain why users express certain emotions, thus helping improve the accuracy of sentiment prediction. To develop a model with MDCA, we construct MABSC datasets with RC and DC by utilizing large language models (LLMs) and visual-language models. Subsequently, we devise a multitask learning framework that leverages the datasets with cause data to train a small generative model, which can generate RC and DC, and predict the sentiment assisted by these causes. Experimental results on MABSC benchmark datasets demonstrate that our MDCA model achieves the state-of-the-art performance, and the small fine-tuned model exhibits superior adaptability to MABSC compared to large models like ChatGPT and BLIP-2.

---

On Interpretability of Artificial Neural Networks: A Survey
10.1109/trpms.2021.3066428
Deep learning as performed by artificial deep neural networks (DNNs) has achieved great successes recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, the interpretability of DNNs has recently attracted much research attention. In this article, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies on interpretability of neural networks, describe applications of interpretability in medicine, and discuss future research directions, such as in relation to fuzzy logic and brain science.

---

Automated Essay Scoring Using Natural Language Processing And Text Mining Method
10.1109/tssa51342.2020.9310845
The use of technology really helps to maximized the effectiveness and efficiency of work expecially in the education field. Elearning is the concept of education that has begun to be widely implemented at this covid-19 pandemic to avoid the spread of transmission through social distancing. One of elearning types is essay but for large participants, it need much effort for evaluate by human rater. The inconsistency of assessment by the rater due to fatigue can also affect the quality of the assessment. Developing a system that can learn and understand on its own without having to be repeatedly programmed by humans used machine learning and computational linguistics to study the interaction between computers and human natural language used natural language processing proposed in this research. Natural language processing and text mining methods are able to provide a good assessment which is influenced by several processes, namely tokenization, stopword, stemming and support with the number of keywords, and the synonym of more complex keywords. The automated essay scoring system is proven to provide consistent and objective assessments and is able to approach human raters assessments.

---

A Generative Adversarial Network Based Learning Approach to the Autonomous Decision Making of High-Speed Trains
10.1109/tvt.2022.3141880
Nowadays, the autonomous driving transportation systems are at the heart of both academic and industry research for the distinguished advantages including increased network capacity, enhanced punctuality, greater flexibility and improved overall safety level. With the responsibility of transporting passengers in a safe, comfortable and efficient way, the decision making method plays a critical position in the autonomous driving of high-speed trains. Focusing on solving the autonomous decision-making problem, this paper proposes a novel learning based framework by combining the deep learning technology with the distributed tracking control approach. To cope with the data insufficiency problem in training the deep learning network that may severely degrade the prediction performance, a generative adversarial network (GAN) based data argumentation scheme is proposed to generate data samples that have the same distribution with the actual data samples, and a hybrid learning network is constructed to predict the speed trajectory from the multi-attribute data with both temporal sequences and static features. Then, based on the model predictive control (MPC) scheme, a distributed tracking control model is formulated to minimize the tracking deviations and balance the performance of punctuality, energy-efficiency and riding comfort. Further, the dual decomposition technique is adopted to deal with the coupling constraints for the safe distance headway such that the separation for the autonomous driving of high-speed trains is achieved. Finally, simulation experiments based on actual scenarios of the Beijing-Shanghai high-speed railway are conducted to illustrate the effectiveness of our methods.

---

Telemetry-based optimisation for user training in racing simulators
10.1109/vs-games.2017.8055808
Motorsports require training and dedication to master, supplemented by hours of rote learning and mentoring by experts. This study explores the question of whether a serious game is a powerful enough pedagogical tool to be gainfully employed in the training of race drivers. A system of heuristics is proposed for a novel telemetry-based feedback model for contextual real-time suggestions. The model has been integrated into a racing simulation game and a study of its performance is reported here. The study consists of 27 participants, partitioned into two groups, to provide a control for the experiment. Two questionnaires have been used to acquire demographic information about the participants and help control for factors such as experience. Quantitative results show that there is an improvement for the group using the feedback system, although this improvement dissipates when the feedback is disabled again for the experimental group. Analysis of the initial results are encouraging, with the model showing promise. Additionally, the lack of cognitive retention on behalf of the participants when feedback was disabled merits further investigation and future work.

---

Evaluating ESG Impacts in African Cities through Topic-Level Sentiment Analysis
10.1109/wincom59760.2023.10322894
In this paper, we focus on describing the measurement of Environmental, Social, and Governance (ESG) impacts in African cities and urban areas. We use Topic-Based Sentiment Analysis methodologies applied to a variety of social media collected dataset. Our solution aims to understand the population’s perception of ESG impacts in the African cities, given their potential influence on societies, leading people to express their thoughts through communication channels. The originality of our work lies in providing a systematic mapping of any collected data with ESG issues. It also ensures comprehensive insights into the subject matter.The solution encompasses a pipeline starting with data collection modules, followed by enrichment based on an ESG framework. This facilitates a semi-supervised topic-modeling solution using BERTopic and domain-based keyword filtering. For Sentiment Analysis, we present a fine-grained methodology calculating sentiment at the topic level. Finally, we propose diverse Key Performance Indicators (KPIs) for comprehensive data understanding and evaluating the proposed solution.

---

The Use of Simulation with Machine Learning and Optimization for a Digital Twin-A Case on Formula 1 DSS
10.1109/wsc57314.2022.10015299
The implementation of a digital twin presents a challenging environment for simulation. One challenge is the need for fast execution speed to maintain synchronization with the real system. When providing predictive outcomes, the complementary use of simulation with machine learning and optimization software may be employed to achieve this aim. The article investigates the use of simulation, machine learning and optimization in terms of providing a digital twin capability. The article presents a case on Formula 1 or F1 competition, where a decision support system (DSS) framework is presented to explore a digital twin capability.

---

The Use of Blockchains to Enhance Sustainability Reporting and Assurance*
10.1111/1911-3838.12241
The changing dynamics of the accounting profession have been strongly influenced by emerging technologies and the demand for nontraditional metrics and information by stakeholders and regulators. In this article, we perform an exploratory content analysis to examine the role that blockchain technology can play in enhancing sustainability reporting and assurance. The benefits to companies and assurance professionals in using the distributed ledger technology of blockchain are increased trust, transparency, and traceability, which matches stakeholders' demands as it relates to sustainability reporting. This article identifies and analyzes potential and current use cases of blockchain in the United States and Canada to assist accountants and auditors in preparing and reviewing sustainability information. We highlight how augmenting traditional reporting systems with blockchain can overcome problems with sustainability reporting. We discuss implications for practice in detail—finding that blockchain is well‐positioned to provide reliable tracking and custodial support as it relates to sustainability information currently being self‐reported by many firms, such as greenhouse gas emissions, conflict mineral disclosure, or product provenance, among others. Expanded adoption of blockchains by companies will lead to higher‐quality information being included in sustainability reports and allow assurance professionals to verify a wider range of information, potentially leading to uniform standards in the evaluation of sustainability reports. L'UTILISATION DE LA CHAINE DE BLOCS POUR AMELIORER L'INFORMATION ET L'ASSURANCE EN MATIERE DE DURABILITE La dynamique evolutive de la profession comptable est fortement influencee par les technologies emergentes et la demande de mesures et informations non conventionnelles de la part des parties prenantes et des organismes de reglementation. Dans cet article, nous effectuons une analyse de contenu exploratoire afin d'examiner le role que la technologie de la chaine de blocs peut jouer pour ameliorer l'information et l'assurance en matiere de durabilite. Pour les entreprises et les professionnels de l'assurance, les avantages lies a l'utilisation de la technologie du registre distribue de la chaine de blocs sont une confiance, une transparence et une tracabilite accrues, ce qui correspond aux demandes des parties prenantes concernant l'information relative a la durabilite. Dans la presente etude, nous cernons et analysons des cas d'utilisation de la chaine de blocs eventuels et actuels aux Etats‐Unis et au Canada pour aider les comptables et les auditeurs a preparer et a passer en revue l'information sur la durabilite. Nous montrons de quelle facon l'ajout de la chaine de blocs aux systemes de communication d'information conventionnels peut resoudre des problemes associes a la production de rapports financiers sur la durabilite. Nous discutons en detail des consequences de cette technologie sur la pratique comptable, en etablissant que la chaine de blocs est en bonne position pour fournir un suivi fiable et une certaine forme de surveillance concernant l'information sur la durabilite actuellement communiquee par de nombreuses entreprises, comme les emissions de gaz a effet de serre, les declarations sur le minerai de conflit ou la provenance des produits. L'adoption a plus grande echelle de la chaine de blocs par les entreprises permettra d'integrer de l'information de meilleure qualite aux rapports sur la durabilite et donnera aux professionnels de l'assurance l'occasion de verifier davantage de renseignements, ce qui pourrait entrainer l'etablissement de normes uniformes pour l'evaluation de l'information sur la durabilite.

---

<scp>FinBERT</scp> : A Large Language Model for Extracting Information from Financial Text*
10.1111/1911-3846.12832
We develop FinBERT, a state-of-the-art large language model that adapts to the finance domain. We show that FinBERT incorporates finance knowledge and can better summarize contextual information in financial texts. Using a sample of researcher-labeled sentences from analyst reports, we document that FinBERT substantially outperforms the Loughran and McDonald dictionary and other machine learning algorithms, including naïve Bayes, support vector machine, random forest, convolutional neural network, and long short-term memory, in sentiment classification. Our results show that FinBERT excels in identifying the positive or negative sentiment of sentences that other algorithms mislabel as neutral, likely because it uses contextual information in financial text. We find that FinBERT's advantage over other algorithms, and Google's original bidirectional encoder representations from transformers (BERT) model, is especially salient when the training sample size is small and in texts containing financial words not frequently used in general texts. FinBERT also outperforms other models in identifying discussions related to environment, social, and governance issues. Last, we show that other approaches underestimate the textual informativeness of earnings conference calls by at least 18% compared to FinBERT. Our results have implications for academic researchers, investment professionals, and financial market regulators. This article is protected by copyright. All rights reserved.

---

A Textual Analysis of US Corporate Social Responsibility Reports
10.1111/abac.12182
We employ computer-based textual analysis to examine disclosure patterns for a sample of US corporate social responsibility (CSR) reports from the period 2002-2016. Starting from 466 features commonly used in computational linguistics, our results show that the linguistics or disclosure patterns in CSR reports can be used to accurately predict the actual CSR performance type of CSR reporters. Specifically, we find that the two most commonly used disclosure characteristics, number of words and number of sentences, alone can be used to predict reporting firms' CSR performance type with 81% accuracy. The accuracy of prediction increases to 96% when the top 50 linguistics features most relevant to firms' CSR performance are included in the prediction model. In addition, we find that the linguistic features of CSR disclosure identified by our study are incrementally value relevant to investors even after controlling for the actual CSR performance score from the professional CSR rating agencies. This finding suggests that the linguistic features of CSR disclosure can be an important venue for capital market participants in evaluating firms' CSR performance type, especially when professional CSR performance ratings are not available.

---

The Effectiveness of News-Based <scp>ESG</scp> Sentiment for Predicting Stock Returns: Evidence From China
10.1111/acfi.70015
ABSTRACT This study examines the impact of ESG sentiment on the forecast of excess stock returns. We construct a monthly ESG sentiment index ( S ESG ) that captures the tone of ESG news coverage, distinguishing between positive and negative sentiment. Our findings indicate that S ESG predictions of market excess returns are statistically significant in both in‐sample and out‐of‐sample analyses, with stronger predictive power during high sentiment periods compared to low sentiment periods. Furthermore, economic tests demonstrate that S ESG generates a high Sharpe ratio and utility gains for investors, highlighting its potential economic benefits as a predictor in the increasingly important field of ESG investments.

---

Contextualized Cross-Domain Aspect Sentiment Transformer: A Fine-Grained Aspect-Centric Approach for Enhanced Context-Aware Sentiment Analysis
10.1111/coin.70081
Context‐aware sentiment analysis (CASA) is increasingly critical due to the complex nature of sentiments in digital communication. Traditional sentiment analysis often fails to capture the nuances in context‐rich environments, facing challenges like disentangling sentiments, adapting to dynamic contexts, and handling cross‐domain variations. Additionally, data sparsity and subjectivity in sentiment interpretation complicate CASA. To address these challenges, this paper proposed a Contextualized Cross‐Domain Aspect Sentiment Transformer Network (CC‐ASTN) that integrates BERT‐based embeddings with aspect‐specific embeddings for nuanced contextual and aspect‐specific sentiment details. A core feature of CC‐ASTN is its fine‐grained sentiment analysis, which begins with word‐level analysis to discern subtle emotional cues and modifiers, enabling the model to detect sentiment nuances. A novel dual attention mechanism dynamically adjusts focus based on relevance, resolving ambiguities. Advanced domain adversarial training and transfer learning techniques ensure effective cross‐domain adaptation, while data augmentation and few‐shot learning strategies tackle data sparsity. A hierarchical approach for sentiment analysis breaks down complex sentiments into granular components. The model's robustness is enhanced through dropout, layer normalization, and noise contrastive estimation (NCE), ensuring stability and performance consistency. A composite loss function balances multiple objectives, facilitating precise, domain‐neutral sentiment analysis. Additionally, the model integrates real‐time feedback mechanisms and leverages a multi‐modal approach by incorporating textual, visual, and contextual data for holistic analysis. The CC‐ASTN model demonstrates significant efficiency, with training typically taking ˜5 h. Experimental results validate the model's effectiveness, showing significant improvements over existing methods on the SemEval2014 Task 4 and SentiHood datasets. The model achieves inference times of ˜2 s, highlighting its suitability for real‐time applications. These findings underscore CC‐ASTN's efficacy as an advanced solution for context‐aware sentiment analysis, capturing sentiment variations and aspect‐level nuances with high precision and efficiency. Its adaptability to rapidly changing trends and real‐time feedback integration enhance its applicability in dynamic, real‐world scenarios, making it an effective tool for sentiment analysis across a range of fields.

---

Self-Adaptive LLM Instructions Optimization for Aspect-Based Sentiment Analysis by Incorporating Emotion-Oriented In-Contexts
10.1111/coin.70129
Aspect‐based Sentiment Analysis (ABSA) is a vital NLP task that identifies sentiment towards specific entities or aspect terms within a text. Recently, large language models (LLMs) have shown impressive capabilities in semantic comprehension and logical inference. However, LLM hallucinations pose challenges in accurately determining sentiment polarity for aspect terms, leading to performance issues. Moreover, current ABSA methods often fail to fully leverage the vast prior knowledge embedded within LLMs, resulting in suboptimal classification outcomes for specific aspects. Inspired by these challenges, we propose the BYD‐OBS‐ABSA framework—‘Beyond Simple Observations, Embracing Comprehensive Contextual Insights’ for ABSA tasks. This framework leverages unique in‐context constraints, backgrounds, and analogical reasoning to address LLM hallucinations and uses self‐adaptive bootstrap instructions optimization to enhance LLM predictions. BYD‐OBS‐ABSA integrates various in‐context augmentation strategies, including emotion‐oriented backgrounds, constraints, and analogical reasoning. BYD‐OBS‐ABSA further improves initial LLM instructions through adaptive iterative optimization using a random search bootstrap algorithm, maximizing the benefits of LLM prompting. Extensive zero/few‐shot experiments with GPT‐3.5‐turbo across six public datasets validate the effectiveness and robustness of our framework, even surpassing human judgment in certain scenarios.

---

Corporate governance and the use of external assurance for integrated reports
10.1111/corg.12430
Research Question/Issue This paper investigates the relationship between the use of external assurance for testing integrated reports (ESG assurance) and firm-level governance features: the board of directors, the audit and/or risk committee, and the internal audit department. Data are collected from South Africa where integrated reporting and corporate governance practices are mature and listed companies have had more time to implement ESG assurance than in other countries. Research Findings/Insights Monitoring attributes of boards of directors promotes the use of ESG assurance which provide both limited (moderate) and reasonable (high) assurance. The monitoring attributes of the audit and risk committees limit the use of limited assurance but are associated with the greater use of reasonable assurance. In contrast, internal audit functions are not affecting the use of ESG assurance. Theoretical/Academic Implications The study provides one of the first accounts of how firm-level governance promotes or reduces the use of external assurance in an integrated reporting context. The research also frames ESG assurance as part of the broader corporate governance machinery rather than seeing assurance and governance as separate issues. Practitioner/Policy Implications Overall, the findings suggest that ESG assurance is an important part of a combined assurance model. As those charged with governance become more proactive in ensuring the credibility of their organizations' corporate reports, they not only choose to appoint an external assuror but also rely on more extensive testing designed to provide higher levels of assurance.

---

Does an optimistic tone in annual reports predict better financial and non-financial performance?
10.1111/emre.70032
Abstract In the current paper, we investigate whether management adopts an optimistic disclosure tone to impress the corporate audience or to provide incremental information (II) by anticipating positive corporate performance. Specifically, we test whether an optimistic tone in annual reports (ARs) is a positive predictor of better financial and non‐financial (environmental and social) performance in the future, or whether impression management (IM) techniques obfuscate unsatisfactory performance. Using 7140 ARs issued by publicly listed firms in the Eurostoxx600 index from 2006 to 2019, we document mixed results. First, according to the II approach, managers use an optimistic tone in ARs to communicate incremental and value‐relevant information on corporate financial performance (CFP), thus aligning the perceptions of stakeholders with their personal perceptions. Moreover, managers adopt an opportunistic IM approach to communicate corporate environmental and social performance (CEP; CSP). Furthermore, our findings indicate that environmental, social, and governance (ESG) engagement positively moderates the incremental information effect on CFP disclosure, whereas corporate governance (CG) mechanisms positively moderate the link between an optimistic tone in ARs and both CEP and CSP.

---

Identifying greenwashing in corporate-social responsibility reports using natural-language processing
10.1111/eufm.12509
Abstract A textual analysis of corporate‐social responsibility (CSR) reports reveals that companies engaged in environmental violations report differently from firms with a clean record. The violators issue longer, more positive and more frequent reports to relay environmental content that is more copious but less readable. The violator firms appear to modify their reporting practices right after committing a violation. The findings suggest that culpable firms exploit the current unregulated–unaudited state of CSR reporting as a means of greenwashing and call for institutional change. Our results are robust to a number of industry‐firm characteristics, including board composition, ownership dispersion and international presence.

---

Artificial intelligence governance: Ethical considerations and implications for social responsibility
10.1111/exsy.13406
A number of articles are increasingly raising awareness on the different uses of artificial intelligence (AI) technologies for customers and businesses. Many authors discuss about their benefits and possible challenges. However, for the time being, there is still limited research focused on AI principles and regulatory guidelines for the developers of expert systems like machine learning (ML) and/or deep learning (DL) technologies. This research addresses this knowledge gap in the academic literature. The objectives of this contribution are threefold: (i) It describes AI governance frameworks that were put forward by technology conglomerates, policy makers and by intergovernmental organizations, (ii) It sheds light on the extant literature on ‘AI governance’ as well as on the intersection of ‘AI’ and ‘corporate social responsibility’ (CSR), (iii) It identifies key dimensions of AI governance, and elaborates about the promotion of accountability and transparency; explainability, interpretability and reproducibility; fairness and inclusiveness; privacy and safety of end users, as well as on the prevention of risks and of cyber security issues from AI systems. This research implies that all those who are involved in the research, development and maintenance of AI systems, have social and ethical responsibilities to bear toward their consumers as well as to other stakeholders in society.

---

Doc-KG: Unstructured documents to knowledge graph construction, identification and validation with Wikidata
10.1111/exsy.13617
The exponential growth of textual data in the digital era underlines the pivotal role of Knowledge Graphs (KGs) in effectively storing, managing, and utilizing this vast reservoir of information. Despite the copious amounts of text available on the web, a significant portion remains unstructured, presenting a substantial barrier to the automatic construction and enrichment of KGs. To address this issue, we introduce an enhanced Doc‐KG model, a sophisticated approach designed to transform unstructured documents into structured knowledge by generating local KGs and mapping these to a target KG, such as Wikidata. Our model innovatively leverages syntactic information to extract entities and predicates efficiently, integrating them into triples with improved accuracy. Furthermore, the Doc‐KG model's performance surpasses existing methodologies by utilizing advanced algorithms for both the extraction of triples and their subsequent identification within Wikidata, employing Wikidata's Unified Resource Identifiers for precise mapping. This dual capability not only facilitates the construction of KGs directly from unstructured texts but also enhances the process of identifying triple mentions within Wikidata, marking a significant advancement in the domain. Our comprehensive evaluation, conducted using the renowned WebNLG benchmark dataset, reveals the Doc‐KG model's superior performance in triple extraction tasks, achieving an unprecedented accuracy rate of 86.64%. In the domain of triple identification, the model demonstrated exceptional efficacy by mapping 61.35% of the local KG to Wikidata, thereby contributing 38.65% of novel information for KG enrichment. A qualitative analysis based on a manually annotated dataset further confirms the model's excellence, outshining baseline methods in extracting high‐fidelity triples. This research embodies a novel contribution to the field of knowledge extraction and management, offering a robust framework for the semantic structuring of unstructured data and paving the way for the next generation of KGs.

---

Environmental, social and governance reporting in annual reports: A textual analysis
10.1111/fmii.12132
Considering environmental, social, and governance (ESG) factors becomes increasingly important for companies and investors. However, ESG is not clearly defined so far and, therefore, it is difficult to measure the ESG activity of companies. We analyze the extent and changes in 10-K reports and proxy statements on ESG, using a textual analysis and creating an ESG dictionary. The results show an average of 4.0 % ESG words on total words in the reports. The ESG word list with 482 items can be used to quantitatively examine the extent of ESG reporting, which will be helpful especially for SRI investors. Our classification of 40 subcategories allows a highly granular analysis of different ESG related aspects. Moreover, indications for a relation between changes in reporting and real events, especially negative media presence, are detected. Regulatory bodies have to be aware of the use of such words and how they are used.

---

THIRD PARTIES, INFORMATION DISCLOSURE AND MONITORING INCENTIVES
10.1111/j.1467-9485.2008.00445.x
Within an incomplete contract setting, the paper analyses the role of third parties in ameliorating incentive problems arising in the context of financial contracts with costly verification and lender's bargaining power. Contrary to the findings of the bilateral lender–borrower relationship, characterised by no information revelation and possibly a breakdown of the market, it is shown that, in the presence of third parties, an optimal contract exists featuring partial information revelation and random monitoring. The importance of third parties is therefore not limited to improving efficiency, as it is when the contract offer comes from the informed party, but to ensure project realisation, and thus to ensure that the surplus that can arise from the project does not get lost.

---

Greenwash: Corporate Environmental Disclosure under Threat of Audit ∗
10.1111/j.1530-9134.2010.00282.x
We develop an economic model of “greenwash,” in which a firm strategically disclosesenvironmentalinformationandanactivistmayauditandpenalizethe firmfordisclosingpositivebutnotnegativeaspectsofitsenvironmentalprofile. We fully characterize the model’s equilibria, and derive a variety of predictions about disclosure behavior. We rationalize conflicting results in the empirical literature, finding a nonmonotonic relationship between a firm’s expected environmental performance and its environmental disclosures. Greater activist pressure deters greenwash, but induces some firms to disclose less about their environmental performance. Environmental management systems discourage firms with poor expected environmental performance from greenwashing, which may justify public policies encouraging firms to adopt them.

---

Validity Arguments for AI-Based Automated Scores: Essay Scoring as an Illustration
10.1111/jedm.12333
In this article, we argue that automated scoring engines should be transparent and construct relevant—that is, as much as is currently feasible. Many current automated scoring engines cannot achieve high degrees of scoring accuracy without allowing in some features that may not be easily explained and understood and may not be obviously and directly relevant to the target assessment construct. We address the current limitations on evidence and validity arguments for scores from automated scoring engines from the points of view of the Standards for Educational and Psychological Testing (i.e., construct relevance, construct representation, and fairness) and emerging principles in Artificial Intelligence (e.g., explainable AI, an examinee's right to explanations, and principled AI). We illustrate these concepts and arguments for automated essay scores.

---

Extended external reporting assurance: Current practices and challenges
10.1111/jifm.12127
This paper summarizes the UNCTAD ISAR WBCSD Webinar—Assurance on Sustainability Reports: Current Practices and Challenges, which explored views and practices on assurance of extended external reporting (EER) and identified challenges and potential ways forward. Stakeholders are demanding more accountability, as reflected in increased publication of EER and regulatory developments. EER can play an important role in rebuilding trust by catalyzing corporate focus and disclosure of business-centric matters material to stakeholders including strategy, business model, governance, and greater transparency on other material non-financial matters. Relatedly, EER cannot rebuild trust unless disclosures are credible and viewed as credible. Therefore, it is important that assurance, and other credibility enhancing techniques, is developed alongside EER frameworks and takes account of regulatory initiatives. We expand on lessons outlined during the Webinar by highlighting questions posed by participants, providing a historical overview of European regulatory developments (e.g., Directive 2014/95/EU and a forthcoming revision), providing a historical overview of the IAASB’s development of ISAE 3000 and forthcoming guidance on addressing major challenges aimed at supporting EER assurance, and providing an overview of practice-focused publications addressing EER assurance. We conclude with an assessment of the way forward in regard to possible changes in the EER institutional setting, potential harmonization of EER standards, and the ability to provide reasonable versus limited assurance. Along with our companion paper (Venter and van Eck, 2021, 32), we contribute to the current discussion on EER assurance by providing a comprehensive assessment of the EER assurance landscape.

---

Are firms motivated to greenwash by financial constraints? Evidence from global firms' data
10.1111/jifm.12153
Corporate social responsibility is the balance between a firm's economic outcomes and environmental protection. However, investors face increasing difficulties in selecting assets with suitable environmental, social, and governance (ESG) policies because companies may “greenwash” their activities by, for example, make misleading ESG disclosures. Here, we investigate the determinants that lead to companies engaging in ESG greenwashing. By analyzing international large-cap companies across 47 countries and territories, we create a peer-relative greenwashing score to measure the magnitude of ESG greenwashing by companies. First, we measure and evaluate the greenwashing by analyzing ESG disclosures and creating peer-relative performance scores that consider the level of disclosure and the real ESG performance. Second, we show that companies' greenwashing decisions are motivated by financial constraints and thus the financial environment is a determinant of greenwashing behavior. Third, we describe how intermediation can alleviate financial constraints and decrease greenwashing behavior. Moreover, highly leveraged companies may have increased financial pressure and thus may enhance their greenwashing behavior. Our findings are robust according to several different measurements of financial constraint indicators.

---

Be good to be wise: Environmental, Social, and Governance awareness as a potential credit risk mitigation factor
10.1111/jifm.12156
Integrating Environmental, Social, and Governance (ESG) factors into credit risk assessment is the new frontier for credit risk management as regulators and investors increasingly require banks to channel loans to “sustainable” borrowers and ultimately foster sustainable growth. Our findings show that higher ESG awareness is strongly associated with better creditworthiness (proxied by the Altman Z-score). We apply a two-step methodology to 3331 companies from various industries and geographies in the 2000–2016 period which reveals that high ESG awareness scores are strongly and very significantly associated with a reduction in firm credit risk. We check the robustness by using the Probability of Default as a dependent variable and an instrumental variable constructed with a factor analysis. Our results support the appropriateness of the introduction of ESG awareness parameters in the creditworthiness assessment of borrowers.

---

Does ESG Report Tone Influence ESG Rating Divergence? Evidence From China
10.1111/jifm.12245
ABSTRACT Tone, as a key linguistic feature of corporate narrative disclosures, plays a crucial role in shaping information users' evaluation of firm performance. However, whether and how the tone in ESG (Environmental, Social, and Governance) reports influences rating agencies' assessments of firms' ESG performance remains underexplored. Using data from Chinese listed firms between 2018 and 2022, we explore the impact of ESG report tone on ESG rating divergence. Our findings indicate that positive expressions in ESG reports worsen the divergence in ESG ratings, particularly in the environmental ratings. Conversely, negative expressions in ESG reports enhance consistency in rating outcomes. Heterogeneity tests reveal that greater positive expressions in ESG reports exacerbate rating divergence only when firms' ESG reports exhibit low credibility, as indicated by mandatory disclosure, low readability, and high media pressure. In contrast, the impact of negative statements in ESG reports on consistency in rating outcomes is unaffected by the credibility of ESG reports. Our study contributes to the existing literature on the relationship between ESG report textual quality and ESG rating divergence in emerging markets.

---

Floodlight or Spotlight? Public Attention and the Selective Disclosure of Environmental Information
10.1111/joms.12920
To meet growing demands for information on their environmental impacts, firms may engage in selective disclosure by strategically reporting only a subset of relevant data. In this article, we draw out and problematize an antecedent to selective disclosure, public attention. Prior studies suggest that public attention brings scrutiny that reduces selective disclosure by increasing the risk of getting caught (the floodlight thesis). The impression management literature, however, suggests that public attention offers the possibility of broad-based image benefits from the disclosure of strategically filtered data (the spotlight thesis). Panel regressions with Trucost data from 2008–19 provide overall support for the spotlight thesis as well as a negative moderator, environmental damage. Results also point to an underlying mechanism: Companies receiving public attention disclose a larger number of environmental metrics, but not ones that, altogether, represent more environmental damage, a tactic that we call strategic fluffing.

---

The dispersal of Austronesian languages in Island South East Asia: Current findings and debates
10.1111/lnc3.12325
Lang Linguist Compass. 2019;13:e12325. https://doi.org/10.1111/lnc3.12325 Abstract This paper reviews the “standard” view of the Austronesian language family tree in connection with the archeological “farming/language dispersal” hypothesis of Neolithic populations moving into Island South East Asia (ISEA) and beyond. It focuses on what is currently known about the dispersal history of the ~650 languages spoken in ISEA (Malaysia, the Philippines, Indonesia, and Timor‐Leste) that belong to the Malayo‐Polynesian branch of Austronesian and points out where the topology of the MP branch is agreed upon and where it is contested. The conclusion is that historical linguistics is currently not in the position to provide information about higher order temporal and spatial relations between speaker groups within ISEA, unlike that which the language/farming dispersal hypothesis suggests. It also reviews some claims that can be heard in support of this hypothesis and concludes (i) that the expansion of MP languages into ISEA was less monolithic than often suggested, but rather that their lexical and structural diversity suggests multiple migrations of different groups, in many different directions, at different points in time; (ii) that the history of MP languages very likely involved long‐term, intense contact in multilingual communities where

---

A Sequential Two-Step Algorithm for Fast Generation of Vehicle Racing Trajectories.
10.1115/1.4033311
The problem of maneuvering a vehicle through a race course in minimum time requires computation of both longitudinal (brake and throttle) and lateral (steering wheel) control inputs. Unfortunately, solving the resulting nonlinear optimal control problem is typically computationally expensive and infeasible for real-time trajectory planning. This paper presents an iterative algorithm that divides the path generation task into two sequential subproblems that are significantly easier to solve. Given an initial path through the race track, the algorithm runs a forward-backward integration scheme to determine the minimum-time longitudinal speed profile, subject to tire friction constraints. With this fixed speed profile, the algorithm updates the vehicle's path by solving a convex optimization problem that minimizes the resulting path curvature while staying within track boundaries and obeying affine, time-varying vehicle dynamics constraints. This two-step process is repeated iteratively until the predicted lap time no longer improves. While providing no guarantees of convergence or a globally optimal solution, the approach performs very well when validated on the Thunderhill Raceway course in Willows, CA. The predicted lap time converges after four to five iterations, with each iteration over the full 4.5 km race course requiring only thirty seconds of computation time on a laptop computer. The resulting trajectory is experimentally driven at the race circuit with an autonomous Audi TTS test vehicle, and the resulting lap time and racing line is comparable to both a nonlinear gradient descent solution and a trajectory recorded from a professional racecar driver. The experimental results indicate that the proposed method is a viable option for online trajectory planning in the near future.

---

Fine-tuning llama-2 and few-shot learning for ABSA
10.1117/12.3030995
The Llama-2 model has demonstrated excellent performance in various research fields and can be adapted to specific tasks through fine-tuning techniques. This paper focuses on exploring the application of Llama-2 in aspect-based sentiment analysis (ABSA), specifically focusing on the joint tasks of aspect term extraction and polarity classification. We propose a few-shot ABSA method based on fine-tuning Llama-2. We discuss the impact of simple and complex training data instructions on model performance and find that their influence is minimal. Additionally, we investigate the performance of the fine-tuned model when using different numbers of context prompts during inference. We find that the fine-tuned Llama-2, combined with few-shot context prompts, performs well and can consistently output JSON format, achieving a maximum F1 score of 69.5%, which is a 3.8% improvement compared to GPT-3.5. Results analysis indicates that finetuning helps reduce false positives, improve model sensitivity, and specificity.

---

Construction and optimization of enterprise ESG performance evaluation model based on support vector machine
10.1117/12.3058835
As computational techniques advance, machine learning models have become essential for analyzing complex datasets, including those used in Environmental, Social, and Governance (ESG) performance evaluation. Traditional ESG evaluation methods face challenges in managing multidimensional data and delivering reliable predictions, especially when addressing the nuances of sustainability and social responsibility metrics. This paper proposes an enterprise ESG performance evaluation model based on Support Vector Machine (SVM), optimized for large-scale, diverse ESG datasets. By leveraging machine learning, specifically SVM, the model addresses limitations in conventional ESG assessment, offering enhanced accuracy, interpretability, and scalability. The study integrates optimized parameter selection with a novel feature fusion approach, providing enterprises with an effective tool for assessing and improving their ESG performance. Comparative analysis demonstrates the model’s superiority over traditional methods, highlighting its adaptability to dynamic ESG metrics and its contributions to sustainable enterprise decision-making.

---

Using artificial intelligence to assess personal qualities in college admissions
10.1126/sciadv.adg9405
Personal qualities like prosocial purpose and leadership predict important life outcomes, including college success. Unfortunately, the holistic assessment of personal qualities in college admissions is opaque and resource intensive. Can artificial intelligence (AI) advance the goals of holistic admissions? While cost-effective, AI has been criticized as a “black box” that may inadvertently penalize already disadvantaged subgroups when used in high-stakes settings. Here, we consider an AI approach to assessing personal qualities that aims to overcome these limitations. Research assistants and admissions officers first identified the presence/absence of seven personal qualities in n = 3131 applicant essays describing extracurricular and work experiences. Next, we fine-tuned pretrained language models with these ratings, which successfully reproduced human codes across demographic subgroups. Last, in a national sample (N = 309,594), computer-generated scores collectively demonstrated incremental validity for predicting 6-year college graduation. We discuss challenges and opportunities of AI for assessing personal qualities.

---

Exploiting Coherence for the Simultaneous Discovery of Latent Facets and associated Sentiments
10.1137/1.9781611972818.43
Facet-based sentiment analysis involves discovering the latent facets, sentiments and their associations. Traditional facet-based sentiment analysis algorithms typically perform the various tasks in sequence, and fail to take advantage of the mutual reinforcement of the tasks. Additionally,inferring sentiment levels typically requires domain knowledge or human intervention. In this paper, we propose aseries of probabilistic models that jointly discover latent facets and sentiment topics, and also order the sentiment topics with respect to a multi-point scale, in a language and domain independent manner. This is achieved by simultaneously capturing both short-range syntactic structure and long range semantic dependencies between the sentiment and facet words. The models further incorporate coherence in reviews, where reviewers dwell on one facet or sentiment level before moving on, for more accurate facet and sentiment discovery. For reviews which are supplemented with ratings, our models automatically order the latent sentiment topics, without requiring seed-words or domain-knowledge. To the best of our knowledge, our work is the first attempt to combine the notions of syntactic and semantic dependencies in the domain of review mining. Further, the concept of facet and sentiment coherence has not been explored earlier either. Extensive experimental results on real world review data show that the proposed models outperform various state of the art baselines for facet-based sentiment analysis.

---

Glitter or gold? Deriving structured insights from sustainability reports via large language models
10.1140/epjds/s13688-024-00481-2
Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors’ increasing attention to Environmental, Social, and Governance (ESG) issues. Publicly released information on sustainability practices is often disclosed in diverse, unstructured, and multi-modal documentation. This poses a challenge in efficiently gathering and aligning the data into a unified framework to derive insights related to Corporate Social Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes an intuitive choice for delivering insightful and actionable data to stakeholders. In this study, we employ Large Language Models (LLMs), In-Context Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract structured insights related to ESG aspects from companies’ sustainability reports. We then leverage graph-based representations to conduct statistical analyses concerning the extracted insights. These analyses revealed that ESG criteria cover a wide range of topics, exceeding 500, often beyond those considered in existing categorizations, and are addressed by companies through a variety of initiatives. Moreover, disclosure similarities emerged among companies from the same region or sector, validating ongoing hypotheses in the ESG literature. Lastly, by incorporating additional company attributes into our analyses, we investigated which factors impact the most on companies’ ESG ratings, showing that ESG disclosure affects the obtained ratings more than other financial or company data.

---

A framework for ontology-driven subspace clustering
10.1145/1014052.1014130
Traditional clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes. While domain knowledge is always the best way to justify clustering, few clustering algorithms have ever take domain knowledge into consideration. In this paper, the domain knowledge is represented by hierarchical ontology. We develop a framework by directly incorporating domain knowledge into clustering process, yielding a set of clusters with strong ontology implication. During the clustering process, ontology information is utilized to efficiently prune the exponential search space of the subspace clustering algorithms. Meanwhile, the algorithm generates automatical interpretation of the clustering result by mapping the natural hierarchical organized subspace clusters with significant categorical enrichment onto the ontology hierarchy. Our experiments on a set of gene expression data using gene ontology demonstrate that our pruning technique driven by ontology significantly improve the clustering performance with minimal degradation of the cluster quality. Meanwhile, many hierarchical organizations of gene clusters corresponding to a sub-hierarchies in gene ontology were also successfully captured.

---

Learning to extract information from semi-structured text using a discriminative context free grammar
10.1145/1076034.1076091
In recent work, conditional Markov chain models (CMM) have been used to extract information from semi-structured text (one example is the Conditional Random Field [10]). Applications range from finding the author and title in research papers to finding the phone number and street address in a web page. The CMM framework combines a priori knowledge encoded as features with a set of labeled training data to learn an efficient extraction process. We will show that similar problems can be solved more effectively by learning a discriminative context free grammar from training data. The grammar has several distinct advantages: long range, even global, constraints can be used to disambiguate entity labels; training data is used more efficiently; and a set of new more powerful features can be introduced. The grammar based approach also results in semantic information (encoded in the form of a parse tree) which could be used for IR applications like question answering. The specific problem we consider is of extracting personal contact, or address, information from unstructured sources such as documents and emails. While linear-chain CMMs perform reasonably well on this task, we show that a statistical parsing approach results in a 50% reduction in error rate. This system also has the advantage of being interactive, similar to the system described in [9]. In cases where there are multiple errors, a single user correction can be propagated to correct multiple errors automatically. Using a discriminatively trained grammar, 93.71% of all tokens are labeled correctly (compared to 88.43% for a CMM) and 72.87% of records have all tokens labeled correctly (compared to 45.29% for the CMM).

---

Information Extraction: Distilling structured data from unstructured text
10.1145/1105664.1105679
In 2001 the U.S. Department of Labor was tasked with building a Web site that would help people find continuing education opportunities at community colleges, universities, and organizations across the country. The department wanted its Web site to support fielded Boolean searches over locations, dates, times, prerequisites, instructors, topic areas, and course descriptions. Ultimately it was also interested in mining its new database for patterns and educational trends. This was a major data-integration project, aiming to automatically gather detailed, structured information from tens of thousands of individual institutions every three months.

---

Clustering support for automated tracing
10.1145/1321631.1321668
Automated trace tools dynamically generate links between various software artifacts such as requirements, design elements, code, test cases, and other less structured supplemental documents. Trace algorithms typically utilize information retrieval methods to compute similarity scores between pairs of artifacts. Results are returned to the user as a ranked set of candidate links, and the user is then required to evaluate the results through performing a top-down search through the list. Although clustering methods have previously been shown to improve the performance of information retrieval algorithms by increasing understandability of the results and minimizing human analysis effort, their usefulness in automated traceability tools has not yet been explored. This paper evaluates and compares the effectiveness of several existing clustering methods to support traceability; describes a technique for incorporating them into the automated traceability process; and proposes new techniques based on the concepts of theme cohesion and coupling to dynamically identify optimal clustering granularity and to detect cross-cutting concerns that would otherwise remain undetected by standard clustering algorithms. The benefits of utilizing clustering in automated trace retrieval are then evaluated through a case study

---

Information extraction challenges in managing unstructured data
10.1145/1519103.1519106
Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community.

---

Enterprise information extraction: recent developments and open challenges
10.1145/1807167.1807339
Information extraction (IE) - the problem of extracting structured information from unstructured text - has become an increasingly important topic in recent years. A SIGMOD 2006 tutorial [3] outlined challenges and opportunities for the database community to advance the state of the art in information extraction, and posed the following grand challenge: "Can we build a System R for information extraction? Our tutorial gives an overview of progress the database community has made towards meeting this challenge. In particular, we start by discussing design requirements in building an enterprise IE system. We then survey recent technological advances towards addressing these requirements, broadly categorized as: (1) Languages for specifying extraction programs in a declarative way, thus allowing database-style performance optimizations; (2) Infrastructure needed to ensure scalability, and (3) Development support for enterprise IE systems. Finally, we outline several open challenges and opportunities for the database community to further advance the state of the art in enterprise IE systems. The tutorial is intended for students and researchers interested in information extraction and its applications, and assumes no prior knowledge of the area.

---

Interactive evolution for the procedural generation of tracks in a high-end racing game
10.1145/2001576.2001631
We present a framework for the procedural generation of tracks for a high-end car racing game (TORCS) using interactive evolution. The framework maintains multiple populations and allow users to work both on their own population (in single-user mode) or to collaborate with other users on a shared population. Our architecture comprises a web frontend and an evolutionary backend. The former manages the interaction with users (e.g., logs registered and anonymous users, collects evaluations, provides access to all the evolved populations) and maintains the database server that stores all the present/past populations. The latter runs all the tasks related to evolution (selection, recombination and mutation) and all the tasks related to the target racing game (e.g., the track generation). We performed two sets of experiments involving five human subjects to evolve racing tracks alone (in a single-user mode) or cooperatively. Our preliminary results on five human subjects show that, in all the experiments, there is an increase of users' satisfaction as the evolution proceeds. Users stated that they perceived improvements in the quality of the individuals between subsequent populations and that, at the end, the process produced interesting tracks.

---

Content Attention Model for Aspect Based Sentiment Analysis
10.1145/3178876.3186001
Aspect based sentiment classification is a crucial task for sentiment analysis. Recent advances in neural attention models demonstrate that they can be helpful in aspect based sentiment classification tasks, which can help identify the focus words in human. However, according to our empirical study, prevalent content attention mechanisms proposed for aspect based sentiment classification mostly focus on identifying the sentiment words or shifters, without considering the relevance of such words with respect to the given aspects in the sentence. Therefore, they are usually insufficient for dealing with multi-aspect sentences and the syntactically complex sentence structures. To solve this problem, we propose a novel content attention based aspect based sentiment classification model, with two attention enhancing mechanisms: sentence-level content attention mechanism is capable of capturing the important information about given aspects from a global perspective, whiles the context attention mechanism is responsible for simultaneously taking the order of the words and their correlations into account, by embedding them into a series of customized memories. Experimental results demonstrate that our model outperforms the state-of-the-art, in which the proposed mechanisms play a key role.

---

Teaching Autonomous Systems at 1/10th-scale: Design of the F1/10 Racecar, Simulators and Curriculum
10.1145/3328778.3366796
Teaching autonomous systems is challenging because it is a rapidly advancing cross-disciplinary field that requires theory to be continually validated on physical platforms. For an autonomous vehicle (AV) to operate correctly, it needs to satisfy safety and performance properties that depend on the operational context and interaction with environmental agents, which can be difficult to anticipate and capture. This paper describes a senior undergraduate level course on the design, programming and racing of 1/10th-scale autonomous race cars. We explore AV safety and performance concepts at the limits of perception, planning, and control, in a highly interactive and competitive environment. The course includes an ethics-centered design philosophy, which seeks to engage the students in an analysis of ethical and socio-economic implications of autonomous systems. Our hypothesis is that $1/10th-scale autonomous vehicles sufficiently capture the scaled dynamics, sensing modalities, decision making and risks of real autonomous vehicles, but are a safe and accessible platform to teach the foundations of autonomous systems. We describe the design, deployment and feedback from two offerings of this class for college seniors and graduate students, open-source community development across 36 universities, international racing competitions, student skill enhancement and employability, and recommendations for tailoring it to various settings.

---

Computational Fact Validation from Knowledge Graph using Structured and Unstructured Information
10.1145/3371158.3371187
In today's world, data or information is increasing at an exponential rate, and so is the fake news. Traditional fact-checking methods like fake news detection by experts, analysts, or some organizations do not match with the volume of information available. This is where the problem of computational fact-checking or validation becomes relevant. Given a Knowledge Graph, a knowledge corpus, and a fact (triple statement), the goal of fact-checking is to decide whether the fact or knowledge is correct or not. Existing approaches extensively used several structural features of the input Knowledge Graph to address the mentioned problem. In this work, our primary focus would be to leverage the unstructured information along with the structured ones. Our approach considers finding evidence from Wikipedia and structured information from Wikidata, which helps in determining the validity of the input facts. As features from the structured domain, we have used TransE embedding considering components of the input fact. The similarity of input fact with elements of relevant Wikipedia pages has been used as unstructured features. The experiments with a dataset consisting of nine relations of Wikidata has established the advantage of combining unstructured features with structured features for the given task.

---

Balancing the Tradeoff Between Clustering Value and Interpretability
10.1145/3375627.3375843
Graph clustering groups entities -- the vertices of a graph -- based on their similarity, typically using a complex distance function over a large number of features. Successful integration of clustering approaches in automated decision-support systems hinges on the interpretability of the resulting clusters. This paper addresses the problem of generating interpretable clusters, given features of interest that signify interpretability to an end-user, by optimizing interpretability in addition to common clustering objectives. We propose a β-interpretable clustering algorithm that ensures that at least β fraction of nodes in each cluster share the same feature value. The tunable parameter β is user-specified. We also present a more efficient algorithm for scenarios with β\!=\!1$ and analyze the theoretical guarantees of the two algorithms. Finally, we empirically demonstrate the benefits of our approaches in generating interpretable clusters using four real-world datasets. The interpretability of the clusters is complemented by generating simple explanations denoting the feature values of the nodes in the clusters, using frequent pattern mining.

---

LayoutLM: Pre-training of Text and Layout for Document Image Understanding
10.1145/3394486.3403172
Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.

---

A Survey on Aspect-Based Sentiment Classification
10.1145/3503044
With the constantly growing number of reviews and other sentiment-bearing texts on the Web, the demand for automatic sentiment analysis algorithms continues to expand. Aspect-based sentiment classification (ABSC) allows for the automatic extraction of highly fine-grained sentiment information from text documents or sentences. In this survey, the rapidly evolving state of the research on ABSC is reviewed. A novel taxonomy is proposed that categorizes the ABSC models into three major categories: knowledge-based, machine learning, and hybrid models. This taxonomy is accompanied with summarizing overviews of the reported model performances, and both technical and intuitive explanations of the various ABSC models. State-of-the-art ABSC models are discussed, such as models based on the transformer model, and hybrid deep learning models that incorporate knowledge bases. Additionally, various techniques for representing the model inputs and evaluating the model outputs are reviewed. Furthermore, trends in the research on ABSC are identified and a discussion is provided on the ways in which the field of ABSC can be advanced in the future.

---

Towards Automatic Green Claim Detection
10.1145/3503162.3503163
Companies frequently make claims about positive impacts on the environment, but these claims are not always valid. In this sense, “greenwashing” is used when a company states a false claim about its products and practices being environmentally friendly. It is an important issue and has attracted the attention of policymakers to strengthen consumer law and new companies with the mission of helping brands and consumers to become more eco-friendly. However, manual screening of websites and social networks for sustainable claims (green claims) is time-consuming. Automatic detection of green claims is an underexplored problem from a computer science perspective, and thus, we present the design, training and evaluation of different approaches in this study. Our experiments reveal that although pre-trained models present high performance, they also show sensibility to adversarial attacks, such as character-swap-based methods, which are common in social networks. In order to understand the applicability in a real-world scenario, we also evaluated its generalization performance, which showed a notable performance across different domains.

---

A Simple yet Effective Framework for Few-Shot Aspect-Based Sentiment Analysis
10.1145/3539618.3591940
The pre-training and fine-tuning paradigm has become the main-stream framework in the field of Aspect-Based Sentiment Analysis (ABSA). Although it has achieved sound performance in the domains containing enough fine-grained aspect-sentiment annotations, it is still challenging to conduct few-shot ABSA in domains where manual annotations are scarce. In this work, we argue that two kinds of gaps, i.e., domain gap and objective gap, hinder the transfer of knowledge from pre-training language models (PLMs) to ABSA tasks. To address this issue, we introduce a simple yet effective framework called FS-ABSA, which involves domain-adaptive pre-training and text-infilling fine-tuning. We approach the End-to-End ABSA task as a text-infilling problem and perform domain-adaptive pre-training with the text-infilling objective, narrowing the two gaps and consequently facilitating the knowledge transfer. Experiments show that the resulting model achieves more compelling performance than baselines under the few-shot setting while driving the state-of-the-art performance to a new level across datasets under the fully-supervised setting. Moreover, we apply our framework to two non-English low-resource languages to demonstrate its generality and effectiveness.

---

DynamicESG: A Dataset for Dynamically Unearthing ESG Ratings from News Articles
10.1145/3583780.3615118
This paper introduces the DynamicESG dataset, a unique resource for dynamically extracting ESG ratings from news articles. The ESG rating, a novel metric employed annually to gauge a company's sustainability, relies heavily on corporate disclosure and other external information, especially news narratives. Our dataset, comprising a wide spectrum of news over a twelve-year span, annotates articles in accordance with MSCI ESG ratings methodology and SASB standards, with relevance to ESG issues. DynamicESG provides a comprehensive means of investigating the relationship between public discourse, ESG-related events, and subsequent ESG rating adjustments. We detail our data collection, curation, annotation procedure, and inter-rater agreement, ensuring high data quality and usability. Importantly, our dataset includes a temporal dimension, enabling the analysis of longitudinal trends in ESG ratings and their correlation with news coverage. Moreover, the dataset incorporates an opportunity/risk tendency, thus permitting analysis from diverse perspectives to discern if the news is beneficial or detrimental to the company. We believe this dataset will serve as a valuable resource for researchers in fields such as corporate social responsibility, sustainable investing, machine learning, and natural language processing. Initial analysis using the dataset underscores its potential to facilitate new insights into the dynamics of ESG ratings and the influence of news media on these ratings.

---

Multimodal Sentiment Analysis: A Survey of Methods, Trends, and Challenges
10.1145/3586075
Sentiment analysis has come long way since it was introduced as a natural language processing task nearly 20 years ago. Sentiment analysis aims to extract the underlying attitudes and opinions toward an entity. It has become a powerful tool used by governments, businesses, medicine, marketing, and others. The traditional sentiment analysis model focuses mainly on text content. However, technological advances have allowed people to express their opinions and feelings through audio, image and video channels. As a result, sentiment analysis is shifting from unimodality to multimodality. Multimodal sentiment analysis brings new opportunities with the rapid increase of sentiment analysis as complementary data streams enable improved and deeper sentiment detection which goes beyond text-based analysis. Audio and video channels are included in multimodal sentiment analysis in terms of broadness. People have been working on different approaches to improve sentiment analysis system performance by employing complex deep neural architectures. Recently, sentiment analysis has achieved significant success using the transformer-based model. This paper presents a comprehensive study of different sentiment analysis approaches, applications, challenges, and resources then concludes that it holds tremendous potential. The primary motivation of this survey is to highlight changing trends in the unimodality to multimodality for solving sentiment analysis tasks.

---

A Context-Focused Attention Evolution Model for Aspect-Based Sentiment Classification
10.1145/3587465
Due to their inherent capability in the semantic alignment of aspects and their context words, Attention and Long-Short-Term-Memory (LSTM) mechanisms are widely adopted for Aspect-Based Sentiment Classification (ABSC) tasks. Instead, it is challenging to handle long-range word dependencies on multiple entities due to the deficiency in attention mechanisms. To solve this problem, we propose a Context-Focused Aspect-Based Network to align attention before LSTM, making the model focus more on aspect-related words and ignore irrelevant words, improving the accuracy of final classification. This can either alleviate attention distraction or reinforce the text representation ability. Experiments on two benchmark datasets show that the results achieve respectable performance compared to the state-of-the-art methods available in ABSC. Our approach has the potential to improve classification accuracy by adaptively adjusting the focus on context.

---

RDGCN: Reinforced Dependency Graph Convolutional Network for Aspect-based Sentiment Analysis
10.1145/3616855.3635775
Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the sentiment polarity of aspect terms within sentences. Employing graph neural networks to capture structural patterns from syntactic dependency parsing has been confirmed as an effective approach for boosting ABSA. In most works, the topology of dependency trees or dependency-based attention coefficients is often loosely regarded as edges between aspects and opinions, which can result in insufficient and ambiguous syntactic utilization. To address these problems, we propose a new reinforced dependency graph convolutional network (RDGCN) that improves the importance calculation of dependencies in both distance and type views. Initially, we propose an importance calculation criterion for the minimum distances over dependency trees. Under the criterion, we design a distance-importance function that leverages reinforcement learning for weight distribution search and dissimilarity control. Since dependency types often do not have explicit syntax like tree distances, we use global attention and mask mechanisms to design type-importance functions. Finally, we merge these weights and implement feature aggregation and classification. Comprehensive experiments show the effectiveness of the criterion and importance functions. RDGCN yields excellent analysis results.

---

Improving In-Context Learning via Sequentially Selection and Preference Alignment for Few-Shot Aspect-Based Sentiment Analysis
10.1145/3626772.3657932
In this paper, we leverage in-context learning (ICL) paradigm to handle few-shot aspect-based sentiment analysis (ABSA). Previous works first rank candidate examples by some metrics and then independently retrieve examples similar to test samples. However, their effectiveness may be discounted because of two limitations: in-context example redundancy and example preference misalignment between retriever and LLM. To alleviate them, we propose a novel framework that sequentially retrieves in-context examples. It not only considers which example is useful for the test sample but also prevents its information from being duplicated by already retrieved examples. Subsequently, we exploit the rewards of LLMs on retrieved in-context examples to optimize parameters for bridging preference gaps. Experiments on four ABSA datasets show that our framework is significantly superior to previous works.

---

Two Heads are Better than One: Zero-shot Cognitive Reasoning via Multi-LLM Knowledge Fusion
10.1145/3627673.3679744
"Cognitive reasoning holds a significant place within Natural Language Processing (NLP). Yet, the exploration of zero-shot scenarios, which align more closely with real-life situations than supervised scenarios, has been relatively limited. While a few studies have employed Large Language Models (LLMs) to tackle zero-shot cognitive reasoning tasks, they still grapple with two key challenges: 1) Traditional approaches rely on the chain-of-thought (CoT) mechanism, wherein LLMs are provided with a ""Let's think step by step'' prompt. However

---

Combat Greenwashing with GoalSpotter: Automatic Sustainability Objective Detection in Heterogeneous Reports
10.1145/3627673.3680110
Sustainable development is nowadays a prominent factor for the public. As a result, companies publish their sustainability visions and strategies in various reports to show their commitment to saving the environment and promoting social progress. However, not all statements in these sustainability reports are fact-based. When a company tries to mislead the public with its non-fact-based sustainability claims, greenwashing happens. To combat greenwashing, society needs effective automated approaches to identify the sustainability claims of companies in their heterogeneous reports. In this paper, we present a new sustainability objective detection system, named GoalSpotter, that automatically identifies the environmental and social claims of companies in their heterogeneous reports. Our system extracts text blocks of diverse reports, preprocesses and labels them using domain expert annotations, and then fine-tunes transformer models on the labeled text blocks. This way, our system can detect sustainability objectives in any new heterogeneous report. As our experiments show, our system out-performs existing state-of-the-art sustainability objective detection approaches. Furthermore, our post-deployment results show the significant impacts of our system in real-world business.

---

Assessing LLMs for High Stakes Applications
10.1145/3639477.3639720
Large Language Models (LLMs) promise strategic benefit for numerous application domains. The current state-of-the-art in LLMs, however, lacks the trust, security, and reliability which prohibits their use in high stakes applications. To address this, our work investigated the challenges of developing, deploying, and assessing LLMs within a specific high stakes application, intelligence reporting workflows. We identified the following challenges that need to be addressed before LLMs can be used in high stakes applications: (1) challenges with unverified data and data leakage, (2) challenges with fine tuning and inference at scale, and (3) challenges in re-producibility and assessment of LLMs. We argue that researchers should prioritize test and assessment metrics, as better metrics will lead to insight to further improve these LLMs.

---

Generating Proactive Suggestions based on the Context: User Evaluation of Large Language Model Outputs for In-Vehicle Voice Assistants
10.1145/3640794.3665568
Large Language Models (LLMs) have recently been explored for a variety of tasks, most prominently for dialogue-based interactions with users. The future in-car voice assistant (VA) is envisioned as a proactive companion making suggestions to the user during the ride. We investigate the use of selected LLMs to generate proactive suggestions for a VA given different context situations by using a basic prompt design. An online study with users was conducted to evaluate the generated suggestions. We demonstrate the feasibility of generating context-based proactive suggestions with different off-the-shelf LLMs. Results of the user survey show that suggestions generated by the LLMs GPT4.0 and Bison received an overall positive evaluation regarding the user experience for response quality and response behavior over different context situations. This work can serve as a starting point to implement proactive interaction for VA with LLMs based on the recognized context situation in the car.

---

Predicting ESG Ratings by Machine Learning and Analyzing Influencing Factors by XAI
10.1145/3647722.3647742
This study examines the role of Environmental, Social, and Governance (ESG) management in corporate strategy, particularly focusing on predicting ESG ratings with machine learning. Given the diverse ESG evaluation criteria employed by global rating agencies, there's a need for clear guidelines to facilitate effective ESG management. The research aims to develop an ESG rating prediction model utilizing a triennial compendium of Korean corporate financial data. This process involves a comparative analysis of linear models, tree-based models, and neural network-based models. Additionally, this study explains the importance of various variables by applying SHAP, one of the XAI techniques. The results indicate that XGB is the most effective, achieving an 85.1% F1 score in ESG rating predictions. By understanding how financial factors impact ESG ratings, companies can develop more effective ESG strategies, forming an essential foundation for sustainable growth.

---

Detection of greenwashing in ESG reports of Chinese listed companies based on Word2vec and TF-IDF
10.1145/3655497.3655513
With the growing emphasis on ESG (Environmental, Social, and Governance) issues, the mandatory disclosure of ESG reports is on the horizon. However, due to the lack of a regulatory framework and a unified international ESG evaluation, the phenomenon of greenwashing in corporate ESG reporting is prevalent. We collected social responsibility reports and actual ESG performance data from A-share companies from 2011 to 2021 and innovatively employed text mining techniques to quantitatively investigate the extent of greenwashing in ESG reports. Our study initially utilized the Word2Vec method, combined with Skip-gram and Continuous Bag of Words models to train word vectors, and built an ESG lexicon using seed words. ESG reports is subsequently segmented based on a defined sentence splitting function and TF-IDF algorithm is employed to extract keywords. By matching the keywords with the ESG lexicon, we precisely extracted the annual ESG discourse for each company and conducted sentiment analysis to derive a greenwashing score. Heterogeneity analysis reveals that firm ownership has no significant impact on the level of greenwashing, yet the industry and region in which the enterprise operates considerably influence the greenwashing level. This study holds implications for enhancing the quality of ESG reporting and optimizing investment decisions.

---

A Review on the Impact of Data Representation on Model Explainability
10.1145/3662178
In recent years, advanced machine learning and artificial intelligence techniques have gained popularity due to their ability to solve problems across various domains with high performance and quality. However, these techniques are often so complex that they fail to provide simple and understandable explanations for the outputs they generate. To address this issue, the field of explainable artificial intelligence has recently emerged. However, most data generated in different domains are inherently structural; that is, they consist of parts and relationships among them. Such data can be represented using either a simple data-structure or form, such as a vector, or a complex data-structure, such as a graph. The effect of this representation form on the explainability and interpretability of machine learning models is not extensively discussed in the literature. In this survey article, we review efficient algorithms proposed for learning from inherently structured data, emphasizing how their representation form affects the explainability of learning models. A conclusion of our literature review is that using complex forms or data-structures for data representation improves not only the learning performance but also the explainability and transparency of the model.

---

Learning Dynamic Multimodal Network Slot Concepts from the Web for Forecasting Environmental, Social and Governance Ratings
10.1145/3663674
Dynamic multimodal networks are networks with node attributes from different modalities where the attributes and network relationships evolve across time, i.e., both networks and multimodal attributes are dynamic; for example, dynamic relationship networks between companies that evolve across time due to changes in business strategies and alliances, which are associated with dynamic company attributes from multiple modalities such as textual online news, categorical events, and numerical financial-related data. Such information can be useful in predictive tasks involving companies. Environmental, social, and governance (ESG) ratings of companies are important for assessing the sustainability risks of companies. The process of generating ESG ratings by expert analysts is, however, laborious and time-intensive. We thus explore the use of dynamic multimodal networks extracted from the web for forecasting ESG ratings. Learning such dynamic multimodal networks from the web for forecasting ESG ratings is, however, challenging due to its heterogeneity and the low signal-to-noise ratios and non-stationary distributions of web information. Human analysts cope with such issues by learning concepts from past experience through relational thinking and scanning for such concepts when analyzing new information about a company. In this article, we propose the Dynamic Multimodal Slot Concept Attention-based Network (DynScan) model. DynScan utilizes slot attention mechanisms together with slot concept alignment and disentanglement loss functions to learn latent slot concepts from dynamic multimodal networks to improve performance on ESG rating forecasting tasks. DynScan is evaluated on forecasting tasks on six datasets, comprising three ESG ratings across two sets of companies. Our experiments show that DynScan outperforms other state-of-the-art models on these forecasting tasks. We also visualize the slot concepts learned by DynScan on five synthetic datasets and three real-world datasets and observe distinct and meaningful slot concepts being learned by DynScan across both synthetic and real-world datasets.

---

Aspect-Based Multimodal Mining: Unveiling Sentiments, Complaints, and Beyond in User-Generated Content
10.1145/3664647.3681703
Sentiment analysis and complaint identification are key tools in mining user preferences by measuring the polarity and breach of expectations. Recent works on complaint identification identify aspect categories and classify them into complaint or not-complaint classes. However, aspect category-based complaint identification provides high-level information about the features of products. In addition, it is also observed that the user sometimes does not complain about a specific aspect but expresses concern about specific aspects in a respectful way. Currently, uni-modal and multimodal studies do not differentiate between this thin line between complaint and concern. In this work, we propose the task of multimodal aspect term-based analysis beyond sentiments and complaints. It comprises of two sub-tasks, viz (i) classification of the given aspect term into one of the four classes, viz. praise, concern, complaint, and others, (ii) identification of the cause of praise, concern, and complaint classes. We propose a first benchmark explainable multi-modal corpus annotated for aspect term-based complaints, praises, concerns, their corresponding causes, and sentiments. Further, we propose an effective technique for the joint learning of aspect term-based complaint/concern/praise identification and cause extraction tasks (primary tasks) where sentiment analysis is used as a secondary task to assist primary tasks and establish them as baselines for further research in this direction. 1

---

DELM: Deep Ensemble Learning Model for Anomaly Detection in Malicious Network Traffic-based Adaptive Feature Aggregation and Network Optimization
10.1145/3690637
With the rapid advancements in internet technology, the complexity and sophistication of network traffic attacks are increasing, making it challenging for traditional anomaly detection systems to analyze and detect malicious network attacks. The increasing advancedness of cyber threats calls for innovative approaches to identify malicious patterns within network traffic precisely. The primary issue lies in the fact that these approaches do not focus on the essential adaptive features of network traffic. We proposed an effective anomaly detection system for malicious network traffic attacks called the Deep Ensemble Learning Model (DELM). We leverage the structure of the Feedforward Deep Neural Network (FDNN), and Deep Belief Network (DBN), incorporating multiple hidden layers with non-linear activation functions. Integrating Adaptive Feature Aggregation (AFA) with the FDNN algorithm dynamically adjusts the feature aggregation process based on incoming traffic characteristics to improve adaptability. The Conditional Generative Network was employed to enhance DELM for generating data for minority classes. To improve the model’s accuracy, we applied batch normalization and data augmentation techniques for preprocessing, utilized n-gram, one-hot encoding, and feature aggregation methods for effective feature extraction. This study significantly contributes to network security by enhancing systems for detecting malicious network traffic. With its interpretability and adaptability, our proposed model shows promise in addressing the evolving cyber threat and fortifying critical network infrastructure. The experimental results demonstrate that our model performs with higher stability than the existing state-of-the-art detection approaches, as reflected by its higher accuracy, precision, recall, f1 score, and AUC-ROC.

---

Predicting Company ESG Ratings from News Articles Using Multivariate Timeseries Analysis
10.1145/3701716.3717509
In recent years, corporate environmental, social, and governance (ESG) engagement has received significant public attention. As mandatory ESG reporting is increasingly adopted and investors place greater emphasis on sustainability in their decisions, the demand for transparent and reliable ESG ratings is growing. However, existing automatic approaches to ESG rating prediction remain limited. Many rely on traditional machine learning methods like random forests or social network analysis, rather than leveraging incoming news article streams and large multivariate time series data, which, for the first time, enables capturing the dynamic relationships between topics, sentiments, and events. In this paper, we propose a novel approach to predicting ESG ratings from news articles by uniquely combining multivariate time series construction with advanced deep learning techniques. We create an extensive dataset of 3.7 million news articles spanning three years and covering 3,000 U.S. companies, providing a robust foundation for training and evaluating our approach. Our approach achieves high accuracy and outperforms existing approaches, underscoring its potential as a scalable, data-driven solution for ESG rating prediction.

---

Beyond Cohesion and Coupling: Integrating Control Flow in Software Modularization Process for Better Code Comprehensibility
10.1145/3707452
As software systems evolve to meet the changing needs of users, understanding the source code becomes a critical step in the process. Clustering techniques, also known as modularization techniques, offer a solution to breaking down complex source code into smaller, more manageable parts. This facilitates improved analysis and understanding of the software’s structure. However, the effectiveness of clustering algorithms in code understanding heavily relies on the chosen criteria. While existing methods typically consider cohesion, coupling, and balance between clusters, we argue that these criteria alone may not fully satisfy one of the primary objectives of clustering, which is to enhance understanding. This is because spaghetti-like structures can be created even when these criteria are satisfied. To address this issue, we introduce two new criteria incorporating program control flow to regulate cluster dependencies. By controlling the uniformity of input and output directions, as well as the distribution of inputs and outputs, clustering algorithms can generate clusters that are more developer-friendly and easier to comprehend. We provide intuitive explanations and real-world projects to demonstrate the effectiveness of our approach, and also incorporate feedback from academics and expert programmers. This paper reveals that integrating these new criteria into existing clustering algorithms enables developers to gain deeper insights into the structure of software systems. This, in turn, leads to better design decisions and improved developer understanding of the source code.

---

From Overall Sentiment to Aspect-Level Insights: A Pretraining Strategy for Unsupervised Aspect-based Sentiment Analysis
10.1145/3708319.3733693
Aspect-Based Sentiment Analysis (ABSA) aims to identify sentiments associated with specific aspects within a text. It plays a crucial role in applications such as product reviews and customer feedback analysis, where understanding nuanced opinions is essential. However, progress in ABSA remains constrained by the need for fine-grained labeled data, limiting the applicability of supervised models in real-world scenarios. In this study, we propose an unsupervised transformer-based approach that leverages sentence-level sentiment annotations to induce aspect-level sentiment representations. By supervising attention distributions during pretraining, our model learns to aggregate token-level sentiment cues into context-aware aspect sentiment predictions aligned with sentence-level supervision. We further introduce an attention-based correction mechanism to refine aspect sentiment classification by accounting for the local context of each aspect term. Evaluated on benchmark datasets including Restaurants, Laptops, and Twitter domains, our method outperforms unsupervised baselines on aspect category classification while remaining comparable with strong supervised baselines on aspect term sentiment tasks. These results demonstrate that attention-guided pretraining enables robust, domain-adaptive ABSA without requiring aspect-level supervision.

---

A Neuro-Symbolic Approach to Symbol Grounding for ALC-Ontologies
10.1145/3711896.3736926
Neuro-symbolic computing aims to integrate neural learning with symbolic reasoning to address the fundamental challenge of symbol grounding. While neural networks excel at pattern recognition, they struggle to maintain logical consistency. Conversely, symbolic systems provide formal reasoning capabilities but lack mechanisms for handling perceptual uncertainty. This paper introduces EmALC , a novel neuro-symbolic framework that bridges neural perception with symbolic logic through differentiable fuzzy semantics. Our approach addresses a key limitation of existing methods: while previous neuro-symbolic approaches like Logic Tensor Networks employ first-order fuzzy logic, where key reasoning problems are undecidable, EmALC ensures decidable reasoning by leveraging a fuzzy variant of ALC -- a decidable fragment of first-order logic. Unlike previous approaches that often compromise logical soundness for learning capability, EmALC maintains provable semantic consistency through a hierarchical loss function while mitigating reasoning shortcuts via rule-based revision strategies. Experimental evaluation demonstrates EmALC's effectiveness: on ontology revision tasks, it achieves 100% success rate in correcting masked groundings while preserving semantic integrity; on semantic image interpretation tasks, it improves object classification F1-scores by up to 5.56% through ontology-guided knowledge revision.

---

Neural Network-Based Sentiment Analysis and Emotional Data Mining of Movie Reviews
10.1145/3714334.3714352
With the increasing emphasis on spiritual and cultural enrichment, the film industry has seen remarkable growth, leading to a heightened demand for high-quality cinematic productions. Movie reviews, as a vital channel for capturing audience perspectives, provide valuable insights into films' strengths and areas for improvement through detailed ratings and commentary. This thesis focuses on extracting reviews from the Internet Movie Database (IMDb), a prominent English-language film review platform, and organizing them systematically within a database. After comprehensive data cleaning, advanced analytical techniques, including word frequency analysis and neural network-based sentiment classification, will be employed to interpret the reviews. The results, such as word clouds, sentiment distributions, and emotional trends, will be visualized via an interactive front-end interface. This approach aims to offer a nuanced understanding of audience sentiment and language patterns, contributing to the development of intelligent tools for movie evaluation and cultural analysis.

---

A Comprehensive Overview of Large Language Models
10.1145/3744746
Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive, informative summaries of the existing works to advance the LLM research.

---

MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks
10.1145/3746027.3758225
Environmental, Social, and Governance (ESG) reports are essential for assessing sustainability, regulatory compliance, and financial transparency. However, these documents are typically long, multimodal, and structurally complex, combining dense text, tables, figures, and layout-sensitive semantics. Existing AI systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated benchmark currently exists in ESG domain. To fill the gap, we introduce MMESGBench, a first-of-its-kind benchmark dataset targeted to evaluate multimodal understanding and reasoning across multi-source ESG documents. This dataset is constructed via a human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates candidate question-answer (QA) pairs by jointly interpreting textual, tabular, and visual information from layout-aware document pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each QA pair. This automated process is followed by an expert-in-the-loop validation, where domain specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct document types and three major ESG source categories. Questions are categorized as single-page, cross-page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments validate that multimodal and retrieval-augmented models substantially outperform text-only baselines. MMESGBench is publicly available as an open-source dataset at https://github.com/Zhanglei1103/MMESGBench.

---

Green by Design: Detecting Environmental Claims in Corporate Web Content
10.1145/3746252.3760931
Corporate entities increasingly embed environmental claims in their digital communication to project sustainability awareness. Detecting such claims is critical for regulatory monitoring, corporate accountability, and mitigation of greenwashing practices. Traditional neural network architectures including large language models however, struggle to capture both the complex linguistic structures and the subtle stylistic cues that characterize environmental assertions. In this work, we propose a novel Graph-Augmented Liquid Neural Network (GLNN) architecture for automatic detection of environmental claims in corporate web content. Our approach first models the syntactic and semantic dependencies of text using a Graph Convolutional Network (GCN), while concurrently encoding stylistic features derived from linguistic markers (e.g., LIWC categories) into vector representations. These representations are concatenated and passed into a Liquid Time-Constant (LTC) Network, which provides dynamic adaptability and low-power efficiency by leveraging continuous-time recurrent dynamics. The integration of GCN-based stylistic encoding with LTC networks enables the model to robustly capture both structural dependencies and temporal signal variations inherent in corporate claims, while remaining energy efficient. Extensive experiments on multiple open datasets demonstrate that our model outperforms baseline neural architectures in both accuracy and computational efficiency, highlighting the potential of graph-augmented liquid networks as a foundation for sustainable AI in sustainability monitoring.

---

LLM-Powered Question Answering for Environmental, Social, and Governance (ESG) Reports
10.1145/3746274.3760395
Sustainability reporting has become a crucial tool for companies to demonstrate commitment and performance on Environmental, Social, and Governance (ESG) matters. However, implementing ESG reporting presents many challenges, including data standardization as well as conforming to various reporting frameworks and standards such as Global Reporting Initiative, Sustainability Accounting Standards Board, Task Force on Climate-related Financial Disclosures, and European Sustainability Reporting Board. These challenges are more prevalent, especially in small-to-medium enterprises. To address these issues, this paper proposes a question answering framework leveraging Retrieval-Augmented Generation combined with large language models to support ESG management in business environments. The framework aims to ease the required resources and efforts for ESG analysis, improve the accuracy and relevance of retrieved information, and aid organizations in analyzing their sustainability reports. Additionally, we explore different methods for handling heterogeneous data types in ESG reports, including text and tabular data retrieval strategies. Our implemented system, tested on data obtained from leading companies in Australia, showcases promising results in accommodating a wide variety of use cases in the ESG domain. This demonstrates the potential of the proposed framework as well as other Artificial Intelligence-driven tools in assisting organizations in effectively managing ESG reporting and improving their sustainability performance.

---

Weakly Supervised Open-Domain Aspect-Based Sentiment Analysis
10.1145/3747849
Aspect-Based Sentiment Analysis (ABSA) comprises several subtasks: aspect term extraction (ATE), opinion term extraction (OTE), aspect term sentiment extraction (ATSE), aspect-opinion pair extraction (AOPE), and aspect sentiment triplet extraction (ASTE). Existing unified frameworks for ABSA rely heavily on large-scale annotated data, limiting scalability across domains. We propose UAOS, a double-layer unified span extraction framework that performs all five ABSA subtasks under weak supervision. Our approach first extracts aspect-opinion pairs using universal dependency-based rules from unannotated corpora. Sentiment labels for these pairs are generated via a novel zero-shot, domain-agnostic prompt-based method. The resulting weak labels train a unified span extraction architecture equipped with canonical correlation analysis for early stopping and a self-training mechanism to mitigate noise and bias in supervision. Extensive experiments on four ABSA benchmarks demonstrate that UAOS achieves competitive or superior performance compared to fully supervised baselines. It improves upon the state-of-the-art ODAO by +1.54 F1 for ATE, +0.56 for OTE, and +0.82 for AOPE. In ATSE and ASTE, where no weakly supervised baselines exist, UAOS outperforms several supervised models, setting new benchmarks. To assess domain generalizability, we evaluate UAOS on a psychology/education-domain dataset of student reflections spanning four instructional conditions. Without in-domain fine-tuning, it achieves macro F1 scores of 71.05 (ATE), 74.39 (OTE), 68.24 (AOPE), and 60.56 (ASTE). These results highlight the model’s ability to generalize to out-of-distribution, non-commercial text, underscoring its scalability for low-resource ABSA applications.

---

Temporal-Aware and Aspect Sentiment Driven Recommender Model for Rating Prediction
10.1145/3748825.3748901
With the rapid growth of information and items, capturing users' preferences for various items through rating prediction is crucial for achieving personalized recommendations. However, existing methods have ignored the accurate modeling of the temporal drift of user preferences and the capture of fine-grained sentiment trends in user review texts, which limits the recommendation effect. This paper proposes Temporal-Aware and Aspect Sentiment Driven Recommender Model for Rating Prediction (TASD-RM) to address these limitations. Specifically, we firstly introduce time-decayed weights to dynamically adjust the influence of historical interactions, thereby achieving real-time adaptation to changing user preferences. Then, a Large Language Model (LLM) is adopted to perform aspect-based sentiment analysis on user reviews to extract fine-grained sentiment signals in 18 predefined dimensions. Finally, a multi-granular attention mechanism explicitly models the interactions between users and items at both the global and aspect-specific levels. Experimental results on real dataset show that our method significantly improves the recommendation accuracy, outperforming the baseline model in terms of MAE (0.471) and RMSE (0.713). This work contributes to the advancement of adaptive recommender systems in dynamic digital environments such as e-commerce and social networks.

---

Demystifying TCFD Disclosures: An AI-Powered Framework for Enhanced Transparency and Trust
10.1145/3768292.3770400
Effective assessment of corporate climate disclosures is crucial for the global economic low-carbon transition but currently hampered by inconsistent ratings and the black-box nature of emerging AI solutions. This paper introduces a novel framework that harnesses large language models (LLMs) for transparent assessment construction rather than direct, opaque evaluation. Our methodology employs a Retrieval Augmented Generation (RAG) architecture to automatically extract granular assessment factors from corporate reports. These factors are then operationalized within an interpretable machine learning model, creating an "open box" system. Applying this framework to a global dataset of climate reports between 2020-2024, we generate macro-level insights into the evolution of disclosure practices, revealing key performance trajectories and systemic challenges. This work delivers a generalizable methodology for evaluating climate disclosure, providing actionable comparative intelligence for regulators, investors, and reporting companies.

---

Hybrid Models for Emotion Classification and Sentiment Analysis in Indonesian Language
10.1155/2024/2826773
The swift growth of social networks has enabled easy access to a wealth of user‐created information for public assessment. These data hold potential for various uses, including analyzing comments and reviews through text analysis. The study utilizes a specialized version of the Bidirectional Encoder Representations from Transformers (BERT) model known as IndoBERT, tailored explicitly for Bahasa Indonesia. It aims to improve accuracy in the Indonesian natural language understanding benchmark by boosting performance in sentiment analysis and emotion classification tasks. The testing for both tasks involved a hybrid methodology that merged the summations of the four last hidden layers from the IndoBERT model with a combination of bidirectional long short‐term memory (BiLSTM), bidirectional gated recurrent unit (BiGRU), and an attention model. The resulting model’s performance was assessed using the F1‐score metric. Based on the experimental results, the proposed model achieves an accuracy of 93% for sentiment analysis and 78% for emotion classification on the Indonesian natural language understanding (IndoNLU) benchmark dataset. The implementation result shows that the optimal accuracy of the models’ performance evaluation was obtained using different hybrid models.

---

ABSA of Indonesian customer reviews using IndoBERT: single- sentence and sentence-pair classification approaches
10.11591/eei.v13i5.8032
Aspect-based sentiment analysis (ABSA) task is important to identify user satisfaction from customer reviews by recognizing the sentiments of all aspects discussed in the reviews. This work investigates a novel study on the effectiveness and efficiency of three IndoBERT-based models for solving the ABSA task in Indonesian language. IndoBERT is a state-of-the-art transformer-based model, i.e., bidirectional encoder representations from transformers (BERT), that was pre-trained on Indonesian language. Our first model utilizes IndoBERT in a feature-based mode, paired with the convolutional neural network (CNN) and machine learning models, for single-sentence classification. Next, our second model is obtained by fine- tuning the IndoBERT model for a typical single-sentence classification to build an end-to-end model. At last, our third model also adopts a fine-tuning approach to use IndoBERT, but for sentence-pair classification by utilizing auxiliary sentences. Our results demonstrate that the third model, the fine- tuned IndoBERT for sentence-pair classification, gains the highest effectiveness. It demonstrates significant improvement over deep learning baselines (Word2Vec-CNN-XGBoost) by 23.6% and transformer-based baselines (mBERT-aux-NLIB) by 2.2% in terms of F-1 score. When considering both effectiveness and efficiency, the results show that the best- performing model is our second model, the fine-tuned IndoBERT for single- sentence classification.

---

Indonesian multilabel classification using IndoBERT embedding and MBERT classification
10.11591/ijece.v14i1.pp1071-1078
The rapid increase in social media activity has triggered various discussion spaces and information exchanges on social media. Social media users can easily tell stories or comment on many things without limits. However, this often triggers open debates that lead to fights on social media. This is because many social media users use toxic comments that contain elements of racism, radicalism, pornography, or slander to argue and corner individuals or groups. These comments can easily spread and trigger users vulnerable to mental disorders due to unhealthy and unfair debates on social media. Thus, a model is needed to classify comments, especially toxic ones, in Indonesian. Transformer-based model development and natural language processing approaches can be applied to create classification models. Some previous research related to the classification of toxic comments has been done, but the classification results of the model still require exploration to get optimal results. So, this research uses the proposed model by using different pre-trained models at the embedding and classification stages, in the embedding stage using Indonesia bidirectional encoder representations from transformers (IndoBERT), and classification using multilingual bidirectional encoder representations from transformers (MBERT). The proposed model provides optimal results with an F1 value of 0.9032.

---

Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction
10.1162/153244304322972685
Information extraction is a form of shallow text processing that locates a speciﬁed set of relevant items in a natural-language document. Systems for this task require signiﬁcant domain-speciﬁc knowledge and are time-consuming and difﬁcult to build by hand, making them a good application for machine learning. We present an algorithm, R APIER , that uses pairs of sample documents and ﬁlled templates to induce pattern-match rules that directly extract ﬁllers for the slots in the template. R APIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the ﬁller and the surrounding text. We present encouraging experimental results on two domains.

---

<i>Siren’s Song in the AI Ocean</i>: A Survey on Hallucination in Large Language Models
10.1162/coli.a.16
Abstract While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.

---

Evaluating Document Coherence Modeling
10.1162/tacl_a_00388
While pretrained language models (“LMs”) have driven impressive gains over morpho-syntactic and semantic tasks, their ability to model discourse and pragmatic phenomena is less clear. As a step towards a better understanding of their discourse modelling capabilities, we propose a sentence intrusion detection task. We examine the performance of a broad range of pretrained LMs on this detection task for English. Lacking a dataset for the task, we introduce INSteD, a novel intruder sentence detection dataset, containing 170,000+ documents constructed from English Wikipedia and CNN news articles. Our experiments show that pretrained LMs perform impressively in in-domain evaluation, but experience a substantial drop in the cross-domain setting, indicating limited generalisation capacity. Further results over a novel linguistic probe dataset show that there is substantial room for improvement, especially in the cross-domain setting.

---

SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization
10.1162/tacl_a_00453
In the summarization domain, a key requirement for summaries is to be factually consistent with the input document. Previous work has found that natural language inference (NLI) models do not perform competitively when applied to inconsistency detection. In this work, we revisit the use of NLI for inconsistency detection, finding that past work suffered from a mismatch in input granularity between NLI datasets (sentence-level), and inconsistency detection (document level). We provide a highly effective and light-weight method called SummaCConv that enables NLI models to be successfully used for this task by segmenting documents into sentence units and aggregating scores between pairs of sentences. We furthermore introduce a new benchmark called SummaC (Summary Consistency) which consists of six large inconsistency detection datasets. On this dataset, SummaCConv obtains state-of-the-art results with a balanced accuracy of 74.4%, a 5% improvement compared with prior work.

---

An IRT Approach to Constructing and Scoring Pairwise Preference Items Involving Stimuli on Different Dimensions: The Multi-Unidimensional Pairwise-Preference Model
10.1177/0146621604273988
This article proposes an item response theory (IRT) approach to constructing and scoring multidimensional pairwise preference items. Individual statements are administered and calibrated using a unidimensional single-stimulus model. Tests are created by combining multidimensional items with a small number of unidimensional pairings needed to identify the latent metric. Trait scores are then obtained using a multidimensional Bayes modal estimation procedure based on a mathematical model called MUPP, which is illustrated and tested here using Monte Carlo simulations. Simulation results show that the MUPP approach to test construction and scoring provides accurate parameter recovery in both one- and two-dimensional simulations, even with relatively few (say, 15%) unidimensional pairings. The implications of these results for constructing and scoring fake-resistant personality items are discussed.

---

The promise of NLP and speech processing technologies in language assessment
10.1177/0265532210364405
Advances in natural language processing (NLP) and automatic speech recognition and processing technologies offer new opportunities for language testing. Despite their potential uses on a range of language test item types, relatively little work has been done in this area, and it is therefore not well understood by test developers, researchers or users in language assessment. This paper introduces NLP for language assessment as an area of inquiry and practice by describing the historical roots coming from computational linguistics, statistical NLP, speech recognition and processing technologies, language assessment, and computer-assisted language learning. It outlines uses of NLP and speech recognition and processing technologies in language assessment through illustrations of current testing projects, and identifies areas in need of further development.

---

More efficient processes for creating automated essay scoring frameworks: A demonstration of two algorithms:
10.1177/0265532220937830
Automated essay scoring (AES) has emerged as a secondary or as a sole marker for many high-stakes educational assessments, in native and non-native testing, owing to remarkable advances in feature engineering using natural language processing, machine learning, and deep-neural algorithms. The purpose of this study is to compare the effectiveness and the performance of two AES frameworks, each based on machine learning with deep language features, or complex language features, and deep neural algorithms. More specifically, support vector machines (SVMs) in conjunction with Coh-Metrix features were used for a traditional AES model development, and the convolutional neural networks (CNNs) approach was used for more contemporary deep-neural model development. Then, the strengths and weaknesses of the traditional and contemporary models under different circumstances (e.g., types of the rubric, length of the essay, and the essay type) were tested. The results were evaluated using the quadratic weighted kappa (QWK) score and compared with the agreement between the human raters. The results indicated that the CNNs model performs better, meaning that it produced more comparable results to the human raters than the Coh-Metrix + SVMs model. Moreover, the CNNs model also achieved state-of-the-art performance in most of the essay sets with a high average QWK score.

---

When AI sees hotter: Overestimation bias in large language model climate assessments
10.1177/09636625251351575
Large language models (LLMs) have emerged as a novel form of media, capable of generating human-like text and facilitating interactive communications. However, these systems are subject to concerns regarding inherent biases, as their training on vast text corpora may encode and amplify societal biases. This study investigates overestimation bias in LLM-generated climate assessments, wherein the impacts of climate change are exaggerated relative to expert consensus. Through non-parametric statistical methods, the study compares expert ratings from the Intergovernmental Panel on Climate Change 2023 Synthesis Report with responses from GPT-family LLMs. Results indicate that LLMs systematically overestimate climate change impacts, and that this bias is more pronounced when the models are prompted in the role of a climate scientist. These findings underscore the critical need to align LLM-generated climate assessments with expert consensus to prevent misperception and foster informed public discourse.

---

EXPRESS: Corporate Use of Social Media after ESG Incidents
10.1177/10591478241277205
Despite increasing investments in Environmental, Social, and Governance (ESG) initiatives and practices, firms often fail to meet public expectations, causing ESG incidents. While firms often choose to remain silent after an incident, we argue that this is not attractive to firms anymore on social media platforms, where consumers and stakeholders can freely share information and concerns. That is, firms tend to use official social media accounts to increase communication frequency and communicate with stakeholders about the incidents (i.e., incident-related posts) after the occurrences. We are also interested in the extent to which firms would adjust their use of social media in terms of non-incident-related posts, as the current prevailing practical advice and studies offer contradicting predictions. Using data from different sources, we construct an event-based firm-day dataset and empirically show that firms significantly increase the number of social media posts after ESG incidents. The impact is more salient for firms in consumer-oriented industries and when the incident is more impactful. Using a semi-supervised, dictionary-based approach, we delve into the content of tweets and demonstrate that firms are inclined to increase both the number of incident-related and the number of non-incident-related tweets after an ESG incident. The follow-up analyses at the incident level indicate that firms that post more after an ESG incident experience a better reaction from the capital market, especially for customer-oriented firms or incidents that receive high attention from the traditional media.

---

Advancing Reproducibility and Accountability of Unsupervised Machine Learning in Text Mining: Importance of Transparency in Reporting Preprocessing and Algorithm Selection
10.1177/10944281221124947
Machine learning (ML) enables the analysis of large datasets for pattern discovery. ML methods and the standards for their use have recently attracted increasing attention in organizational research; recent accounts have raised awareness of the importance of transparent ML reporting practices, especially considering the influence of preprocessing and algorithm choice on analytical results. However, efforts made thus far to advance the quality of ML research have failed to consider the special methodological requirements of unsupervised machine learning (UML) separate from the more common supervised machine learning (SML). We confronted these issues by studying a common organizational research dataset of unstructured text and discovered interpretability and representativeness trade-offs between combinations of preprocessing and UML algorithm choices that jeopardize research reproducibility, accountability, and transparency. We highlight the need for contextual justifications to address such issues and offer principles for assessing the contextual suitability of UML choices in research settings.

---

Comparative study on credit risk assessment models for the new energy vehicle industry—predictive analytics using Logit models and BP neural networks based on composite versus single-factor indicator systems
10.1177/14727978251352155
The new energy vehicle (NEV) industry urgently requires tailored credit assessment frameworks to address its nonlinear risk characteristics, driven by rapid technological iterations and policy dependency. This study selects 46 listed companies in the NEV supply chain (2019–2023) as samples, innovatively integrating multi-source data including financial metrics, textual tone analysis of annual reports, and ESG ratings. A three-dimensional composite indicator system (“financial robustness–strategic credibility–environmental resilience”) is developed to compare the predictive performance of Logit models and backpropagation (BP) neural networks in estimating corporate default probabilities. Empirical findings reveal: (1) Under the composite indicator system, the BP neural network achieves 81.7% default prediction accuracy, significantly outperforming the Logit model (72.4%), with a 32-percentage-point improvement in identifying defaulted entities; (2) Using single financial indicators, the BP network maintains superiority (58.3% overall accuracy vs 48.3% for Logit), validating its capacity to capture complex risk features; (3) The composite system enhances prediction accuracy by 23.4% (BP) and 24.1% (Logit) compared to single indicators, demonstrating the early-warning value of non-financial metrics. These results suggest that the synergistic application of multi-source composite indicators and BP neural networks substantially improves the precision of dynamic credit risk assessment in the NEV sector, offering methodological support for differentiated financial services and regulatory oversight.

---

The Language of Optimism in Corporate Sustainability Reports: A Computerized Content Analysis
10.1177/23294906211065507
The discussion of sustainability reporting rarely addresses the inherent paradox within this concept—tremendous costs associated with sustainability efforts and lack of direct return on these investments. This study contributes to the discussion on sustainability by studying this paradox from the linguistic standpoint in order to answer a simple question: Why are sustainability reports produced? The study’s main contribution is evaluation of the place of sustainability reporting in the corporate communication genre: whether sustainability reporting is a vehicle of fair and objective sustainability disclosure or whether sustainability reporting belongs with marketing and promotional communication.

---

Knowledge Engineering in the Age of Neurosymbolic Systems
10.1177/29498732251320078
The field of knowledge engineering is experiencing a substantial impact from the rapid growth and widespread adoption of Neurosymbolic Systems (NeSys). In this paper, we investigate how NeSys are already used in knowledge engineering practices leading to the emergence of the new area of neurosymbolic knowledge engineering . To that end, we apply a data-driven analysis based on data collected in a large scale Systematic Mapping Study about systems used to create knowledge resource by employing NeSy approaches that combine Machine Learning and Semantic Web components. We characterise several aspects of this novel field, including specific approaches to knowledge engineering with NeSys identified from the data, the maturity of these systems as well as the main Machine Learning and Semantic Web modules used. Additionally, we also provide concrete examples of neurosymbolic knowledge engineering systems. We conclude with an overview of research challenges such as the need for new methodologies, increased auditability, and considering the impact of human users in neurosymbolic knowledge engineering.

---

Attention-based Sentiment Reasoner for aspect-based sentiment analysis
10.1186/s13673-019-0196-3
Aspect-based sentiment analysis (ABSA) is a powerful way of predicting the sentiment polarity of text in natural language processing. However, understanding human emotions and reasoning from text like a human continues to be a challenge. In this paper, we propose a model, named Attention-based Sentiment Reasoner (AS-Reasoner), to alleviate the problem of how to capture precise sentiment expressions in ABSA for reasoning. AS-Reasoner assigns importance degrees to different words in a sentence to capture key sentiment expressions towards a specific aspect, and transfers them into a sentiment sentence representation for reasoning in the next layer. To obtain appropriate importance degree values for different words in a sentence, two attention mechanisms we designed: intra attention and global attention. Specifically, intra attention captures the sentiment similarity between any two words in a sentence to compute weights and global attention computes weights by a global perspective. Experiments on all four English and four Chinese datasets show that the proposed model achieves state-of-the-art accuracy and macro-F1 results for aspect term level sentiment analysis and obtains the best accuracy for aspect category level sentiment analysis. The experimental results also indicate that AS-Reasoner is language-independent.

---

Pre-trained transformer-based language models for Sundanese
10.1186/s40537-022-00590-7
The Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.

---

Sentiment analysis of Indonesian datasets based on a hybrid deep-learning strategy
10.1186/s40537-023-00782-9
Abstract Various attempts have been conducted to improve the performance of text-based sentiment analysis. These significant attempts have focused on text representation and model classifiers. This paper introduced a hybrid model based on the text representation and the classifier models, to address sentiment classification with various topics. The combination of BERT and a distilled version of BERT (DistilBERT) was selected in the representative vectors of the input sentences, while the combination of long short-term memory and temporal convolutional networks was taken to enhance the proposed model in understanding the semantics and context of each word. The experiment results showed that the proposed model outperformed various counterpart schemes in considered metrics. The reliability of the proposed model was confirmed in a mixed dataset containing nine topics.

---

A computational analysis of aspect-based sentiment analysis research through bibliometric mapping and topic modeling
10.1186/s40537-025-01068-y
With the rising volume of public and consumer engagement on social media platforms, the field of aspect-based sentiment analysis (ABSA) has garnered substantial attention. ABSA contains the systematic extraction of aspects, the analysis of associated sentiments, and the temporal evolution of these sentiments. Researchers have responded to the burgeoning interest by innovating new methodologies and strategies to address specific research challenges, thereby navigating complex scenarios and evolving challenges within ABSA. While existing reviews on ABSA encompass strategies, methods, and applications utilizing survey methodologies, a conspicuous gap exists in literature specifically addressing the development of methodologies and topics and their interaction in ABSA. Furthermore, the application of topic modeling and keyword co-occurrence has been limited in the extant literature. This study conducts a comprehensive overview of the ABSA field by leveraging biblio-metrics, topic modeling, social network analysis, and keyword co-occurrence analysis to scrutinize 1325 ABSA research articles spanning the years 2009 to 2023. The analyses encompass research themes and topics, scientific collaborations, top publication sources, research areas, institutions, countries/regions, and publication and citation trends. Beyond examining and contrasting the connections between research topics and methodologies, this study identifies emerging trends and hotspots, providing researchers with insight into technical directions, limitations, and future research regarding ABSA topics and methodologies.

---

Sustainable strategy for corporate governance based on the sentiment analysis of financial reports with CSR
10.1186/s40854-018-0086-0
Focusing only on shareholders’ financial return is not consistent with the concept of sustainable corporate governance. In contrast to financial performance, corporate social responsibility (CSR) is a non-financial performance index. Financial reports consist of both financial and non-financial disclosures. These disclosures help investors make decisions. This paper characterizes the interaction between the sentiment analysis of financial reports and CSR scores. The classification accuracy through SVM exceeds 86%. The empirical study shows that the financial report sentiment based on the PESTEL model, Porter’s Five Forces model, and Value Chain (Primary and Support Activities) significantly correlates to the CSR score.

---

Grey zone in – greenwash out. A review of greenwashing research and implications for the voluntary-mandatory transition of CSR
10.1186/s40991-019-0044-9
As public concern over greenwashing has grown in the last two decades, academic research has increased correspondingly, and there is now a substantial body of research addressing issues related to greenwashing. In this paper, we therefore review and analyze greenwashing research, to provide an evaluation of trends and progress in the field and a synthesis of the empirical and conceptual results presented in existing studies. Our main finding leading to our theory contribution is the criticism raised in greenwashing research that the entirely voluntary CSR (Corporate Social Responsibility) approach facilitates the diffusion of greenwashing. The voluntary idea of CSR is still prevalent in the CSR literature and appears to be a grey-zone that creates space for misleading ‘green’ communication. Consequently, we propose that greenwashing could be better prevented with a combination of voluntary and mandatory aspects. The new paradigm should promote creative and effective corporate CSR initiatives, while at the same time design the limits and the rules for their accomplishments and communication, as firms would risk breaching legislation when overstretching CSR messages.

---

Sustainable small ports: performance assessment tool for management, responsibility, impact, and self-monitoring
10.1186/s41072-023-00142-z
This paper proposes a conceptual performance assessment tool for evaluating the environmental performance in small seaports. The developed tool is based and built based on a literature review. Ports, depending on their size, tend to have several sustainability and environmental management needs. However, especially small or cargo specialized ports do not often have sufficient resources to implement environmental effectiveness enhancing tools, even if they need them. This paper reviews international quality and environmental management tools, standards, and selected frameworks. These include International Organization for Standardization standards, and Global Reporting Initiative and Corporate Social Responsibility concepts. Because checklist type self-diagnosis solutions are the easiest to adopt, and universally most applicable, the proposed environmental performance measurement tool has four specific categories: (1) environmental management; (2) responsibility; (3) impact assessment; and (4) self-monitoring. The proposed tool allows the ports to assess whether their environmental management practices are comparable to more expensive standards and certificates. The paper concludes with a discussion on the limitations and challenges related to different port types and their specific needs.

---

A novel AI-driven approach to greenwashing: breakthroughs in the future fit between domain-specific Islamic enterprises with varying developmental progress and ESG landscapes
10.1186/s43093-025-00497-8
Abstract This research presents an innovative framework for exploring the phenomenon of greenwashing within the context of domain-specific enterprises that are adapting to diverse institutional landscapes. This is achieved through the deployment of a groundbreaking environmental, social, and governance leadership transition index, specifically designed for climate resilience. The index effectively integrates artificial intelligence and machine learning in conjunction with human cognitive expertise. Additionally, the study utilizes a chrono-convolutional neural network to investigate the dynamics of long-term memory concerning the nexus of innovative green fintech index and sectoral investments, thereby assessing the potential for greenwashing activities. This study also recognizes the varying institutional frameworks and approaches to climate risk management between emerging and developed nations. Adopting the quantile-based method, the long-term total connectedness index is assessed across market states. The analysis incorporates 11 sectoral investment indices from emerging and developed countries, comprising 22 international evolving investments. Temporal convolutional networks are leveraged to evaluate long-term memory under varying market conditions. The investigation highlights significant variances in the accuracy of long-term memory between indices representing emerging and developed markets. Notably, emerging markets exhibit a greater degree of precision about climate-smart initiatives. In particular, the mid-range quantiles of emerging market indices display the highest levels of accuracy across a broad spectrum of investments. These observations imply that developed markets, particularly under extreme economic conditions, may foster more favorable conditions for greenwashing practices. Moreover, a sectoral analysis reveals that, irrespective of market maturity, the energy and utilities sectors demonstrate the lowest propensity for greenwashing, while the information technology sector ranks similarly low in this regard. In contrast, real estate firms reveal a heightened susceptibility to greenwashing within developed markets, whereas their counterpart firms in emerging markets exhibit a markedly lower risk of engaging in such practices. The study offers valuable guidance to policymakers and regulators. Insights can inform targeted interventions promoting climate-resilient investment practices, contributing to sustainable development goals. Fostering reliable, interconnected emerging markets enhances the institutional quality and sustainability transition. Overall, the research provides a crucial perspective for navigating the complex landscape at the intersection of finance, climate resilience, and greenwashing. It illuminates the interplay between market fluctuations, green deception, and sustainable climate investment needs.

---

AI-driven sustainable finance: computational tools, ESG metrics, and global implementation
10.1186/s43093-025-00610-x
Abstract The integration of artificial intelligence (AI) into sustainable finance has emerged as a pivotal innovation for improving environmental, social, and governance (ESG) evaluation. However, ESG data remain fragmented, inconsistent, and prone to methodological opacity—particularly in climate-related investments. This systematic review analyzes 43 peer-reviewed articles and 23 institutional reports to examine whether machine-learning-based ESG scoring models outperform rule-based systems in predicting sustainable investment performance. Following the PRISMA protocol, we applied thematic coding, regional benchmarking, and methodological evaluation across AI–ESG applications. A comparative framework was used to assess AI’s role in ESG integration, predictive analytics, impact measurement, and risk management, with a focus on the climate domain. We also propose and refine the ESG–AI Maturity Index, evaluating AI adoption capacity across data quality, model transparency, and portfolio integration. Findings reveal that AI-enhanced models—particularly those using ensemble learning and sentiment analysis—demonstrate superior performance in forecasting ESG outcomes. However, challenges such as algorithmic bias, regional data gaps, and lack of interpretability remain persistent. The ESG–AI Maturity Index provides a preliminary diagnostic tool for evaluating institutional readiness to adopt AI in ESG scoring. This review contributes by clarifying the performance advantages and limitations of AI-based ESG systems, proposing a framework for maturity assessment, and highlighting urgent policy and technical needs to advance ethical, scalable AI adoption in sustainable finance.

---

Natural Language Processing for Writing and Speaking
10.1201/9781351264808-5
In this chapter we introduce the topic of automated scoring for written and spoken constructed responses and provide a detailed overview of the methodological underpinnings of natural language processing (NLP) that enable it. In the first section, we start with a high-level overview of the types of assessment tasks that automated text and speech scoring systems have been applied to and highlight the relevant aspects of the construct for each task type that motivate the use of automated scoring. In the second section, we provide an introduction to fundamental concepts from the fields of linguistics and NLP that are used in automated scoring technologies. In the third section, we introduce the standard processing pipeline for designing and using an automated scoring system for written or spoken language and provide in-depth overviews of the types of linguistic features that are typically extracted as well as the state-of-the-art NLP and speech processing methodologies that enables this. Finally, we conclude with a discussion of the readiness of automated scoring of text and speech for different use cases and discuss potential future directions for the field.

---

■ News, Trading, and Stock Return Volatility
10.1201/9781420099553-27
Existing literature finds that equity return variances over trading periods substantially exceed those over nontrading periods and suggests three potential explanations for the effect: (1) more public information reaches the marketplace during normal business hours; (2) the trading activity of informed investors reveals their private information inducing greater return variance; (3) the process of trading itself introduces noise into stock prices and returns as investors overreact to each other's trades. I offer a direct test of the public information, private information, and noise hypotheses utilizing data on order flow in the after-hours, pre-market, and regular trading sessions along with an extensive dataset of the contemporaneous public information flow for a large sample of Nasdaq securities. Consistent with the findings of prior literature, I show evidence in favor of the private information hypothesis. Contrary to the existing studies, however, my results also support the public information hypothesis.

---

Technological Innovations in Large-Scale Assessment
10.1207/s15324818ame1504_02
Computers have had a tremendous impact on assessment practices over the past half century. Advances in computer technology have substantially influenced the ways in which tests are made, administered, scored, and reported to examinees. These changes are particularly evident in computer-based testing, where the use of computers has allowed test developers to re-envision what test items look like and how they are scored. By integrating technology into assessments, it is increasingly possible to create test items that can sample as broad or as narrow a range of behaviors as needed while preserving a great deal of fidelity to the construct of interest. In this article we review and illustrate some of the current technological developments in computer-based testing, focusing on novel item formats and automated scoring methodologies. Our review indicates that a number of technological innovations in performance assessment are increasingly being researched and implemented by testing programs. In some cases, complex psychometric and operational issues have successfully been dealt with, but a variety of substantial measurement concerns associated with novel item types and other technological aspects impede more widespread use. Given emerging research, however, there appears to be vast potential for expanding the use of more computerized constructed-response type items in a variety of testing contexts.

---

Natural language processing and information extraction
10.12681/eadd/50236
Το αντικείμενο της παρούσας διατριβής είναι η Επεξεργασία Φυσικής Γλώσσας και η Εξαγωγή Πληροφοριών από κείμενα. Η διατριβή ασχολείται με τα ακόλουθα ερευνητικά προβλήματα που αφορούν συγκεκριμένες εργασίες της Επεξεργασίας Φυσικής Γλώσσας και Εξαγωγής Πληροφοριών: α) βελτίωση της διαδικασίας λήψης κλινικών αποφάσεων μέσω της Αναγνώρισης Βιοϊατρικών Οντοτήτων, β) βελτιστοποίηση της Εξόρυξης Βιοϊατρικών Επιχειρημάτων, γ) αποδοτικότερη τεχνική Μοντελοποίησης Γλώσσας με χρήση απομακρυσμένων πληροφοριών, δ) ανάπτυξη εφαρμογών Επεξεργασίας Φυσικής Γλώσσας για επίλυση πραγματικών προβλημάτων. Αρχικά, παρουσιάζονται μια σειρά από καινοτόμες αρχιτεκτονικές για καλύτερη Αναγνώριση Βιοϊατρικών Οντοτήτων, στοχευμένες σε οντότητες Ιατρικής Βασισμένης στη Τεκμηρίωση. Αυτές οι σημασιολογικές οντότητες είναι πιο περιγραφικές από βιοϊατρικές οντότητες γενικών κατηγοριών, προσφέρουν χρήσιμες πληροφορίες κατά την δημιουργία σχεδίων θεραπείας καθώς και είναι πιο δύσκολο να αναγνωριστούν από μοντέλα Μηχανικής Μάθησης. Η αρχιτεκτονικές βαθιών νευρωνικών δικτύων που προτάθηκαν σκοπεύουν στη βελτίωση της απόδοσης της αναγνώρισης όλων των βιοϊατρικών οντοτήτων και παρέχουν πιο αποδοτική λύση στις εργασίες αυτές. Επιπλέον μελετάται η χρήση οντοτήτων Ιατρικής Βασισμένης στη Τεκμηρίωση για την εξαγωγή συμπερασμάτων από ιατρικές δημοσιεύσεις. Για την περαιτέρω βελτίωση της κλινικής πρακτικής, επεκτείνουμε τη προσέγγισή μας για εξαγωγή συμπερασμάτων για τη δημιουργία δομών συμπερασμάτων από ιατρικές δημοσιεύσεις. Παρουσιάζεται μία καινοτόμα προσέγγιση Εξόρυξης Βιοϊατρικών Επιχειρημάτων, η οποία χρησιμοποιεί οντότητες Ιατρικής Βασισμένης στη Τεκμηρίωση. Το τελικό σύστημα έχει αυξημένη απόδοση σε όλες της εργασίες της Εξόρυξης Επιχειρημάτων και δημιουργεί πιο περιγραφικές δομές συμπερασμάτων. Αξιοποιώντας τη σημασία των σημασιολογικών οντοτήτων, παρουσιάζουμε δύο μεθοδολογίες για την χρήση αναφορικών πληροφοριών στην Αναπαράσταση Γλώσσας. Συνδυάζοντας την Επίλυση Αναφορών με την Αναπαράσταση Γλώσσας, παρουσιάζουμε νέες αρχιτεκτονικές για την αποδοτική χρήση αναφορικών εκφράσεων και τη δημιουργία κρυμμένων αναπαραστάσεων των οντοτήτων που περιγράφονται. Τα τελικά μοντέλα αναπαράστασης γλώσσας έχουν καλύτερη απόδοση σε εργασίες Επεξεργασίας Φυσικής Γλώσσας, με μικρή αύξηση στην υπολογιστική πολυπλοκότητα. Τέλος, παρουσιάζονται μια σειρά από εφαρμογές εργασιών Επεξεργασίας Φυσικής Γλώσσας σε πραγματικά προβλήματα και μελετάται η πρόσθετη αξία των εφαρμογών αυτών. Οι εφαρμογές αυτές είναι, ένα σύστημα ανάλυσης συναισθημάτων σε πραγματικό χρόνο με τη χρήση τεχνολογιών εξαγωγής τοποθεσίας, μία δομή για την μετατροπή μεγάλων αποθετηρίων εγγράφων σε Γράφους Γνώσης και μία πλατφόρμα εφαρμογής αλγορίθμων Μηχανιης Μάθησης, ανοιχτού κώδικα, για την διασύνδεση της ερευνητικής κοινότητας και της ελεύθερης αγοράς.

---

Optimizing data analysis through offline large language models and scalable data management techniques
10.12681/eadd/58941
Η σημασία που έχουν αποκτήσει τα δεδομένα σε ποικίλους τομείς, καθιστά αναγκαία την ανάπτυξη καινοτόμων προσεγγίσεων στη διαχείριση και ανάλυσή τους. Η παρούσα διδακτορική διατριβή ερευνά την ενσωμάτωση τοπικών Μεγάλων Γλωσσικών Μοντέλων (Offline LLMs) για την αυτόματη παραγωγή κώδικα, με στόχο την απλοποίηση των διαδικασιών ανάλυσης δεδομένων και, κατ’ επέκταση, τη βελτίωση της επεκτασιμότητας και αποδοτικότητας των συστημάτων διαχείρισης δεδομένων. Μέσω της αξιοποίησης τοπικά εκτελούμενων LLMs, η προτεινόμενη προσέγγιση δίνει τη δυνατότητα σε χρήστες χωρίς εκτεταμένες προγραμματιστικές γνώσεις να πραγματοποιούν αναλύσεις δεδομένων, συμβάλλοντας έτσι στη δημοκρατικοποίηση της επιστήμης των δεδομένων. Η έρευνα εστιάζει στην αρχιτεκτονική και την υλοποίηση επεκτάσιμων συστημάτων διαχείρισης δεδομένων, ικανών να διαχειρίζονται αποδοτικά σύνολα δεδομένων μεγάλου όγκου. Βασιζόμενη σε μια αποδοτική πλατφόρμα διαχείρισης δεδομένων, η εργασία εξετάζει τις δυνατότητες των offline LLMs για την παραγωγή αναλυτικού κώδικα, αναδεικνύοντας πώς αυτά τα μοντέλα μπορούν να μετασχηματίσουν φυσικές ερωτήσεις χρηστών σε εκτελέσιμα σενάρια, τα οποία διευκολύνουν τον χειρισμό και την ερμηνεία των δεδομένων. Μέσα από πειραματικές διαδικασίες και μελέτες περίπτωσης, παρουσιάζονται οι πρακτικές εφαρμογές και τα οφέλη της προτεινόμενης προσέγγισης. Τα αποτελέσματα αναδεικνύουν την αποδοτικότητα των τοπικά εκτελούμενων Μεγάλων Γλωσσικών Μοντέλων στην επιστήμη και ανάλυση δεδομένων. Η διατριβή συμβάλλει στο πεδίο παρουσιάζοντας μια μελέτη που συνδυάζει τεχνικές αυτόματης παραγωγής κώδικα μέσω τεχνητής νοημοσύνης με στιβαρές πρακτικές διαχείρισης δεδομένων, ανοίγοντας τον δρόμο για πιο αποδοτικές και φιλικές προς τον χρήστη λύσεις ανάλυσης δεδομένων.

---

Optimizing responsible decentralized machine learning methods for AI applications
10.12681/eadd/60207
Η εκθετική ανάπτυξη των τεχνολογιών Τεχνητής Νοημοσύνης έχει οδηγήσει στην ευρεία υιοθέτησή τους σε ένα μεγάλο φάσμα τομέων, όπως η υγεία, οι τηλεπικοινωνίες, η ενέργεια και οι εξατομικευμένες ψηφιακές υπηρεσίες. Καθώς τα συστήματα ΤΝ ενσωματώνονται ολοένα και περισσότερο σε κρίσιμες υποδομές και διαδικασίες λήψης αποφάσεων στον πραγματικό κόσμο, αναδύονται σημαντικές ηθικές και τεχνικές προκλήσεις. Αυτές περιλαμβάνουν κινδύνους για την ιδιωτικότητα των δεδομένων, έλλειψη διαφάνειας στις αποφάσεις των μοντέλων, ζητήματα δικαιοσύνης, καθώς και το αυξανόμενο περιβαλλοντικό αποτύπωμα των μεγάλων μοντέλων Μηχανικής Μάθησης. Η αντιμετώπιση αυτών των προκλήσεων απαιτεί μια μετάβαση από την αναζήτηση αποκλειστικά υψηλής απόδοσης προς ένα πιο υπεύθυνο μοντέλο χρήσης ΤΝ, δηλαδή μια προσέγγιση που δίνει προτεραιότητα σε ηθικές, βιώσιμες και ανθρωποκεντρικές αρχές για τον σχεδιασμό και την υλοποίηση ευφυών συστημάτων. Η παρούσα διατριβή προσεγγίζει την Υπεύθυνη ΤΝ μέσα από τρεις βασικούς πυλώνες: ιδιωτικότητα, ερμηνευσιμότητα και βιωσιμότητα. Εξετάζει πώς μπορούν να σχεδιαστούν σύγχρονες ροές Μηχανικής Μάθησης ώστε να ενσωματώνουν αυτές τις αρχές στην πράξη, αναπτύσσοντας μεθόδους που δεν είναι μόνο ακριβείς αλλά και ηθικά και κοινωνικά ευθυγραμμισμένες. Κεντρικό άξονα της διατριβής αποτελεί η εφαρμογή των αρχών της Υπεύθυνης ΤΝ σε αποκεντρωμένα περιβάλλοντα, όπου η διατήρηση της ιδιωτικότητας, η ετερογένεια του συστήματος και η απουσία κεντρικού ελέγχου εγείρουν μοναδικές προκλήσεις. Στο πλαίσιο αυτό, εξετάζεται η Ομοσπονδιακή Μάθηση ως κύρια τεχνική για την εφαρμογή αυτών των αρχών σε ποικίλους τομείς. Η Ομοσπονδιακή Μάθηση επιτρέπει σε πολλαπλούς φορείς που διαθέτουν δεδομένα, όπως έξυπνες συσκευές και ιδρύματα, να εκπαιδεύουν συνεργατικά μοντέλα χωρίς να μοιράζονται τα αρχικά δεδομένα, κάτι που ευθυγραμμίζεται φυσικά με κατανεμημένα περιβάλλοντα. Ωστόσο, παραμένουν σημαντικά ανοιχτά ζητήματα, όπως η αντιμετώπιση μη ανεξάρτητων και μη ομοιόμορφα κατανεμημένων δεδομένων, η ενεργειακή αποδοτικότητα και η ερμηνευσιμότητα των μοντέλων. Η διατριβή εξετάζει τέσσερα βασικά ερευνητικά ερωτήματα σχετικά με την εφαρμογή της Ομοσπονδιακής Μάθησης: πώς μπορεί να υποστηρίξει τις αρχές της Υπεύθυνης ΤΝ σε ετερογενή περιβάλλοντα, ποιες τεχνολογικές και συστημικές στρατηγικές ενισχύουν τη βιωσιμότητα και ενεργειακή αποδοτικότητα, με ποιους τρόπους μπορούν να ενσωματωθούν μέθοδοι ερμηνευσιμότητας στις ροές Μηχανικής Μάθησης και ποιες είναι οι κύριες προκλήσεις στην εφαρμογή της σε πραγματικές συνθήκες. Για την απάντηση αυτών των ερωτημάτων, παρουσιάζεται ένα ολοκληρωμένο μεθοδολογικό και πειραματικό πλαίσιο, βασισμένο σε εφαρμοσμένη έρευνα σε πολλούς τομείς. Στον τομέα των τηλεπικοινωνιών, εξετάζεται η χρήση της Ομοσπονδιακής Μάθησης για την πρόβλεψη κυκλοφορίας σε σταθμούς βάσης, με πραγματικά δεδομένα από δίκτυα κινητής τηλεφωνίας. Πραγματοποιείται αξιολόγηση διαφορετικών στρατηγικών συγχώνευσης μοντέλων, τεχνικών εξατομίκευσης σε επίπεδο κόμβου και ενσωμάτωσης εξωγενών δεδομένων για τη βελτίωση της ακρίβειας των προβλέψεων. Παράλληλα, αξιολογείται ο περιβαλλοντικός αντίκτυπος διαφορετικών παραμετροποιήσεων μέσω μετρήσεων κατανάλωσης ενέργειας, αναδεικνύοντας τους συμβιβασμούς μεταξύ επίδοσης και βιωσιμότητας. Στον τομέα της υγείας παρουσιάζονται δύο εφαρμογές που εστιάζουν στην προστασία της ιδιωτικότητας: η έγκαιρη ανίχνευση του Αυτιστικού Φάσματος μέσω δεδομένων από εκπαιδευτικό παιχνίδι, όπου προτείνεται ημι-εποπτευόμενο πλαίσιο με τεχνικές ανίχνευσης ανωμαλιών, κρυπτογράφηση και προηγμένες μεθόδους συγχώνευσης, και η αρχιτεκτονική ενός αποκεντρωμένου συστήματος παρακολούθησης ωορρηξίας που συνδυάζει Ομοσπονδιακή Μάθηση, τεχνολογίες blockchain και εργαλεία προστασίας ιδιωτικότητας, διατηρώντας ευαίσθητα δεδομένα τοπικά στις συσκευές των χρηστών. Για την ενίσχυση της αποδοτικότητας, προτείνεται νέος αλγόριθμος επιλογής κόμβων βασισμένος σε Κυψελιδωτά Αυτόματα, ο οποίος επιτρέπει δυναμική συμμετοχή κόμβων ανάλογα με διαθέσιμους πόρους, βελτιώνοντας την κλιμάκωση και λειτουργία σε περιορισμένα περιβάλλοντα. Συμπληρωματικά, ενσωματώνεται μηχανισμός επιλογής συμμετεχόντων με βάση βαθμολογία, ο οποίος αξιοποιείται σε εφαρμογή υγείας με κινητά τηλέφωνα, μειώνοντας επικοινωνιακή επιβάρυνση και ενισχύοντας τη σταθερότητα εκπαίδευσης σε μη ομοιόμορφα δεδομένα. Σε ό,τι αφορά την ερμηνευσιμότητα, εξετάζονται μοντέλα για την ανίχνευση Μη Τεχνικών Απωλειών σε ηλεκτρικά δίκτυα, με χρήση δεδομένων κατανάλωσης από παραδοσιακούς μετρητές. Εφαρμόζονται τεχνικές ερμηνευσιμότητας που επιτρέπουν στους παρόχους να κατανοούν τους παράγοντες που οδηγούν στις προβλέψεις, ενώ αποδεικνύεται ότι παρόμοια ακρίβεια επιτυγχάνεται και με περιορισμένο αριθμό χαρακτηριστικών, ενισχύοντας τη διαφάνεια και μειώνοντας το υπολογιστικό κόστος. Τέλος, πραγματοποιείται αναλυτική μελέτη των προκλήσεων που αφορούν την υλοποίηση της Ομοσπονδιακής Μάθησης σε πραγματικές συνθήκες, όπως η ετερογένεια των δεδομένων, οι τεχνικές προστασίας ιδιωτικότητας, η περιορισμένη διαθεσιμότητα δεδομένων και η εξατομίκευση μοντέλων. Διατυπώνονται πρακτικές συστάσεις που γεφυρώνουν το χάσμα μεταξύ θεωρητικής έρευνας και εφαρμοσμένης χρήσης. Συνολικά, η διατριβή αποδεικνύει ότι η κατασκευή Υπεύθυνων Συστημάτων Τεχνητής Νοημοσύνης είναι όχι μόνο εφικτή αλλά και αναγκαία. Μέσα από την ανάλυση των αρχών της ιδιωτικότητας, της ερμηνευσιμότητας, της βιωσιμότητας και των προκλήσεων υλοποίησης, συμβάλλει με πρακτικά εργαλεία και γνώσεις που ενισχύουν τη χρηστικότητα, την αξιοπιστία και τη συμμόρφωση της ΤΝ με τις ανάγκες της κοινωνίας.

---

Greenwash vs. Brownwash: Exaggeration and Undue Modesty in Corporate Sustainability Disclosure
10.1287/orsc.2014.0949
Corporate greenwashing has accelerated in recent years, bringing in its wake growing skepticism about corporate green claims. Although a theory of the drivers and deterrents of greenwashing has begun to emerge, it is static in nature and does not incorporate the full range of ways in which firms can misrepresent their environmental performance. Our contribution is threefold. First, we extend the theory of organizational information disclosure to incorporate the possibility of undue modesty about a firm's environmental, social, and governance practices. Second, we hypothesize about the drivers of exaggeration and undue modesty based on which of a firm's stakeholders are salient at a given point in time; to do so, we place the firm within a dynamic context that has largely been missing in the prior literature. Third, we test our hypotheses using a data set that allows us to directly compare corporate green claims against actual performance. Results reveal that corporate output growth, deregulation, and low profits under deregulation significantly affect the choice between greenwashing and brownwashing. The effects of growth and profits are mitigated by external scrutiny.

---

Reluctant Disclosure and Transparency: Evidence from Environmental Disclosures
10.1287/orsc.2019.1298
Strategic management research increasingly examines firms’ strategies for corporate environmental and social disclosures. There are benefits to being perceived as having superior environmental perf...

---

Parental Leave and beyond: recent international developments, current issues and future directions
10.1332/policypress/9781447338772.003.0020
This chapter investigates the recent developments and current issues around parental leave such as multiple rationales for leave, policy designs, the politics of flexibility and choice, eligibility (employment related entitlement or social right?), adapting policies to increasing diversity in employment and household type. In addition, it covers topics such as the relationship between parental leave, workplace and other policy areas, future directions for politics and policy for parental leave.

---

A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations
10.1371/journal.pone.0179488
Evidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations.

---

Comparing driving behavior of humans and autonomous driving in a professional racing simulator.
10.1371/journal.pone.0245320
Motorsports have become an excellent playground for testing the limits of technology, machines, and human drivers. This paper presents a study that used a professional racing simulator to compare the behavior of human and autonomous drivers under an aggressive driving scenario. A professional simulator offers a close-to-real emulation of underlying physics and vehicle dynamics, as well as a wealth of clean telemetry data. In the first study, the participants' task was to achieve the fastest lap while keeping the car on the track. We grouped the resulting laps according to the performance (lap-time), defining driving behaviors at various performance levels. An extensive analysis of vehicle control features obtained from telemetry data was performed with the goal of predicting the driving performance and informing an autonomous system. In the second part of the study, a state-of-the-art reinforcement learning (RL) algorithm was trained to control the brake, throttle and steering of the simulated racing car. We investigated how the features used to predict driving performance in humans can be used in autonomous driving. Our study investigates human driving patterns with the goal of finding traces that could improve the performance of RL approaches. Conversely, they can also be applied to training (professional) drivers to improve their racing line.

---

Finding common development paths in voluntary national reviews reporting on sustainable development goals using aspect-based sentiment analysis
10.1371/journal.pone.0307886
Voluntary National Reviews (VNRs) provide a platform for participating countries to share their experiences, failures, and successes in achieving the United Nations (UN) Sustainable Development Goals (SDGs). The objective of this study is to gain a deeper understanding of the narrative elements, particularly the sentiment, in VNRs in order to more effectively assess and support global SDG progress. A total of 232 VNRs from 166 countries are analyzed using Aspect-Based Sentiment Analysis (ABSA) to extract each country’s sentiment toward the 17 SDGs. The sentiment scores are then compared to the corresponding official UN SDG scores, and countries are grouped by their sentiment toward all 17 SDGs to identify potential common development pathways. The analysis uncovers a notable positive correlation between the reported sentiment and official SDG scores for SDG 2 (zero hunger) and SDG 11 (sustainable cities and communities), and a negative correlation for SDG 5 (gender equality). Conversely, this relationship is not significant for the majority of SDGs, suggesting that VNR narratives may not directly reflect actual progress. A t-distributed stochastic neighbor embedding (t-SNE) approach indicates a consistent sentiment score among developed countries. In contrast, there are greater differences in reporting sentiment among Emerging Markets, Frontier Markets, and Least Developed Countries (LDCs), where there is greater dispersion (especially among LDCs) and sentiment in reporting on SDG progress that appears to have changed from one reporting year to another. These findings highlight the need to interpret VNRs in the context of each country’s unique situation and challenges specific to each country.

---

Media coverage as a moderator in the nexus between audit quality and ESG performance: Evidence from China
10.1371/journal.pone.0312510
In response to growing pressure on companies to manage and improve their reputation regarding environmental, social, and governance (ESG) issues, the audit is regarded as a vital resource for ensuring ESG risk management, improving transparency, mitigating opportunistic constraints, and guaranteeing accurate reporting. The objective of this paper was to investigate the role of audit quality in improving ESG performance, as well as to examine the role of media coverage represented by ESG controversy score in moderating these relationships. We analyzed 303 Chinese companies with 2,121 observations covering the period from 2017 to 2023. The results suggest that the effects of audit quality as measured by the Big 4 and audit fee on improving ESG performance are positive but not significant. On the other hand, the results reveal that media coverage serves as a positive, albeit non-significant, moderating variable between audit quality measured by the Big 4 and ESG performance, while it has a significant negative effect when audit quality is evaluated based on audit fees. The results indicate that improving ESG performance is significantly linked to auditors intensifying their practices and implementing their work more stringently. More importantly, media coverage is an important additional driver and economic incentive that encourages companies to steer clear of poor ESG-related practices.

---

A Conceptual Framework for Subdomain Specific Pre-Training of Large Language Models for Green Claim Detection
10.14207/ejsd.2023.v12n4p319
Detection of false or misleading green claims (referred to as “greenwashing”) within company sustainability disclosures is challenging for a number of reasons, which include the textual and qualitative nature, volume, and complexity of such disclosures. In recent years, notable progress made in the fields of artificial intelligence and specifically, large language models (LLMs), has showcased the capacity of these tools to effectively analyse extensive and intricate textual data, including the contents of sustainability disclosures. Transformer-based LLMs, such as Google’s BERT architecture, were trained on general domain text corpora. Subsequent research has shown that further pre-training of such LLMs on specific domains, such as the climate or sustainability domains, may improve performance. However, previous research often uses text corpora that exhibit significant variation across topics and language and which often consist of heterogeneous subdomains. We therefore propose a conceptual framework for further pre-training of transformer based LLMs using text corpora relating to specific sustainability subdomains i.e. subdomain specific pre-training. We do so as a basis for the improved performance of such models in analysing sustainability disclosures. The main contribution is a conceptual framework to advance the use of LLMs for the reliable identification of green claims and ultimately, greenwashing. 
Keywords: greenwashing, artificial intelligence, sustainability, sustainability reporting, sustainability disclosures.

---

Towards efficient multi-objective alignment of large language models
10.14711/thesis-991013340354503412
This study addresses the challenge of multi-objective alignment of foundation models, particularly Large Language Models (LLMs), with human values and preferences—a crucial step towards developing helpful and harmless AI systems. Fine-tuning large foundation models using reinforcement learning (RL) is often costly and unstable. Additionally, the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), a novel approach that conditions the response of a foundation model on multiple rewards within its prompt context and employs supervised fine-tuning for alignment. RiC is characterized by its simplicity and adaptability, requiring only the supervised fine-tuning of a sing...[ Read more ]

---

Federated Learning for Secure and Intelligent Data Analytics in Banking and Insurance
10.14741/ijmcr/v.8.2.17
The increasing volume of financial transactions in banking and insurance sectors necessitates advanced fraud detection techniques while ensuring data privacy and security. Traditional centralized machine learning approaches pose significant risks related to data breaches, regulatory non-compliance, and lack of interpretability. To address these challenges, this paper proposes a Federated Learning (FL)-based framework for secure and intelligent data analytics in financial services. FL enables multiple institutions to collaboratively train fraud detection models without sharing raw data, preserving privacy and compliance with regulations such as GDPR and PCI-DSS.The proposed methodology applies Min-Max Normalization for data preprocessing to ensure balanced feature scaling, enhancing model convergence and performance. Secure aggregation techniques, including homomorphic encryption and secure multiparty computation, are incorporated to prevent data leakage and strengthen model robustness. Additionally, Shapley values are utilized to improve model interpretability, ensuring transparency in fraud detection and financial risk assessment. The framework is evaluated against traditional machine learning and centralized learning models, considering key performance metrics such as accuracy, efficiency, security, and computational cost. Experimental results demonstrate that the FL-based approach significantly enhances fraud detection accuracy while reducing privacy risks and maintaining regulatory compliance. Moreover, the proposed model achieves improved scalability and adaptability to evolving financial threats. This research demonstrates that the FL-based approach significantly enhances fraud detection accuracy while ensuring privacy and compliance. The accuracy results indicate improvements, with Traditional ML achieving 85.4%, Centralized ML at 88.7%, and Federated Learning reaching 92.3%, highlighting its effectiveness in financial security applications

---

Greenwashing: The Darker Side Of CSr
10.15373/2249555x/mar2014/20
Greenwashing is a practice followed by organisations in which unsubstantiated or misleading claims are made of the environmental and social attributes of a product, service or the company as a brand. Greenwashing practice is adopted to make the company look more environment-friendly than it actually is, by spending more money, time and efforts on marketing its products as ‘green’, rather than actually minimizing its adverse impact on the environment. This paper studies the green marketing practices of certain selected companies belonging to four sectors Automobile, Electronics, Food & Beverages and Personal Care Sector, through analysis of their advertisements, company websites and sustainability reports. The main objective of the paper is to identify the extent of green washing done by the companies and to rate their environmental claims on the weighted scale of 1 to 5. Further, this paper correlates the greenwashing score with the overall CSR score, along with cross-sector analysis of their greenwashing scores. The paper finds that even the companies with a high overall CSR score are involved in some form of greenwashing practices. The authors also suggest ways and means for companies to avoid greenwashing, for consumers to spot it and for regulators to curb it. INTrODUCTION Environmental Sustainability is currently a burning issue worldwide. It has become a major cause of concern for governments, corporates and individuals. The rapid globalisation and industrialisation in the past few decades have significantly contributed towards environmental degradation in the form of pollution, greenhouse gas emissions, ozone depletion, global warming etc. Brundtland (1987) defined sustainable development as– “development that meets the needs of the present generation without compromising the ability of future generations to meet their own needs.” The consumers, investors and other stakeholders are increasingly becoming conscious about environment and society. They keep environmental and social considerations in mind while taking buying and investment decisions. Thus the companies are under a constant pressure to perform well on these grounds and to think beyond profits. Corporate social responsibility means that the organisations should be accountable towards all the stakeholders including consumers, investors, environment, employees, community, government and public at large. They should align their operations and decisions in accordance with the expectations of stakeholders (ISO 26000). There is a growing trend among companies to adopt “go green” strategy in order to gain an edge over their competitors. Therefore, the concepts of green marketing and sustainability reporting have become significant. According to Global Reporting Initiative (2011) “Sustainability reporting is the practice of measuring, disclosing, and being accountable to internal and external stakeholders for organizational performance towards the goal of sustainable development”. Green marketing is a holistic marketing phenomenon used by an organisation to promote the environment-friendly image of its products and the organisation as a whole. It encompasses innovation and modification in product development, manufacturing, packaging and advertising. Green marketing is used as a weapon by companies to compete in the global market.In today’s age of sustainability it is often said that “green is the new black”. The practice of green marketing is being misused by companies in order to build their false green brand image in the eyes of consumers and investors. This is nothing but greenwashing. According to Greenpeace (www.stopgreenwash.org) – “greenwashing is the act of misleading consumers regarding the environmental practices of a company or the environmental benefits of a product or service.”It involves use of deceptive and manipulative sustainable claims by companies to portray a superficial eco-friendly image than it actually is, by investing more resources on marketing its products as ‘green’ rather than actually minimizing its adverse impact on the environment. Delmas and Cuerel Burbano (2011) classified the drivers of greenwashing into market, non-market, organisational and individual drivers. These are shown in Figure 1 below. Source: Delmas and Cuerel Burbano (2011) Figure 1 – Drivers of Greenwashing In this paper, we analyse the greenwashing practices of select popular companies. The following sections describe greenwashing practices, regulations & certifications, review of literature, objectives of study, hypotheses, research methodology, results, recommendations, limitations and scope for further research. GrEENWASHING PrACTICES The environmental consciousness among the consumers and companies has its origin in mid 1960’s which led to the adoption of green marketing strategies by companies worldwide. The environmental disasters such as Bhopal gas tragedy (1984), Chernobyl nuclear power-plant disaster (1986), Exxon Valdez oil spill (1989), etc. prompted the companies to prac-

---

Aspect Based Sentiment analysis SemEval-2014 Task 4
10.15520/ajcsit.v4i8.9.g5
The “Aspect Based Sentiment Analysis” task focuses on the recognition of aspect term and category and classification of emotions (positive, negative, conflict, neutral) in restaurant reviews for the aspect.  In this paper we propose the system for recognizing aspects and analyzing the sentiments using SVM for the restaurant review dataset. We compare the performance of the system with well-known KNN classifier.

---

Large language model (LLM) comparison between GPT-3 and PaLM-2 to produce Indonesian cultural content
10.15587/1729-4061.2024.309972
Large language models can help to compile content with a cultural theme. However, any information generated by large language models needs to be evaluated to see the truth/fact of the information generated. With many studies discussing the comparison of the capabilities of large language models, there is not much research that directly discusses the comparison of the performance of large language models in producing Indonesian cultural content. This research compares the correctness of the information generated by the large language model using the expert judgment method when creating Indonesian cultural content and its fine-tuning capabilities evaluated using BERTScore. The evaluation method was successfully applied and the results show that in this case, PaLM-2 included less misinformation while GPT-3 excelled in fine-tuning. Using the combination of expert judgment and BERTScore makes it possible to evaluate large language models and obtain additional valid training data to correct deficiencies. The results showed that PaLM-2 produced more valid content with a score of 27 points, while GPT-3 scored 8 points. For training on new datasets/fine-tuning, it was found that the GPT-3 language model was able to learn the dataset more quickly, with a time of 50 minutes and a cost of IDR 27,000, while PaLM-2 took 2 hours 10 minutes and a cost of IDR 1,377,204. For the training dataset evaluation results, GPT-3 is superior with an average of all scores reaching 0.85205. Meanwhile, the PaLM-2 Tuned Model got an average overall score of 0.78942. In this case, the GPT-3 Tuned Model is superior by 8 %. In practice, this method can be used if the assessment is descriptive and requires direct assessment from experts

---

Trends in Explanations: Understanding and Debugging Data-driven Systems
10.1561/1900000074
Humans reason about the world around them by seeking to understand why and how something occurs. The same principle extends to the technology that so many of human activities increasingly rely on. Issues of trust, transparency, and understandability are critical in promoting adoption and proper use of systems. However, with increasing complexity of the systems and technologies we use, it is hard or even impossible to comprehend their function and behavior, and justify surprising observations through manual investigation alone. Explanation support can ease humans’ interactions with technology: explanations can help users understand a system’s function, justify system results, and increase their trust in automated decisions. Our goal in this article is to provide an overview of existing work in explanation support for data-driven processes, through a lens that identifies commonalities across varied problem settings and solutions. We suggest a classification of explainability requirements across three dimensions: the target of the explanation (“What”), the audience of the Boris Glavic, Alexandra Meliou and Sudeepa Roy (2021), “Trends in Explanations: Understanding and Debugging Data-driven Systems”, Foundations and Trends® in Databases: Vol. 11, No. 3, pp 226–318. DOI: 10.1561/1900000074. The version of record is available at: http://dx.doi.org/10.1561/1900000074

---

Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM
10.1609/aaai.v32i1.12048

 
 Analyzing people’s opinions and sentiments towards certain aspects is an important task of natural language understanding. In this paper, we propose a novel solution to targeted aspect-based sentiment analysis, which tackles the challenges of both aspect-based sentiment analysis and targeted sentiment analysis by exploiting commonsense knowledge. We augment the long short-term memory (LSTM) network with a hierarchical attention mechanism consisting of a target-level attention and a sentence-level attention. Commonsense knowledge of sentiment-related concepts is incorporated into the end-to-end training of a deep neural network for sentiment classification. In order to tightly integrate the commonsense knowledge into the recurrent encoder, we propose an extension of LSTM, termed Sentic LSTM. We conduct experiments on two publicly released datasets, which show that the combination of the proposed attention architecture and Sentic LSTM can outperform state-of-the-art methods in targeted aspect sentiment tasks.
 


---

Multi-Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis
10.1609/aaai.v33i01.3301371
As a fundamental task of sentiment analysis, aspect-level sentiment analysis aims to identify the sentiment polarity of a specific aspect in the context. Previous work on aspect-level sentiment analysis is text-based. With the prevalence of multimodal user-generated content (e.g. text and image) on the Internet, multimodal sentiment analysis has attracted increasing research attention in recent years. In the context of aspect-level sentiment analysis, multimodal data are often more important than text-only data, and have various correlations including impacts that aspect brings to text and image as well as the interactions associated with text and image. However, there has not been any related work carried out so far at the intersection of aspect-level and multimodal sentiment analysis. To fill this gap, we are among the first to put forward the new task, aspect based multimodal sentiment analysis, and propose a novel Multi-Interactive Memory Network (MIMN) model for this task. Our model includes two interactive memory networks to supervise the textual and visual information with the given aspect, and learns not only the interactive influences between cross-modality data but also the self influences in single-modality data. We provide a new publicly available multimodal aspect-level sentiment dataset to evaluate our model, and the experimental results demonstrate the effectiveness of our proposed model for this new task.

---

Knowing What, How and Why: A Near Complete Solution for Aspect-Based Sentiment Analysis
10.1609/aaai.v34i05.6383
Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from “Waiters are very friendly and the pasta is simply average” could be (‘Waiters’, positive, ‘friendly’). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods.

---

Target-Aspect-Sentiment Joint Detection for Aspect-Based Sentiment Analysis
10.1609/aaai.v34i05.6447
Aspect-based sentiment analysis (ABSA) aims to detect the targets (which are composed by continuous words), aspects and sentiment polarities in text. Published datasets from SemEval-2015 and SemEval-2016 reveal that a sentiment polarity depends on both the target and the aspect. However, most of the existing methods consider predicting sentiment polarities from either targets or aspects but not from both, thus they easily make wrong predictions on sentiment polarities. In particular, where the target is implicit, i.e., it does not appear in the given text, the methods predicting sentiment polarities from targets do not work. To tackle these limitations in ABSA, this paper proposes a novel method for target-aspect-sentiment joint detection. It relies on a pre-trained language model and can capture the dependence on both targets and aspects for sentiment prediction. Experimental results on the SemEval-2015 and SemEval-2016 restaurant datasets show that the proposed method achieves a high performance in detecting target-aspect-sentiment triples even for the implicit target cases; moreover, it even outperforms the state-of-the-art methods for those subtasks of target-aspect-sentiment detection that they are competent to.

---

A System for Medical Information Extraction and Verification from Unstructured Text
10.1609/aaai.v34i08.7042
A wealth of medical knowledge has been encoded in terminologies like SNOMED CT, NCI, FMA, and more. However, these resources are usually lacking information like relations between diseases, symptoms, and risk factors preventing their use in diagnostic or other decision making applications. In this paper we present a pipeline for extracting such information from unstructured text and enriching medical knowledge bases. Our approach uses Semantic Role Labelling and is unsupervised. We show how we dealt with several deficiencies of SRL-based extraction, like copula verbs, relations expressed through nouns, and assigning scores to extracted triples. The system have so far extracted about 120K relations and in-house doctors verified about 5k relationships. We compared the output of the system with a manually constructed network of diseases, symptoms and risk factors build by doctors in the course of a year. Our results show that our pipeline extracts good quality and precise relations and speeds up the knowledge acquisition process considerably.

---

Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis.
10.1609/aaai.v35i16.17659
Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow finer-grained inferences about sentiment to be drawn from the same text, depending on context. For example, a given text can have different targets (e.g., neighborhoods) and different aspects (e.g., price or safety), with different sentiment associated with each target-aspect pair. In this paper, we investigate whether adding context to self-attention models improves performance on (T)ABSA. We propose two variants of Context-Guided BERT (CG-BERT) that learn to distribute attention under different contexts. We first adapt a context-aware Transformer to produce a CG-BERT that uses context-guided softmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model that learns a compositional attention that supports subtractive attention. We train both models with pretrained BERT on two (T)ABSA datasets: SentiHood and SemEval-2014 (Task 4). Both models achieve new state-of-the-art results with our QACG-BERT model having the best performance. Furthermore, we provide analyses of the impact of context in the our proposed models. Our work provides more evidence for the utility of adding context-dependencies to pretrained self-attention-based language models for context-based natural language tasks.

---

Mastering the Explicit Opinion-Role Interaction: Syntax-Aided Neural Transition System for Unified Opinion Role Labeling
10.1609/aaai.v36i10.21404
Unified opinion role labeling (ORL) aims to detect all possible opinion structures of 'opinion-holder-target' in one shot, given a text. The existing transition-based unified method, unfortunately, is subject to longer opinion terms and fails to solve the term overlap issue. Current top performance has been achieved by employing the span-based graph model, which however still suffers from both high model complexity and insufficient interaction among opinions and roles. In this work, we investigate a novel solution by revisiting the transition architecture, and augmenting it with a pointer network (PointNet). The framework parses out all opinion structures in linear-time complexity, meanwhile breaks through the limitation of any length of terms with PointNet. To achieve the explicit opinion-role interactions, we further propose a unified dependency-opinion graph (UDOG), co-modeling the syntactic dependency structure and the partial opinion-role structure. We then devise a relation-centered graph aggregator (RCGA) to encode the multi-relational UDOG, where the resulting high-order representations are used to promote the predictions in the vanilla transition system. Our model achieves new state-of-the-art results on the MPQA benchmark. Analyses further demonstrate the superiority of our methods on both efficacy and efficiency.

---

Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations
10.1609/aaai.v37i12.26661
Knowledge tracing (KT) is a crucial technique to predict students’ future performance by observing their historical learning processes. Due to the powerful representation ability of deep neural networks, remarkable progress has been made by using deep learning techniques to solve the KT problem. The majority of existing approaches rely on the homogeneous question assumption that questions have equivalent contributions if they share the same set of knowledge components. Unfortunately, this assumption is inaccurate in real-world educational scenarios. Furthermore, it is very challenging to interpret the prediction results from the existing deep learning based KT models. Therefore, in this paper, we present QIKT, a question-centric interpretable KT model to address the above challenges. The proposed QIKT approach explicitly models students’ knowledge state variations at a ﬁne-grained level with question-sensitive cognitive representations that are jointly learned from a question-centric knowledge acquisition module and a question-centric problem solving module. Meanwhile, the QIKT utilizes an item response theory based prediction layer to generate interpretable prediction results. The proposed QIKT model is evaluated on three public real-world educational datasets. The results demonstrate that our approach is superior on the KT prediction task, and it outperforms a wide range of deep learning based KT models in terms of prediction accuracy with better model interpretability. To encourage reproducible results, we have provided all the datasets and code at https://pykt.org/.

---

Unveiling the Tapestry of Automated Essay Scoring: A Comprehensive Investigation of Accuracy, Fairness, and Generalizability
10.1609/aaai.v38i20.30254
Automatic Essay Scoring (AES) is a well-established educational pursuit that employs machine learning to evaluate student-authored essays. While much effort has been made in this area, current research primarily focuses on either (i) boosting the predictive accuracy of an AES model for a specific prompt (i.e., developing prompt-specific models), which often heavily relies on the use of the labeled data from the same target prompt; or (ii) assessing the applicability of AES models developed on non-target prompts to the intended target prompt (i.e., developing the AES models in a cross-prompt setting). Given the inherent bias in machine learning and its potential impact on marginalized groups, it is imperative to investigate whether such bias exists in current AES methods and, if identified, how it intervenes with an AES model's accuracy and generalizability. Thus, our study aimed to uncover the intricate relationship between an AES model's accuracy, fairness, and generalizability, contributing practical insights for developing effective AES models in real-world education. To this end, we meticulously selected nine prominent AES methods and evaluated their performance using seven distinct metrics on an open-sourced dataset, which contains over 25,000 essays and various demographic information about students such as gender, English language learner status, and economic status. Through extensive evaluations, we demonstrated that: (1) prompt-specific models tend to outperform their cross-prompt counterparts in terms of predictive accuracy; (2) prompt-specific models frequently exhibit a greater bias towards students of different economic statuses compared to cross-prompt models; (3) in the pursuit of generalizability, traditional machine learning models (e.g., SVM) coupled with carefully engineered features hold greater potential for achieving both high accuracy and fairness than complex neural network models.

---

Aspect Enhancement and Text Simplification in Multimodal Aspect-Based Sentiment Analysis for Multi-Aspect and Multi-Sentiment Scenarios
10.1609/aaai.v39i2.32161
Multimodal Aspect-Based Sentiment Analysis (MABSA) plays a pivotal role in the advancement of sentiment analysis technology. Although current methods strive to integrate multimodal information to enhance the performance of sentiment analysis, they still face two critical challenges when dealing with multi-aspect and multi-sentiment data: i) the importance of aspect terms within multimodal data is often overlooked, and ii) models fail to accurately associate specific aspect terms with corresponding sentiment words in multi-aspect and multi-sentiment sentences. To tackle these problems, we propose a novel multimodal aspect-based sentiment analysis method that combines Aspect Enhancement and Text Simplification (AETS). Specifically, we develop an aspect enhancement module that boosts the ability of model to discern relevant aspect terms. Concurrently, we employ text simplification module to simplify and restructure multi-aspect and multi-sentiment texts, accurately capturing aspects and their corresponding sentiments while reducing irrelevant information. Leveraging this method, we perform three tasks including multimodal aspect term extraction, multimodal aspect sentiment classification, and joint multimodal aspect-based sentiment analysis. Experimental results indicate that our proposed AETS model achieved state-of-the-art performance on two benchmark datasets.

---

VERO: Verification and Zero-Shot Feedback Acquisition for Few-Shot Multimodal Aspect-Level Sentiment Classification
10.1609/aaai.v39i24.34707
Deep learning approaches for multimodal aspect-level sentiment classification (MALSC) often require extensive data, which is costly and time-consuming to obtain. To mitigate this, current methods typically fine-tune small-scale pretrained models like BERT and BART with few-shot examples. While these models have shown success, Large Vision-Language Models (LVLMs) offer significant advantages due to their greater capacity and ability to understand nuanced language in both zero-shot and few-shot settings. However, there is limited work on fine-tuning LVLMs for MALSC. A major challenge lies in selecting few-shot examples that effectively capture the underlying patterns in data for these LVLMs. To bridge this research gap, we propose an acquisition function designed to select challenging samples for the few-shot learning of LVLMs for MALSC. We compare our approach, Verification and ZERO-shot feedback acquisition (VERO), with diverse acquisition functions for few-shot learning in MALSC. Our experiments show that VERO outperforms prior methods, achieving an F1 score improvement of up to 6.07% on MALSC benchmark datasets.

---

My Computer Is an Honor Student - but How Intelligent Is It? Standardized Tests as a Measure of AI
10.1609/aimag.v37i1.2636
Given the well-known limitations of the Turing Test, there is a need for objective tests to both focus attention on, and measure progress towards, the goals of AI. In this paper we argue that machine performance on standardized tests should be a key component of any new measure of AI, because attaining a high level of performance requires solving significant AI problems involving language understanding and world modeling - critical skills for any machine that lays claim to intelligence. In addition, standardized tests have all the basic requirements of a practical test: they are accessible, easily comprehensible, clearly measurable, and offer a graduated progression from simple tasks to those requiring deep understanding of the world. Here we propose this task as a challenge problem for the community, summarize our state-of-the-art results on math and science tests, and provide supporting datasets

---

Landscape of Large Language Models in Global English News: Topics, Sentiments, and Spatiotemporal Analysis
10.1609/icwsm.v18i1.31416
Generative AI has exhibited considerable potential to transform various industries and public life. The role of news media coverage of generative AI is pivotal in shaping public perceptions and judgments about this significant technological innovation. This paper provides in-depth analysis and rich insights into the temporal and spatial distribution of topics, sentiment, and substantive themes within global news coverage focusing on the latest emerging technology—generative AI. We collected a comprehensive dataset of English news articles (January 2018 to November 2023, N = 24,827) through ProQuest databases. For topic modeling, we employed the BERTopic technique and combined it with qualitative coding to identify semantic themes. Subsequently, sentiment analysis was conducted using the RoBERTa-base model. Analysis of temporal patterns in the data reveals notable variability in coverage across key topics—business, corporate technological development, regulation and security, and education—with spikes in articles coinciding with major AI developments and policy discussions. Sentiment analysis shows a predominantly neutral to positive media stance, with the business-related articles exhibiting more positive sentiment, while regulation and security articles receive a reserved, neutral to negative sentiment. Our study offers a valuable framework to investigate global news discourse and evaluate news attitudes and themes related to emerging technologies.

---

Explainable Deep Learning: A Field Guide for the Uninitiated
10.1613/jair.1.13200
Deep neural networks (DNNs) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model’s input drive its decisions. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN’s decisions has thus blossomed into an active and broad area of research. The field’s complexity is exacerbated by competing definitions of what it means “to explain” the actions of a DNN and to evaluate an approach’s “ability to explain”. This article offers a field guide to explore the space of explainable deep learning for those in the AI/ML field who are uninitiated. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) discusses user-oriented explanation design and future directions. We hope the guide is seen as a starting point for those embarking on this research field.

---

Large Language Models Powered Aspect-Based Sentiment Analysis for Enhanced Customer Insights
10.18089/tms.20250101
In the age of social networks, user-generated content has become vital for organizations in tourism and hospitality. Traditional sentiment analysis methods often struggle to process large volumes of data and capture implicit sentiments. This study examines the potential of Aspect-Based Sentiment Analysis (ABSA) using Large Language Models (LLMs) to enhance sentiment analysis. By employing GPT-4o via ChatGPT, we benchmark three approaches: a fuzzy logic-based method, manual human analysis, and a new ChatGPT-based analysis. We analyze a dataset of 500 all-inclusive hotel reviews, comparing these methods to assess ChatGPT's effectiveness in identifying nuanced language and handling subjectivity. The findings reveal a high similarity between ChatGPT and human analysis, showcasing ChatGPT’s ability to interpret complex sentiments and automate sentiment classification tasks. This study highlights the potential of LLMs in transforming customer feedback analysis, providing deeper insights, and improving responsiveness in the hospitality industry. These results contribute to academia by presenting a framework for using LLMs in ABSA and guiding future applications and development.

---

Impulsando la sostenibilidad en las mipymes de América Latina y el Caribe: propuesta de una herramienta de autodiagnóstico ASG
10.18235/0004122
Si bien la crisis generada por el COVID-19 no ha hecho más que profundizar la vulnerabilidad de las micro, pequeñas y medianas empresas (mipymes), surgen condiciones para impulsar una recuperación económica sostenible, a través del acceso a un financiamiento productivo, complementado con asistencia técnica, que promueva inversiones que contribuyan a reducir tanto las brechas de género y de diversidad en las empresas, como la exposición a riesgos del cambio climático. La presente publicación constituye una guía para que los bancos públicos de desarrollo (BPD) y otras agencias de promoción del desarrollo puedan implementar una herramienta de autodiagnóstico para conocer, de forma preliminar, el nivel de sostenibilidad de las mipymes. A partir de la aplicación de esta herramienta, se espera lograr un doble beneficio: (i) para los BPD, contar con información inicial que les permita desarrollar y/o ajustar sus líneas de apoyo a las mipymes, tanto de crédito como de asistencia técnica; y (ii) para las mipymes, acceder a una herramienta de fácil aplicación que les posibilite identificar brechas y oportunidades en cuanto a la sostenibilidad de su modelo de negocio.

---

Earmarking Tax Policy on Local Taxation in Indonesia: Towards Pro Fiscal Legitimacy and Budget Flexibility Policy
10.18502/kss.v3i10.2922
<jats:p>.</jats:p>

---

Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis
10.18653/v1/2020.acl-main.293
The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification. Rather than considering the tasks separately, we build an end-to-end ABSA solution. Previous works in ABSA tasks did not fully leverage the importance of syntactical information. Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms. On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words. This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning. We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor. We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms. This increases the accuracy of the aspect sentiment classifier. Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.

---

Relational Graph Attention Network for Aspect-based Sentiment Analysis
10.18653/v1/2020.acl-main.295
Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words. However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections. In this paper, we address this problem by means of effective encoding of syntax information. Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree. Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction. Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.

---

IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP
10.18653/v1/2020.coling-main.66
Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.

---

LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding
10.18653/v1/2021.acl-long.201
Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672).

---

Towards Generative Aspect-Based Sentiment Analysis
10.18653/v1/2021.acl-short.64
Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discriminative manner, designing various task-specific classification networks for the prediction. Despite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each ABSA task as a text generation problem. We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design.

---

Open Aspect Target Sentiment Classification with Natural Language Prompts.
10.18653/v1/2021.emnlp-main.509
For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not trainable if annotated datasets are not available. Even with labeled data, they fall short of reaching satisfactory performance. To address this, we propose simple approaches that better solve ATSC with natural language prompts, enabling the task under zero-shot cases and enhancing supervised settings, especially for few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop domain, our method of reformulating ATSC as an NLI task outperforms supervised SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points. Moreover, we demonstrate that our prompts could handle implicitly stated aspects as well: our models reach about 77% accuracy on detecting sentiments for aspect categories (e.g., food), which do not necessarily appear within the text, even though we trained the models only with explicitly mentioned aspect terms (e.g., fajitas) from just 16 reviews - while the accuracy of the no-prompt baseline is only around 65%.

---

IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation
10.18653/v1/2021.emnlp-main.699
Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resource—yet widely spoken—languages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBART-large (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient learning and faster inference at very low-resource languages like Javanese and Sundanese.

---

Aspect Sentiment Quad Prediction as Paraphrase Generation
10.18653/v1/2021.emnlp-main.726
Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for a given opinionated sentence, which can reveal a more comprehensive and complete aspect-level sentiment structure. We further propose a novel Paraphrase modeling paradigm to cast the ASQP task to a paraphrase generation process. On one hand, the generation formulation allows solving ASQP in an end-to-end manner, alleviating the potential error propagation in the pipeline solution. On the other hand, the semantics of the sentiment elements can be fully exploited by learning to generate them in the natural language form. Extensive experiments on benchmark datasets show the superiority of our proposed method and the capacity of cross-task transfer with the proposed unified Paraphrase modeling framework.

---

IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization
10.18653/v1/2021.emnlp-main.833
We present IndoBERTweet, the first large-scale pretrained model for Indonesian Twitter that is trained by extending a monolingually-trained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark different ways of initializing the BERT embedding layer for new word types. We find that initializing with the average BERT subword embedding makes pretraining five times faster, and is more effective than proposed methods for vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based datasets.

---

The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task
10.18653/v1/2021.fever-1.1
The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) shared task, asks participating systems to determine whether human-authored claims are Supported or Refuted based on evidence retrieved from Wikipedia (or NotEnoughInfo if the claim cannot be verified). Compared to the FEVER 2018 shared task, the main challenge is the addition of structured data (tables and lists) as a source of evidence. The claims in the FEVEROUS dataset can be verified using only structured evidence, only unstructured evidence, or a mixture of both. Submissions are evaluated using the FEVEROUS score that combines label accuracy and evidence retrieval. Unlike FEVER 2018, FEVEROUS requires partial evidence to be returned for NotEnoughInfo claims, and the claims are longer and thus more complex. The shared task received 13 entries, six of which were able to beat the baseline system. The winning team was “Bust a move!”, achieving a FEVEROUS score of 27% (+9% compared to the baseline). In this paper we describe the shared task, present the full results and highlight commonalities and innovations among the participating systems.

---

FANG-COVID: A New Large-Scale Benchmark Dataset for Fake News Detection in German
10.18653/v1/2021.fever-1.9
As the world continues to fight the COVID-19 pandemic, it is simultaneously fighting an ‘infodemic’ – a flood of disinformation and spread of conspiracy theories leading to health threats and the division of society. To combat this infodemic, there is an urgent need for benchmark datasets that can help researchers develop and evaluate models geared towards automatic detection of disinformation. While there are increasing efforts to create adequate, open-source benchmark datasets for English, comparable resources are virtually unavailable for German, leaving research for the German language lagging significantly behind. In this paper, we introduce the new benchmark dataset FANG-COVID consisting of 28,056 real and 13,186 fake German news articles related to the COVID-19 pandemic as well as data on their propagation on Twitter. Furthermore, we propose an explainable textual- and social context-based model for fake news detection, compare its performance to “black-box” models and perform feature ablation to assess the relative importance of human-interpretable features in distinguishing fake news from authentic news.

---

Detecting Hallucinated Content in Conditional Neural Sequence Generation
10.18653/v1/2021.findings-acl.120
Neural sequence models can generate highly fluent sentences but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input, which can cause a lack of trust in the model. To better assess the faithfulness of the machine outputs, we propose a new task to predict whether each token in the output sequence is hallucinated conditioned on the source input, and collect new manually annotated evaluation sets for this task. We also introduce a novel method for learning to model hallucination detection, based on pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations. Experiments on machine translation and abstract text summarization demonstrate the effectiveness of our proposed approach -- we obtain an average F1 of around 0.6 across all the benchmark datasets. Furthermore, we demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in the low-resource machine translation and achieve significant improvements over strong baseline methods. We will also release our annotated data and code for future research.

---

GO FIGURE: A Meta Evaluation of Factuality in Summarization
10.18653/v1/2021.findings-acl.42
Text generation models can generate factually inconsistent text containing distorted or fabricated facts about the source text. Recent work has focused on building evaluation models to verify the factual correctness of semantically constrained text generation tasks such as document summarization. While the field of factuality evaluation is growing fast, we don't have well-defined criteria for measuring the effectiveness, generalizability, reliability, or sensitivity of the factuality metrics. Focusing on these aspects, in this paper, we introduce a meta-evaluation framework for evaluating factual consistency metrics. We introduce five necessary, common-sense conditions for effective factuality metrics and experiment with nine recent factuality metrics using synthetic and human-labeled factuality data from short news, long news and dialogue summarization domains. Our framework enables assessing the efficiency of any new factual consistency metric on a variety of dimensions over multiple summarization domains and can be easily extended with new meta-evaluation criteria. We also present our conclusions towards standardizing the factuality evaluation metrics.

---

Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble
10.18653/v1/2021.naacl-main.231
It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the graph and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis.

---

Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis
10.18653/v1/2022.acl-long.152
As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention in recent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodal alignment or (ii) use vision-language models pre-trained with general pre-training tasks, which are inadequate to identify finegrained aspects, opinions, and their alignments across modalities. To tackle these limitations, we propose a task-specific Vision-Language Pre-training framework for MABSA (VLPMABSA), which is a unified multimodal encoder-decoder architecture for all the pretraining and downstream tasks. We further design three types of task-specific pre-training tasks from the language, vision, and multimodal modalities, respectively. Experimental results show that our approach generally outperforms the state-of-the-art approaches on three MABSA subtasks. Further analysis demonstrates the effectiveness of each pretraining task. The source code is publicly released at https://github.com/NUSTM/VLP-MABSA.

---

Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity
10.18653/v1/2022.acl-long.556
When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.

---

Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion
10.18653/v1/2022.findings-acl.23
Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).

---

Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis
10.18653/v1/2022.findings-acl.285
Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we first take the Stack-BERT layers as a primary encoder to grasp the overall semantic of the sentence and then fine-tune it by incorporating a lightweight Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention to a small region of the sentences at each step and re-weigh the vitally important words for better aspect-aware sentiment understanding. Finally, experimental results on three benchmark datasets demonstrate the effectiveness and the rationality of our proposed model and provide good interpretable insights for future semantic modeling.

---

FinSim4-ESG Shared Task: Learning Semantic Similarities for the Financial Domain. Extended edition to ESG insights
10.18653/v1/2022.finnlp-1.28
This paper describes FinSim4-ESG 1 shared task organized in the 4th FinNLP workshopwhich is held in conjunction with the IJCAI-ECAI-2022 confer- enceThis year, the FinSim4 is extended to the Environment, Social and Government (ESG) insights and proposes two subtasks, one for ESG Taxonomy Enrichment and the other for Sustainable Sentence Prediction. Among the 28 teams registered to the shared task, a total of 8 teams submitted their systems results and 6 teams also submitted a paper to describe their method. The winner of each subtask shows good performance results of 0.85% and 0.95% in terms of accuracy, respectively.

---

TCS WITM 2022@FinSim4-ESG: Augmenting BERT with Linguistic and Semantic features for ESG data classification
10.18653/v1/2022.finnlp-1.32
Advanced neural network architectures have provided several opportunities to develop systems to automatically capture information from domain-specific unstructured text sources. The FinSim4-ESG shared task, collocated with the FinNLP workshop, proposed two sub-tasks. In sub-task1, the challenge was to design systems that could utilize contextual word embeddings along with sustainability resources to elaborate an ESG taxonomy. In the second sub-task, participants were asked to design a system that could classify sentences into sustainable or unsustainable sentences. In this paper, we utilize semantic similarity features along with BERT embeddings to segregate domain terms into a fixed number of class labels. The proposed model not only considers the contextual BERT embeddings but also incorporates Word2Vec, cosine, and Jaccard similarity which gives word-level importance to the model. For sentence classification, several linguistic elements along with BERT embeddings were used as classification features. We have shown a detailed ablation study for the proposed models.

---

Transformers-Based Approach for a Sustainability Term-Based Sentiment Analysis (STBSA)
10.18653/v1/2022.nlp4pi-1.19
Traditional sentiment analysis is a sentence level or document-level task. However, a sentence or paragraph may contain multiple target terms with different sentiments, making sentiment prediction more challenging. Although pre-trained language models like BERT have been successful, incorporating dynamic semantic changes into aspect-based sentiment models remains difficult, especially for domain-specific sentiment analysis. To this end, in this paper, we propose a Term-Based Sentiment Analysis (TBSA), a novel method designed to learn Environmental, Social, and Governance (ESG) contexts based on a sustainability taxonomy for ESG aspect-oriented sentiment analysis. Notably, we introduce a technique enhancing the ESG term’s attention, inspired by the success of attention-based neural networks in machine translation (Bahdanau et al., 2015) and Computer Vision (Bello et al., 2019). It enables the proposed model to focus on a small region of the sentences at each step and to reweigh the crucial terms for a better understanding of the ESG aspect-aware sentiment. Beyond the novelty in the model design, we propose a new dataset of 125,000+ ESG analyst annotated data points for sustainability term based sentiment classification, which derives from historical sustainability corpus data and expertise acquired by development finance institutions. Our extensive experiments combining the new method and the new dataset demonstrate the effectiveness of the Sustainability TBSA model with an accuracy of 91.30% (90% F1-score). Both internal and external business applications of our model show an evident potential for a significant positive impact toward furthering sustainable development goals (SDGs).

---

Raison d’être of the benchmark dataset: A Survey of Current Practices of Benchmark Dataset Sharing Platforms
10.18653/v1/2022.nlppower-1.1
This paper critically examines the current practices of benchmark dataset sharing in NLP and suggests a better way to inform reusers of the benchmark dataset. As the dataset sharing platform plays a key role not only in distributing the dataset but also in informing the potential reusers about the dataset, we believe data-sharing platforms should provide a comprehensive context of the datasets. We survey four benchmark dataset sharing platforms: HuggingFace, PaperswithCode, Tensorflow, and Pytorch to diagnose the current practices of how the dataset is shared which metadata is shared and omitted. To be specific, drawing on the concept of data curation which considers the future reuse when the data is made public, we advance the direction that benchmark dataset sharing platforms should take into consideration. We identify that four benchmark platforms have different practices of using metadata and there is a lack of consensus on what social impact metadata is. We believe the problem of missing a discussion around social impact in the dataset sharing platforms has to do with the failed agreement on who should be in charge. We propose that the benchmark dataset should develop social impact metadata and data curator should take a role in managing the social impact metadata.

---

Semantic Accuracy in Natural Language Generation: A Thesis Proposal
10.18653/v1/2023.acl-srw.48
We propose a thesis in which we explore how evaluation and interpretability techniques could lead to better natural language generation systems.

---

Fast and Accurate Factual Inconsistency Detection Over Long Documents
10.18653/v1/2023.emnlp-main.105
Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural Language Inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE's decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub.

---

Enhancing Information Retrieval in Fact Extraction and Verification
10.18653/v1/2023.fever-1.4
Modern fact verification systems have distanced themselves from the black box paradigm by providing the evidence used to infer their veracity judgments. Hence, evidence-backed fact verification systems’ performance heavily depends on the capabilities of their retrieval component to identify these facts. A popular evaluation benchmark for these systems is the FEVER task, which consists of determining the veracity of short claims using sentences extracted from Wikipedia. In this paper, we present a novel approach to the the retrieval steps of the FEVER task leveraging the graph structure of Wikipedia. The retrieval models surpass state of the art results at both sentence and document level. Additionally, we show that by feeding our retrieved evidence to the best-performing textual entailment model, we set a new state of the art in the FEVER competition.

---

Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models
10.18653/v1/2023.findings-emnlp.193
Recently, aspect-based sentiment analysis (ABSA) models have yielded promising results. However, they are susceptible to learning spurious correlations between certain words of the input text and output labels while modeling the sentiment feature of the aspect. This spurious correlation will potentially undermine the performance of ABSA models. One direct so-lution for this problem is to make the model see and learn an explanation of sentiment expression rather than certain words. Motivated by this, we exploit explanations for the sentiment polarity of each aspect from large language models (LLMs) to reduce spurious correlations in ABSA. First, we formulate a prompt template that wraps the sentence, an aspect, and the sentiment label. This template is utilized to prompt LLMs to generate an appropriate explanation that states the sentiment cause. Then, we propose two straightforward yet effective meth-ods to leverage the explanation for preventing the learning of spurious correlations. We conducted extensive comparative experiments on five datasets by integrating them with some representative ABSA models. Results show that our methods can achieve performance gains and enhance the performance and generalization ability of ABSA models.

---

A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection
10.18653/v1/2023.findings-emnlp.256
Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costing fewer tokens and less time. Furthermore, we manually analyze some hallucination cases that LLM failed to capture, revealing the shared limitation of zero-resource methods.

---

Lazybob at SemEval-2023 Task 9: Quantifying Intimacy of Multilingual Tweets with Multi-Task Learning
10.18653/v1/2023.semeval-1.128
This study presents a systematic method for analyzing the level of intimacy in tweets across ten different languages, using multi-task learning for SemEval 2023 Task 9: Multilingual Tweet Intimacy Analysis. The system begins with the utilization of the official training data, and then we experiment with different fine-tuning tricks and effective strategies, such as data augmentation, multi-task learning, etc. Through additional experiments, the approach is shown to be effective for the task. To enhance the model’s robustness, different transformer-based language models and some widely-used plug-and-play priors are incorporated into our system. Our final submission achieved a Pearson R of 0.6160 for the intimacy score on the official test set, placing us at the top of the leader board among 45 teams.

---

A Dataset for Explainable Sentiment Analysis in the German Automotive Industry
10.18653/v1/2023.wassa-1.13
While deep learning models have greatly improved the performance of many tasks related to sentiment analysis and classification, they are often criticized for being untrustworthy due to their black-box nature. As a result, numerous explainability techniques have been proposed to better understand the model predictions and to improve the deep learning models. In this work, we introduce InfoBarometer, the first benchmark for examining interpretable methods related to sentiment analysis in the German automotive sector based on online news. Each news article in our dataset is annotated w.r.t. overall sentiment (i.e., positive, negative and neutral), the target of the sentiment (focusing on innovation-related topics such as e.g. electromobility) and the rationales, i.e., textual explanations for the sentiment label that can be leveraged during both training and evaluation.For this research, we compare different state-of-the-art approaches to perform sentiment analysis and observe that even models that perform very well in classification do not score high on explainability metrics like model plausibility and faithfulness. We calculated the polarity scores for the best method BERT and got an F-score of 73.6. Moreover, we evaluated different interpretability algorithms (LIME, SHAP, Integrated Gradients, Saliency) based on explicitly marked rationales by human annotators quantitatively and qualitatively.Our experiments demonstrate that the textual explanations often do not agree with human interpretations, and rarely help to justify the models decision. However, local and global features provide useful insights to help uncover spurious features in the model and biases within the dataset. We intend to make our dataset public for other researchers

---

Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis
10.18653/v1/2023.wassa-1.3
Siddharth Varia, Shuai Wang, Kishaloy Halder, Robert Vacareanu, Miguel Ballesteros, Yassine Benajiba, Neha Anna John, Rishita Anubhai, Smaranda Muresan, Dan Roth. Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis. 2023.

---

LLaMA-Based Models for Aspect-Based Sentiment Analysis
10.18653/v1/2024.wassa-1.6
While large language models (LLMs) show promise for various tasks, their performance in compound aspect-based sentiment analysis (ABSA) tasks lags behind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA remains unexplored. This paper examines the capabilities of open-source LLMs fine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the performance across four tasks and eight English datasets, finding that the fine-tuned Orca 2 model surpasses state-of-the-art results in all tasks. However, all models struggle in zero-shot and few-shot scenarios compared to fully fine-tuned ones. Additionally, we conduct error analysis to identify challenges faced by fine-tuned models.

---

LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation
10.18653/v1/2025.acl-long.41
Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed sentiment analysis in a target language by transferring knowledge from a source language with available annotated data. Most existing methods depend heavily on often unreliable translation tools to bridge the language gap. In this paper, we propose a new approach that leverages a large language model (LLM) to generate high-quality pseudo-labelled data in the target language without the need for translation tools. First, the framework trains an ABSA model to obtain predictions for unlabelled target language data. Next, LLM is prompted to generate natural sentences that better represent these noisy predictions than the original text. The ABSA model is then further fine-tuned on the resulting pseudo-labelled dataset. We demonstrate the effectiveness of this method across six languages and five backbone models, surpassing previous state-of-the-art translation-based approaches. The proposed framework also supports generative models, and we show that fine-tuned LLMs outperform smaller multilingual models.

---

Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis
10.18653/v1/2025.acl-short.48
Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time.

---

Single Ground Truth Is Not Enough: Adding Flexibility to Aspect-Based Sentiment Analysis Evaluation
10.18653/v1/2025.naacl-long.603
Aspect-based sentiment analysis (ABSA) is a challenging task of extracting sentiments along with their corresponding aspects and opinion terms from the text. The inherent subjectivity of span annotation makes variability in the surface forms of extracted terms, complicating the evaluation process. Traditional evaluation methods often constrain ground truths (GT) to a single term, potentially misrepresenting the accuracy of semantically valid predictions that differ in surface form. To address this limitation, we propose a novel and fully automated pipeline that expands existing evaluation sets by adding alternative valid terms for aspect and opinion. Our approach facilitates an equitable assessment of language models by accommodating multiple-answer candidates, resulting in enhanced human agreement compared to single-answer test sets (achieving up to a 10\%p improvement in Kendall's Tau score). Experimental results demonstrate that our expanded evaluation set helps uncover the capabilities of large language models (LLMs) in ABSA tasks, which is concealed by the single-answer GT sets. Consequently, our work contributes to the development of a flexible evaluation framework for ABSA by embracing diverse surface forms to span extraction tasks in a cost-effective and reproducible manner. Our code and dataset is open at https://github.com/dudrrm/zoom-in-n-out-absa.

---

Attention-based LSTM for Aspect-level Sentiment Classification
10.18653/v1/d16-1058
Aspect-level sentiment classification is a fine-grained task in sentiment analysis. Since it provides more complete and in-depth results, aspect-level sentiment analysis has received much attention these years. In this paper, we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect. For instance, “The appetizers are ok, but the service is slow.”, for aspect taste, the polarity is positive while for service, the polarity is negative. Therefore, it is worthwhile to explore the connection between an aspect and the content of a sentence. To this end, we propose an Attention-based Long Short-Term Memory Network for aspect-level sentiment classification. The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input. We experiment on the SemEval 2014 dataset and results show that our model achieves state-ofthe-art performance on aspect-level sentiment classification.

---

A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis
10.18653/v1/d16-1103
Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources.

---

Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks
10.18653/v1/d19-1464
Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.

---

A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis
10.18653/v1/d19-1654
Aspect-based sentiment analysis (ABSA) has attracted increasing attention recently due to its broad applications. In existing ABSA datasets, most sentences contain only one aspect or multiple aspects with the same sentiment polarity, which makes ABSA task degenerate to sentence-level sentiment analysis. In this paper, we present a new large-scale Multi-Aspect Multi-Sentiment (MAMS) dataset, in which each sentence contains at least two different aspects with different sentiment polarities. The release of this dataset would push forward the research in this field. In addition, we propose simple yet effective CapsNet and CapsNet-BERT models which combine the strengths of recent NLP advances. Experiments on our new dataset show that the proposed model significantly outperforms the state-of-the-art baseline methods

---

Hierarchical Attention Networks for Document Classification
10.18653/v1/n16-1174
We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.

---

FEVER: a large-scale dataset for Fact Extraction and VERification
10.18653/v1/n18-1074
In this paper we introduce a new publicly\\r\\navailable dataset for verification against\\r\\ntextual sources, FEVER: Fact Extraction\\r\\nand VERification. It consists of 185,445\\r\\nclaims generated by altering sentences extracted\\r\\nfrom Wikipedia and subsequently\\r\\nverified without knowledge of the sentence\\r\\nthey were derived from. The\\r\\nclaims are classified as SUPPORTED, REFUTED\\r\\nor NOTENOUGHINFO by annotators\\r\\nachieving 0.6841 in Fleiss κ. For\\r\\nthe first two classes, the annotators also\\r\\nrecorded the sentence(s) forming the necessary\\r\\nevidence for their judgment. To\\r\\ncharacterize the challenge of the dataset\\r\\npresented, we develop a pipeline approach\\r\\nand compare it to suitably designed oracles.\\r\\nThe best accuracy we achieve on labeling\\r\\na claim accompanied by the correct\\r\\nevidence is 31.87%, while if we ignore the\\r\\nevidence we achieve 50.91%. Thus we believe\\r\\nthat FEVER is a challenging testbed\\r\\nthat will help stimulate progress on claim\\r\\nverification against textual sources.

---

BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis
10.18653/v1/n19-1242
Question-answering plays an important role in e-commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making. Inspired by the recent success of machine reading comprehension (MRC) on formal documents, this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions. We call this problem Review Reading Comprehension (RRC). To the best of our knowledge, no existing work has been done on RRC. In this work, we first build an RRC dataset called ReviewRC based on a popular benchmark for aspect-based sentiment analysis. Since ReviewRC has limited training examples for RRC (and also for aspect-based sentiment analysis), we then explore a novel post-training approach on the popular language model BERT to enhance the performance of fine-tuning of BERT for RRC. To show the generality of the approach, the proposed post-training is also applied to some other review-based tasks such as aspect extraction and aspect sentiment classification in aspect-based sentiment analysis. Experimental results demonstrate that the proposed post-training is highly effective.

---

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
10.18653/v1/n19-1423
We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

---

Neural Models for Documents with Metadata
10.18653/v1/p18-1189
Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.

---

Aspect Based Sentiment Analysis with Gated Convolutional Networks
10.18653/v1/p18-1234
Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.

---

SemEval-2015 Task 12: Aspect Based Sentiment Analysis
10.18653/v1/s15-2082
SemEval-2015 Task 12, a continuation of SemEval-2014 Task 4, aimed to foster research beyond sentenceor text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is to identify opinions expressed about specific entities (e.g., laptops) and their aspects (e.g., price). The task provided manually annotated reviews in three domains (restaurants, laptops and hotels), and a common evaluation procedure. It attracted 93 submissions from 16 teams.

---

SemEval-2016 Task 5: Aspect Based Sentiment Analysis
10.18653/v1/s16-1002
This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams.

---

Towards making NLG a voice for interpretable Machine Learning
10.18653/v1/w18-6522
This paper presents a study to understand the issues related to using NLG to humanise explanations from a popular interpretable machine learning framework called LIME. Our study shows that self-reported rating of NLG explanation was higher than that for a non-NLG explanation. However, when tested for comprehension, the results were not as clear-cut showing the need for performing more studies to uncover the factors responsible for high-quality NLG explanations.

---

Decision Making, Artificial Intelligence, and ESG Regulations
10.18690/um.epf.5.2025.65
The growing incorporation of Artificial Intelligence (AI) into environmental, social, and governance (ESG) practices, alongside non-financial reporting, is reshaping decision-making processes within organizations. This research investigates the multifaceted impact of AI on ESG decision making, exploring how AI-driven tools and analytics support strategic decisions pertaining to the implementation of the CSRD Directive. The research method is the analysis of the content of documents and their examination in terms of determining the basic areas of decision-making, identifying the risks associated with these decisions, and developing a methodology for using AI in these decision-making processes. The analysis concludes that although AI can provide substantial support in executing the CSRD directive, it is incapable of substituting a knowledgeable team familiar with the company’s specific environment. The research did not include any empirical methods that would allow one to understand the practical possibilities of using AI in decision-making processes related to the implementation of the CSRD directive by companies. The study exposes the significant scope for exploration in the practical applications of AI, serving as a potential foundation for advancing decision-making processes within contemporary corporate management.

---

The Impact of External Integration and Internal Integration to Product Innovation and Competitive Advantage on Small and Medium Enterprises (SMEs)
10.18775/ijied.1849-7551-7020.2015.64.2006
This study aims to analyze the influence of external integration and internal integration to product innovation and competitive advantage. The research was conducted on a sample of 180 manager small and medium enterprises (SMEs) in East Java clothing taken with stratified cluster sampling technique. Selection of sample areas based on areas that have the potential development of the industry. Data collection was done by using questionnaire that has been tested for its validity and reliability. Data analysis used Structural Equation Modeling (SEM) analysis with help of AMOS version 21 program. Based on the analysis of SEM, it was found that the first, external integration influence on product innovation SMEs and affect the competitive advantage of SMEs. Second, the internal integration effect on product innovation SMEs and affect the competitive advantage of SMEs. Third, product innovation SMEs effect on the competitive advantage of SMEs. In this respect, the company needs to build a collaboration of external integration and internal integration in order to improve product innovation and competitive advantage of SMEs.

---

From Reporting to Responsibility: Legal Innovations in Corporate Law and Governance under the examples of the EU's “Corporate Sustainability Reporting Directive (CSRD)”
10.19044/esipreprint.8.2025.p306
The topic of this research is as practical as it is theoretical and
cognitive. It is based on the example of the EU's Corporate Sustainability
Reporting Directive (CSRD). The practical relevance of the research issue is
considered in terms of global environmental, social, labour and governance
issues, as well as regulatory issues, which have affected the entire EU and
given rise to the need for a mechanism to protect corporations from
inefficient outcomes and create a more transparent, accountable and
sustainable corporate environment within the EU. In the modern digital era,
companies have started to take action for sustainability after facing the
failures of corporations to make efficient reporting efforts, which leads to
poor risk management, increased costs and decreased innovation. However,
it is obvious that the global community has not created the mechanisms that
would vitally promote sustainable economic development over the last
decade. In the paper Principles of Political Economy by the renowned
English philosopher John Stuart Mill, we read that 'the most cogent reason
for establishing a rule of conduct is that it promotes general happiness; it has
been found to do so by experience, and that constitutes its title to be
respected as a rule'. In order to improve transparency and accountability
within companies, promote sustainable business practices, support informed decision-making and contribute to the EU's Green Deal, the EU consolidated
the directive, the meaning of which is considered within the context of recent
history (Guerman, 2021). The Volkswagen emissions scandal (also known as
'Dieselgate', 2015) revealed the problem of environmental degradation when
it was discovered that the company had installed software in its diesel
vehicles to cheat emissions tests. This allowed the cars to emit nitrogen
oxides at levels up to 40 times higher than the legal limit. The scandal has
highlighted the need for greater transparency and accountability in corporate
environmental practices. Similarly, Amazon has faced ongoing criticism for
its labour practices, including reports of harsh working conditions,
inadequate breaks and high injury rates in its warehouses (Guerman, 2021).
Repeated investigations and media reports have brought these issues to light,
demonstrating the need for greater transparency and accountability in how
companies treat their employees. However, the legal process is ongoing. In
2018, Facebook faced intense scrutiny following the Cambridge Analytica
scandal, in which the personal data of millions of users was harvested
without consent and used for political advertising. The lack of transparency
in data handling practices and inadequate accountability measures were
widely discussed, highlighting the need for more robust reporting and user
privacy protection. These examples illustrate the diverse range of problems
that corporations have faced, demonstrating the urgent need for the EU's
Corporate Sustainability Reporting Directive. The dilemmas between
'reporting' and 'responsibilities' are evident in the market, and the legal and
economic analysis of innovations in the corporate sustainability process is a
fascinating area of research.

---

NLP-Based Quantification of ESG in Sustainability Reports and Firm-Specific Risk: Evidence from Borsa İstanbul
10.20409/berj.2025.475
<jats:p xml:lang="tr"/>

---

ACCESS TO FINANCE CONSTRAINTS IN ADOPTING LATEST TECHNOLOGIES FOR BUSINESS PRODUCTION IN SMALL- AND MEDIUM-SIZED ENTERPRISES (SMES)
10.20472/iac.2019.045.019
The small and medium-sized enterprises (SMEs) play crucial role in supporting the economies of developing countries. However, with globalization and open market trends, SMEs need to compete with new entrants to maintain their market share and growth. The degree of technology adaptation and level of process sophistication help the SMEs to achieve their targeted level of growth through improvement of business effectiveness and efficiency. Access to finance is a critical success factor in such endeavor and supports the SMEs to alleviate growth constraints. This paper presents research data on the degree that access to finance impacts the abilities of SMEs to grow through improvement of process sophistication for enhanced business production. The raw data was gathered from over 400 firms independently using one unified assessment tool over a period of five years. This data is then analyzed using inferential statistics through one-way analysis of variance (ANOVA) to evaluate and compare the average scores associated with the research variables. Furthermore, the post hoc analysis is applied to assess the extent and direction of variation in the scores and moderated across different years. The results showed that the adopted level of process sophistication is positively correlated with the availability of technology with limited role played by access to finance. These relationships are discussed and analyzed in the context of local market growth, sophistication, financial capabilities, and financial facilities. The research aims to assist SME stakeholders and governing bodies in recognizing the impact of changes on macroeconomic scale from SMEs growth and sustainability perspectives.

---

Aspect-based Sentiment and Correlation-based Emotion Detection on Tweets for Understanding Public Opinion of Covid-19
10.20473/jisebi.9.1.84-94
Background: During the Covid-19 period, the government made policies dealing with it. Policies issued by the government invited public opinion as a form of public reaction to these policies. The easiest way to find out the public’s response is through Twitter’s social media. However, Twitter data have limitations. There is a mix between facts and personal opinions. It is necessary to distinguish between these. Opinions expressed by the public can be both positive and negative, so correlation is needed to link opinions and their emotions.
Objective: This study discusses sentiment and emotion detection to understand public opinion accurately. Sentiment and emotion are analyzed using Pearson correlation to determine the correlation.
Methods: The datasets were about public opinion of Covid-19 retrieved from Twitter. The data were annotated into sentiment and emotion using Pearson correlation. After the annotation process, the data were preprocessed. Afterward, single model classification was carried out using machine learning methods (Support Vector Machine, Random Forest, Naïve Bayes) and deep learning method (Bidirectional Encoder Representation from Transformers). The classification process was focused on accuracy and F1-score evaluation.
Results: There were three scenarios for determining sentiment and emotion, namely the factor of aspect-based and correlation-based, without those factors, and aspect-based sentiment only. The scenario using the two aforementioned factors obtained an accuracy value of 97%, while an accuracy of 96% was acquired without them.
Conclusion: The use of aspect and correlation with Pearson correlation has helped better understand public opinion regarding sentiment and emotion more accurately.
 
Keywords: Aspect-based sentiment, Deep learning, Emotion detection, Machine learning, Pearson correlation, Public opinion.

---

Decoding Fan and Societal Sentiment: ABSA of The Saudi Pro League’s Recent Evolution
10.20885/snati.v4.i1.6
The Saudi Professional League (SPL) has attained global recognition through its recruitment of high-profile international players, yet this rise has intensified public scrutiny regarding incidents involving these athletes, such as Controversies surrounding sportsmanship, provocative celebrations, verbal altercations with spectators. This study analyzes (4,884) Arabic-language posts from (2021 to 2024), employing Aspect-Based Sentiment Analysis (ABSA) and the fine-tuned MARBERT model. The findings reveal a dominant negative sentiment (71.9%) across the dataset, with 'Player-Conduct' and 'Disciplinary-Action' emerging as the most frequently discussed aspects. Co-occurrence and correlation analyses indicate that negative sentiment is closely tied to perceptions of inadequate governance and cultural misalignment within the SPL, further intensifying public dissatisfaction. This research underscores the duality of high-profile players as drivers of global visibility and sources of domestic tension, particularly within culturally sensitive contexts. By addressing these challenges, the SPL can mitigate reputational risks while harmonizing its international ambitions with domestic expectations. This study advances Arabic Aspect-based sentiment analysis in the sports domain and provides actionable insights to enhance ethical governance, align with cultural sensitivities, and strengthen stakeholder engagement, thereby supporting the SPL’s long-term credibility and growth.

---

Leveraging Long-Context Large Language Models for Multi-Document Understanding and Summarization in Enterprise Applications
10.20944/preprints202409.1734.v1
The rapid increase in unstructured data across various fields has made multi-document comprehension and summarization a critical task. Traditional approaches often fail to capture relevant context, maintain logical consistency, and extract essential information from lengthy documents. This paper explores the use of Long-context Large Language Models (LLMs) for multi-document summarization, demonstrating their exceptional capacity to grasp extensive connections, provide cohesive summaries, and adapt to various industry domains and integration with enterprise applications/systems. The paper discusses the workflow of multi-document summarization for effectively deploying long-context LLMs, supported by case studies in legal applications, enterprise functions such as HR, finance, and sourcing, as well as in the medical and news domains. These case studies show notable enhancements in both efficiency and accuracy. Technical obstacles, such as dataset diversity, model scalability, and ethical considerations like bias mitigation and factual accuracy, are carefully analyzed. Prospective research avenues are suggested to augment the functionalities and applications of long-context LLMs, establishing them as pivotal tools for transforming information processing across diverse sectors and enterprise applications.

---

Novel Data Mining Methodologies for Environmental, Social and Governance Analytics: A Comprehensive Framework for Sustainable Investment
10.20944/preprints202501.0620.v1
This paper presents a systematic analysis of novel data mining applications in Environmental, Social, and Governance (ESG) assessment, addressing the growing complexity of sustainable investment decisions. Through empirical examination of machine learning methodologies, including deep learning architectures and natural language processing, we demonstrate enhanced capabilities in processing unstructured ESG data and identifying latent patterns in corporate sustainability metrics. Our research establishes a comprehensive framework for integrating diverse analytical techniques, achieving 85% accuracy in governance anomaly detection and significant improvements in environmental risk assessment through hierarchical clustering. The study reveals substantial correlations between ESG performance and financial outcomes, whilst identifying critical challenges in data standardisation and algorithmic bias mitigation. The findings contribute to both theoretical understanding and practical implementation of data-driven ESG analysis, offering valuable insights for investment professionals and corporate stakeholders. This research advances the field of sustainable finance analytics through innovative methodological approaches to ESG assessment.

---

The Impact of AI-Integrated ESG Reporting on Firm Valuation in Emerging Markets: A Multimodal Analytical Approach
10.20944/preprints202509.0814.v1
This study examines the impact of Artificial Intelligence (AI)-enhanced Environmental, Social, and Governance (ESG) reporting on firm valuation in emerging markets. It aims to explore how AI integration enhances the interpretability and predictive accuracy of ESG metrics in shaping market perceptions and investor decisions, particularly in non-financial sectors where ESG performance is experiencing significant growth. This study employs a panel dataset from 2018 to 2024, and it analyses publicly listed non-financial firms across five major sectors: manufacturing, energy, telecommunica-tions, consumer goods, and industrials. This study employs AI-powered multimodal analysis to examine the relationships between ESG and firm valuation in emerging markets. The research combines Fixed-Effects Regression and Machine Learning (ML) algorithms such as Extreme Gradient Boosting (XGBoost) and Random Forest to identify both linear and non-linear relationships between ESG scores and firm valuation. The results show empirical evidence that integrating ML enhances the explanatory power of ESG data. Findings indicate that ESG performance is positively correlated with higher market valuations, particularly in Environmental and Social dimensions. Governance metrics show more inconsistent effects. Firms identified in ESG controversies tend to face valuation penalties, which stresses market sensitivity to reputational risks. ML algo-rithms outperform conventional techniques in predictive accuracy, revealing complex, non-linear interactions within ESG data. The findings contribute to both academic lit-erature and practitioner understanding of the implications of how AI-driven ESG re-porting can improve robust firm valuation models and complex interdependencies in the ESG dataset.

---

Explainable Multi-Hop Question Answering: A Rationale-Based Approach
10.20944/preprints202509.1957.v1
Multi-hop question answering tasks involve identifying relevant supporting sentences from a given set of documents, which serve as the rationale for deriving answers. Most research in this area consists of two main components: a rationale identification module and a reader module. Since the rationale identification module often relies on retrieval models or supervised learning, annotated rationales are typically essential. However, approaches that rely on annotations face challenges when adapting to open-domain settings. Moreover, when models are trained on annotated rationales, explainable artificial intelligence (XAI) requires clear explanations of how these rationales are derived. Therefore, traditional multi-hop question answering approaches that depend on annotated rationales are unsuitable for XAI, which demands transparency in the model’s reasoning process. To address this issue, we propose a rationale reasoning framework that can effectively infer rationales and demonstrate the model’s reasoning process, even in open-domain environments without annotations. The proposed model is applicable to various tasks without structural constraints, and experimental results demonstrate its significantly improved rationale reasoning capabilities in multi-hop question answering, relation extraction, and sentence classification tasks.

---

An Interpretable Artificial Intelligence Approach for Reliability and Regulation-Aware Decision Support in Power Systems
10.20944/preprints202511.0860.v1
Modern medium-voltage (MV) distribution networks face increasing reliability challenges driven by aging assets, climate variability, and evolving operational demands. In Colombia and across Latin America, reliability metrics such as the system average interruption frequency index (SAIFI), standardized under IEEE 1366, serve as key indicator for regulatory compliance and service quality. However, existing analytical approaches struggle to jointly deliver predictive accuracy, interpretability, and traceability required for regulated environments. Here, we introduces CRITAIR (Criticality Analysis through Interpretable Artificial Intelligence-based Recommendations), an integrated framework that combines predictive modeling, explainable analytics, and regulation-aware reasoning to enhance reliability management in MV networks. CRITAIR unifies three components: (i) a TabNet-based predictive module that estimates SAIFI using outage, asset, and meteorological data while producing global and local attributions; (ii) an agentic retrieval-and-reasoning layer that grounds recommendations in regulatory evidence from RETIE and NTC 2050; and (iii) interpretable reasoning graphs that map decision pathways for full auditability. Evaluations conducted on real operational data demonstrate that CRITAIR achieves competitive predictive performance—comparable to Random Forest and XGBoost—while maintaining transparency through sparse attention and sequential feature explainability. Also, our regulation-aware reasoning module exhibits coherent and verifiable recommendations, achieving high semantic alignment scores (BERTScore) and expert-rated interpretability. Overall, CRITAIR bridges the gap between predictive analytics and regulatory governance, offering a transparent, auditable, and deployment-ready solution for digital transformation in electric distribution systems.

---

Neurosymbolic AI for Safe and Trustworthy High-Stakes Applications
10.20944/preprints202511.1342.v1
Artificial intelligence is increasingly deployed in high-stakes domains such as healthcare, public welfare, and autonomous transportation, where errors can cost lives or infringe on human rights. However, current AI approaches dominated by neural networks and generative models (e.g., large language models) have well-documented shortcomings: they can hallucinate false information, exhibit bias, and lack explainability. This paper argues that these limitations make purely neural AI insufficient for safety-critical applications like medical diagnostics (where misdiagnosis or unsafe advice can be deadly), public welfare decision-making (where biased algorithms have unfairly denied benefits or targeted vulnerable groups), and autonomous systems (where failures can result in fatal accidents). We then introduce neurosymbolic AI – a hybrid paradigm combining data-driven neural networks with rule-based symbolic reasoning – as a viable path toward trustworthy AI. By integrating neural perception with symbolic knowledge and logic, neurosymbolic systems can provide built-in safety guardrails, robust reasoning abilities, and transparent decision traces. We survey evidence that neurosymbolic architectures can mitigate hallucinations and bias by enforcing domain constraints (e.g. medical guidelines or legal rules), while also enhancing explainability and accountability through explicit reasoning steps. Through examples and literature (including the IEEE's “Neurosymbolic Artificial Intelligence: Why, What, and How”), we illustrate how neurosymbolic AI can bridge the gap between the accuracy of neural methods and the reliability required in life-critical environments. Diagrams comparing architectures and error mitigation strategies are provided to visualize how the neurosymbolic approach improves safety.

---

Multi-label emotion classification of Tweets with transformer models
10.21203/rs.3.rs-2583392/v1
Abstract Analysis and classification of emotions expressed in social media content such as tweets have been useful for numerous commercial and social purposes for tasks like hate speech detection. Emotion classification of social media content has been performed using traditional techniques such as Recurrent Neural Networks (RNN) and Multivariate Long Short Term Memory (LSTM) in the past, which can be outperformed by the new transformer models. The ‘SemEval-2018 Task 1: Affect in Tweets’ (Mohommad et al. 2018) presents a challenge on multi-label classification of emotions expressed in tweets into 11 sentiment classes. The datasets given for this challenge are used in this work to explore the accuracy of the transformer models against other techniques used by the competitors of the particular challenge. Additionally the transformer models (BERT, RoBERTa and XLM RoBERTa) were compared with each other on their performance based on accuracy and speed. The best performing BERT-large model which is trained using bert-large-uncased tokenizer has shown a multi-label accuracy (Jaccard Index) which is higher than the sixth recorded score in the SemEval-2018 Task 1 competition, F1-micro which is higher than the best F1 score recorded.

---

Document-Level Relation Extraction based Knowledge Graph Construction for Industrial Domain
10.21203/rs.3.rs-2589475/v2
Abstract The full text of this preprint has been withdrawn by the authors while they make corrections to the work. Therefore, the authors do not wish this work to be cited as a reference. Questions should be directed to the corresponding author.

---

Environmental, Social, and Governance (ESG) and Artificial Intelligence in Finance: State-of-the-Art and Research Takeaways
10.21203/rs.3.rs-2849051/v1
Abstract The rapidly growing research landscape in finance, encompassing environmental, social, and governance (ESG) topics and associated Artificial Intelligence (AI) applications, presents challenges for both new researchers and seasoned practitioners. This study aims to systematically map the research area, identify knowledge gaps, and examine potential research areas for researchers and practitioners. The investigation centers around three research questions: key research themes for ESG and AI in finance, research intensity and interest evolution, and the use and progression of AI techniques within these themes. Eight archetypical research domains were identified: (i) Trading and Investment, (ii) ESG Disclosure, Measurement and Governance, (iii) Firm Governance, (iv) Financial Markets and Instruments, (v) Risk Management, (vi) Forecasting and Valuation, (vii) Data, and (viii) Responsible Use of AI. Distinctive AI techniques were found to be employed across these archetypes. The study contributes to consolidating knowledge on the intersection of ESG, AI, and finance, offering an ontological inquiry and key takeaways for practitioners and researchers. Important insights include the popularity and crowding of the Trading and Investment domain, the growth potential of the Data archetype, and the high potential of Responsible Use of AI, despite its low publication count. By understanding the nuances of different research archetypes, researchers and practitioners can better navigate this complex landscape and contribute to a more sustainable and responsible financial sector.

---

Use of Deep Learning for Full-text Data Elements Extraction for Systematic Literature Review Tasks
10.21203/rs.3.rs-4426541/v1
<title>Abstract</title> <bold>Background:</bold> Systematic literature review (SLR) is an important tool for Health Economics and Outcomes Research (HEOR) evidence synthesis. SLRs involve the identification and selection of pertinent publications and extraction of relevant data elements from full-text articles, which can be a manually intensive procedure. Previously we developed machine learning models to automatically identify relevant publications based on pre-specified inclusion and exclusion criteria. <bold>Objective:</bold> This study investigates the feasibility of applying Natural Language Processing (NLP) approaches to automatically extract data elements from the relevant scientific literature. <bold>Methods:</bold> First, 239 full-text articles were collected and annotated for 12 important variables including study cohort, lab technique, and disease type, for proper SLR summary of Human papillomavirus (HPV) Prevalence, Pneumococcal Epidemiology, and Pneumococcal Economic Burden. The three resulting annotated corpora are shared publicly at [<underline>https://github.com/Merck/NLP-SLR-corpora</underline>], to provide training data and a benchmark baseline for the NLP community to further research this challenging task. We then compared three start-of-the-art Named Entity Recognition (NER) algorithms, namely Conditional Random Fields (CRF), Long Short-Term Memory (LSTM), and the Bidirectional Encoder Representations from Transformers (BERT) models, to assess performance on the data element extraction task. <bold>Results:</bold> The annotation corpora contain 4,498, 579, and 252 annotated entity mentions for HPV Prevalence, Pneumococcal Epidemiology, and Pneumococcal Economic Burden tasks respectively. Deep learning algorithms achieved superior performance in recognizing the targeted SLR data elements, compared to conventional machine learning algorithms. LSTM models have achieved 0.890, 0.646 and 0.615 micro-averaged F1 scores for three tasks respectively. CRF models could not provide comparable performance on most of the elements of interest. Although BERT-based models are known to generally achieve superior performance on many NLP tasks, we did not observe improvement in our three tasks. Considering the much lower requirements for computation resources, LSTM models will be preferable for deployment in our case. <bold>Conclusions:</bold> Deep learning-based NLP algorithms demonstrated their efficacy in the extraction of SLR data elements, which showed potential in expediting and automating SLR tasks.

---

Comparative Analysis On The Impact of GPT On Human Thinking Using Sentiment Analysis
10.21203/rs.3.rs-6803979/v1
<title>Abstract</title> This research article examines the effects of GPT (Generative Pre-trained Transformer) models on human thought processes and emotions, specifically looking at changes in sentiment and themes in user feedback. Using a multi-step approach that includes comparative sentiment evaluation, word-level sentiment examination, and thematic modelling, the study assesses how users’ views and cognitive articulations evolve before and after their interaction with GPT. Sentiment evaluations were performed via VADER and TextBlob to ensure thoroughness and validation of polarity and subjectivity ratings. The thematic analysis revealed shifting trends in trust, doubt, and hands-on engagement with AI-created material. Statistical analyses, such as Tukey’s HSD, were utilized to determine the relevance of sentiment differences among various user demographics, identifying significant variations linked to age. By combining sentiment trend observations, word co-occurrence networks, and comparisons of polarity and subjectivity scores, the research provides a detailed perspective to gauge the nuanced yet quantifiable impact of GPT on human cognition and emotional perspectives. These results enhance the overall comprehension of human-AI relationships and their significance for digital interaction, AI acceptance, and cognitive changes.

---

Explainable AI with Fine-Tuned Large Language Models for Sustainable Cultural Heritage Management: A Public Perception Analysis Approach
10.21203/rs.3.rs-7076430/v1
<title>Abstract</title> The redevelopment of cultural heritage areas, especially in historical urban environments, requires a nuanced understanding of public perceptions to balance preservation with modernization. While Aspect-Based Sentiment Analysis (ABSA) has proven valuable in capturing public emotions towards the built environment, it often overlooks implicit emotional cues and subtle sentiments. This study introduces an advanced framework for Aspect Sentiment Quadruple Prediction (ASQP), employing AI-driven techniques to assess public perceptions of Lijiang Ancient Town, a UNESCO World Heritage site in China. By fine-tuning large models using LoRA-based methods, this research enhances aspect- and emotion-related term recognition, integrating BERT, multi-layer BiLSTM, multi-head self-attention, Convolutional Neural Networks (CNN), and Conditional Random Fields (CRF) for entity recognition and sentiment classification. Interpretability is ensured through SHAP (SHapley Additive exPlanations) analysis. Additionally, implicit sentiment data augmentation using Large Language Models (LLMs) uncovers underlying emotional cues that are not overtly expressed in social media content. Analyzing data from platforms such as Weibo, Dazhong Dianping, and Xiaohongshu (2018–2024), the research uncovers key factors influencing public perception, offering actionable insights for heritage site management, urban planning, and the sustainable preservation of cultural heritage. This methodology provides broader implications for global heritage management and sustainable urban development.

---

Adversarial-Robust Deep Reinforcement Learningfor High-Frequency Cryptocurrency Trading withExplainable AI Framework
10.21203/rs.3.rs-8214644/v1
<title>Abstract</title> High-frequency trading (HFT) in cryptocurrency markets has increasingly adopted deep reinforcement learning (DRL) algorithms to capitalize on microsecond-level price move- ments and market inefficiencies. However, the susceptibility of DRL models to adversarial attacks poses significant security risks, potentially leading to substantial financial losses and market manipulation. This paper introduces a novel adversarial- robust DRL framework specifically designed for high-frequency cryptocurrency trading, integrated with explainable AI (XAI) mechanisms to ensure regulatory compliance and transparency. We propose a multi-scale adversarial training methodology that addresses unique cryptocurrency market microstructure char- acteristics, including 24/7 trading, extreme volatility, and frag- mented liquidity across exchanges. Our framework incorporates five distinct attack vectors—FGSM, PGD, C&amp;W, order book manipulation, and latency-based attacks—to comprehensively evaluate robustness. Defense mechanisms include adversarial training, defensive distillation, and a novel dynamic adapta- tion strategy that adjusts defenses based on real-time market conditions. The explainability component integrates SHAP for global feature importance and LIME for local decision in- terpretation, maintaining sub-10ms latency to meet HFT re- quirements. Experimental results on Bitcoin, Ethereum, and major altcoin datasets demonstrate that our adversarial-robust framework achieves 94.3% of baseline trading performance while successfully defending against 89.7% of adversarial attacks. The explainability framework provides transparent insights into trading decisions with 8.3ms average latency. This research con- tributes the first comprehensive adversarial defense framework for cryptocurrency HFT, advancing the state-of-the-art in secure and interpretable algorithmic trading systems.

---

The Value of Non-Financial Information in SME Risk Management
10.2139/ssrn.1320612
Within the commercial client segment, small business lending is gradually becoming a major target for many banks. The new Basel Capital Accord has helped the financial sector to recognize small and medium sized enterprises (SMEs) as a client, distinct from the large corporate. Some argue that this client base should be treated like retail clients from a risk management point of view in order to lower capital requirements and realize efficiency and profitability gains. In this context, it is increasingly important to develop appropriate risk models for this large and potentially even larger portion of bank assets. So far, none of the few studies that have focused on developing credit risk models specifically for SMEs have included qualitative information as predictors of the company credit worthiness. For the first time, in this study we have available non-financial and 'event' data to supplement the limited accounting data which are often available for non-listed firms. We employ a sample consisting of over 5.8 million sets of accounts of unlisted firms of which over 66,000 failed during the period 2000-2007. We find that qualitative data relating to such variables as legal action by creditors to recover unpaid debts, company filing histories, comprehensive audit report/opinion data and firm specific characteristics make a significant contribution to increasing the default prediction power of risk models built specifically for SMEs.

---

Corporate Governance Ratings and Corporate Performance: An Analysis of Governance Metrics International (GMI) ratings of US Firms, 2003 to 2008
10.2139/ssrn.1392313
Since 2003, Governance Metrics International (GMI), the corporate governance research and ratings firm, has produced both home market and global ratings for an increasing number of companies worldwide. GMI’s premise, as stated on its website, is that “companies that emphasize corporate governance and transparency will, over time, generate superior returns and economic performance and lower their cost of capital.” This paper is one of the first attempts to empirically evaluate whether GMI’s governance ratings have any economically significant relationship to future corporate performance. The analysis of GMI's US rated companies evaluates whether this additional source of information relating to the governance of corporations has any relevance for investor decision making and, as claimed by GMI, has the potential to increase shareholder welfare? Our results show that the GMI ratings are statistically significantly related to corporate characteristics, prior performance and to future returns. The results of a portfolio simulation analysis from 2003 to 2008 also shows that the lowly rated GMI portfolios achieved much lower realized returns (and sector-relative returns) than higher rated portfolios. These results strongly suggest that the GMI ratings may be of significant relevance/value for investor decision making.

---

Independent Assurance on Environmental, Social and Governance (ESG) Reporting – A Case Study of Hong Kong
10.2139/ssrn.3336656
This paper aims to study the reliability and credibility of information disclosed in the Environmental, Social and Governance (ESG) report. ESG reporting is mandatory for all listed companies in Hong Kong and other countries in recent years; however, independent assurance on ESG reports is voluntary and there are a small portion of companies engaged in independent assurance provider and therefore, most of the companies may not disclose the holistic picture of what they are actually performed, i.e. green-washing, especially when the results on environment and society is negative. This is a qualitative case study based on comparative case study analysis of other countries through macro-level, i.e. government authorities to micro-level, i.e. external assurance provider. First, theoretical concepts regarding determinants of assured ESG reporting will be studied. Second, current literature, journal articles, reports from Big 4 and other accounting firms and websites from stock exchange and other relevant agency etc. will be investigated. Finally, a conclusion and recommendation based on the findings in this study will be drawn.

---

Green Washing’ or ‘Authentic Effort’? An Empirical Investigation of the Quality of Sustainability Reporting by Banks
10.2139/ssrn.3715959
Purpose – This study explores the quality of sustainability reporting (QSR) and the impact of regulatory guidelines, social performance and a standardised reporting framework (using the Global Reporting Initiative [GRI] guidelines) on QSR in the context of banks in Bangladesh. 
 
Design/methodology/approach – Using a sample of 315 banking firm-year observations over 13 years (2002–2014), a content analysis technique is used to develop the 11-item QSR index. Regression analysis is used to test the research hypotheses. 
 
Findings – Initially, QSR evolved symbolically in Bangladesh’s banks but, over our investigation period, with QSR indicators gradually improving, the trends became substantive. The influences on QSR were sustainable banking practice regulatory guidelines, social performance and use of the GRI guidelines. However, until banks improve reporting information, such as external verification and trends over time, QSR cannot be regarded as fully substantive. 
 
Research limitations/implications – This study advances QSR research and debate among academic researchers. With regulatory agencies and stakeholders increasingly using sustainability reporting information for decision making, the information’s reliability is vital. 
 
Originality/value – This study is the first on QSR in the banking industry context, with previous research mostly investigating the quantity of sustainability reporting. The current study also synthesises QSR with sustainability regulation and social performance factors which have rarely been used in the sustainability literature. To gain a holistic understanding of QSR, existing QSR measures are advanced by combining external reporting efforts with banks’ internalisation initiatives.

---

Green Data or Greenwashing? Do Corporate Carbon Emissions Data Enable Investors to Mitigate Climate Change?
10.2139/ssrn.3722973
Absent mandatory reporting, and although many companies report their carbon emissions, much of the emissions data are estimated by data providers. As we evaluate the forward-looking carbon scores from several popular data providers, we find no evidence that these scores predict future changes in emissions. Further, we find that data on estimated emissions are at least 2.4 times less effective than reported data in identifying the worst emitters and provide little information to identify green companies in brown sectors. Our results debunk the belief that third-party estimated emissions are a satisfactory substitute for company-reported emissions and call for mandatory and audited carbon emissions disclosure.

---

Carbonwashing: A New Type of Carbon Data-Related ESG Greenwashing
10.2139/ssrn.3901278
Despite the increased attention and capital incentives around corporate sustainability, the development of sustainability reporting standards and monitoring systems has been progressing at a slow pace. As a result, companies have misaligned incentives to deliberately or selectively communicate information not matched with actual environmental impacts or make largely unsubstantiated promises around future ambitions. These incidents are broadly called “greenwashing,” but there is no clear consensus on its definition and taxonomy. We pay particular attention to the threat of greenwashing concerning carbon emission reductions by coining a new term, “carbonwashing.” Since carbon mitigation is the universal goal, the corporate carbon performance data supply chain is relatively more advanced than that of the entire sustainability data landscape. Nonetheless, the threat of carbonwashing persists, even far more severe than general greenwashing due to the financial values attached to corporate carbon performance. This paper contextualizes sustainable finance-related carbonwashing via an outline of the communication as well as the measurement, reporting, and verification (MRV) of carbon emission mitigation performance. Moreover, it proposes several actionable policy recommendations on how industry stakeholders and government regulators can reduce carbonwashing risks.

---

Misreporting of Mandatory ESG Disclosures: Evidence from Gender Pay Gap Information
10.2139/ssrn.4192257
We examine environmental, social, and governance (ESG) misreporting in the context of the UK government requirement to report gender employment ratios and pay gaps. Highlighting misreporting, many employers report a set of disclosures that are together mathematically impossible. Further, a disproportionate number of employers report perfectly balanced gender statistics, consistent with misreporting as a form of ESG-washing. Suggesting that a lack of ethical considerations encourages misreporting, employers involved in ESG controversies and who commit labor violations are more likely to misreport. Suggesting that capital market and media scrutiny discourage misreporting, employers that are the subject of an article about their gender pay gap reports and employers that receive an ESG audit or financial audit from a Big 4 auditor are less likely to misreport. Overall, our results suggest that 12%–15% of employers misreport in 2017 and 8%–11% misreport in subsequent years. Our findings highlight the importance of meaningful oversight within the context of ESG reporting.
 This paper was accepted by Ranjani Krishnan, accounting.
 Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2024.05125 .

---

A Dataset for Detecting Real-World Environmental Claims
10.2139/ssrn.4207369
In this paper, we introduce an expert-annotated dataset for detecting real-world environmental claims made by listed companies. We train and release baseline models for detecting environmental claims using this new dataset. We further preview potential applications of our dataset: We use our ﬁne-tuned model to detect environmental claims made in answer sections of quarterly earning calls between 2012 and 2020 – and we ﬁnd that the amount of environmental claims steadily increased since the Paris Agreement in 2015.

---

ClimateBert: A Pretrained Language Model for Climate-Related Text
10.2139/ssrn.4229146
Over the recent years, large pretrained language models (LM) have revolutionized the field of natural language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In particular, climate-related texts include specific language that common LMs can not represent accurately. We argue that this shortcoming of today's LMs limits the applicability of modern NLP to the broad field of text processing of climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts, crawled from various sources such as common news, research articles, and climate reporting of companies. We find that CLIMATEBERT leads to a 48% improvement on a masked language model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related downstream tasks like text classification, sentiment analysis, and fact-checking.

---

Does the Interaction of Informativeness, Readability, and Sentiment within Company’s Sustainability Disclosure Shape an Entity’s ESG Score? – Evidence from Germany
10.2139/ssrn.4697807
The integration of ESG practices in sustainability disclosure as well as the increasingly evolving regulations in the European Union and Germany signify a transformative shift in individual behaviors and corporate dynamics. Thus, the focus on sustainability disclosure by German companies has increased, along with the evaluation of ESG performance through rating agencies. Since sustainability disclosure emphasizes qualitative information, it is not surprising that both, management and analysts, prioritize the textual content of such reports. This leads to the question of how textual elements of a company’s sustainability disclosure are related to analysts’ ESG scores. To address this research question, we analyze sustainability disclosure of companies listed in the German Prime Standard for financial years from 2017 to 2022 in terms of informativeness, readability, and sentiment by using OLS panel regression analyses. Our research contributes to the growing sustainability disclosure literature by referring to companies within the German regulatory framework, utilizing state-of-the-art LLMs FinBERT and FinBERT-ESG, applying various methods for textual analysis. The results of our study suggest that a high level of transparency, precision, and comprehensibility of language are important factors in achieving a superior ESG score.

---

The Abnormal Tone of CEO Letters in ESG Reports: What Does it Tell us about Future ESG and Financial Performance?
10.2139/ssrn.4744534
Download This Paper Open PDF in Browser Add Paper to My Library Share: Permalink Using these links will ensure access to this page indefinitely Copy URL Copy DOI

---

Optimizing ESG Reporting: Innovating with E-Bert Models in Nature Language Processing
10.2139/ssrn.4781946
Download This Paper Open PDF in Browser Add Paper to My Library Share: Permalink Using these links will ensure access to this page indefinitely Copy URL Copy DOI

---

Combining AI and Domain Expertise to Assess Corporate Climate Transition Disclosures
10.2139/ssrn.4826207
Companies need sound planning to reduce their emissions and deal with the transition to a more sustainable economy. The disclosure of such plans is key for effective capital allocation and risk management. Transition and sustainability disclosures are a compass for market participants to guide their actions and strategies toward the net-zero target. If companies plan their transition appropriately, the negative implications of physical and transition risks for micro-and macro-financial stability can be reduced. Many frameworks have been suggested to assess transition plans’ ambition, credibility, and feasibility. However, the lack of one clear reference framework paves the way for inconsistencies in transition plans and the risk of greenwashing. We propose a set of 64 common ground indicators from 28 different transition plan disclosure frameworks to comprehensively assess transition plans and develop a novel natural language processing (NLP)–based tool to automate the assessment of companies’ disclosures. This can help investors and financial supervisors assess transition risks while supporting companies’ disclosure efforts. Applying the tool to 143 reports from the carbon-intensive CA100+ companies, we find that companies tend to disclose more indicators related to target setting (talk) but fewer indicators related to the concrete implementation of strategies (walk). Our results demonstrate that machine learning can be used to generate a positive impact on the transition towards a more sustainable economy by identifying the elements of transition plans that require further scrutiny and/or effort. Our work will be a starting point for further leveraging new technologies in sustainable finance. For example, the assessment of the plans could be used by financial regulators in their supervisory practices or to investigate whether the risk of greenwashing is reflected in stock returns.

---

Information Extraction from ESG Reports Using Nlp: A Chatgpt Comparison
10.2139/ssrn.4836432
Corporate sustainability has garnered increasing interest from analysts, researchers, and regulators. However, deriving actionable information from environmental, social, and governance (ESG) reports is challenging. Reports are largely unstandardized, dense with technical data, and lengthy, hindering interpretation. Traditional natural language processing (NLP) techniques often struggle to accurately extract information due to their inability to understand context, but emerging techniques, including large language models like ChatGPT, may present user-friendly avenues to automating information extraction from ESG reports while preserving context. We compare ChatGPT's performance to a keyword search method in extracting from ESG reports their assurance status and the frameworks they adhere to. We find that while ChatGPT demonstrates higher specificity, it is less sensitive than a keyword search because ChatGPT is effective at evaluating text contextually but is susceptible to false negatives. Results suggest that extraction would benefit from standardized formats and terminology or an XBRL-style tagging system to clarify context.

---

Artificial intelligence driven approaches to strengthening Environmental, Social, and Governance (ESG) criteria in sustainable business practices: a review
10.2139/ssrn.4843215
Artificial Intelligence (AI) is revolutionizing Environmental, Social, and Governance (ESG) criteria, significantly enhancing sustainable business practices. This paper reviews AI-driven methods that improve ESG performance across various industries. With growing regulatory demands and stakeholder expectations for transparency, businesses are utilizing AI technologies to monitor and report ESG metrics with remarkable precision. AI enables real-time data collection and analysis, allowing companies to track environmental impacts, optimize resource use, and reduce carbon emissions. Socially, AI promotes workforce diversity and inclusion through unbiased recruitment algorithms and improves workplace safety with predictive analytics in occupational health and safety. In governance, AI-driven risk management tools and blockchain technologies strengthen corporate governance by ensuring compliance and transparency in supply chains and financial reporting. The use of Natural Language Processing (NLP) and machine learning algorithms in ESG data analytics has improved the robustness and insightfulness of sustainability reporting, aligning business strategies with global sustainability goals. Additionally, AI-powered predictive models help identify future ESG risks and opportunities, supporting businesses in making informed decisions aligned with long-term sustainability objectives. This review highlights recent advancements demonstrating AI's effectiveness in ESG implementation, focusing on the role of AI in promoting a culture of sustainability within organizations. As AI technologies advance, their potential to transform ESG practices will significantly contribute to building resilient and sustainable businesses for the future.

---

Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks
10.2139/ssrn.5189069
This study provides the first comprehensive assessment of consistency and reproducibility in Large Language Model (LLM) outputs in finance and accounting research. We evaluate how consistently LLMs produce outputs given identical inputs through extensive experimentation with 50 independent runs across five common tasks: classification, sentiment analysis, summarization, text generation, and prediction. Using three OpenAI models (GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse financial source texts and data, covering MD&As, FOMC statements, finance news articles, earnings call transcripts, and financial statements. Our findings reveal substantial but task-dependent consistency, with binary classification and sentiment analysis achieving near-perfect reproducibility, while complex tasks show greater variability. More advanced models do not consistently demonstrate better consistency and reproducibility, with task-specific patterns emerging. LLMs significantly outperform expert human annotators in consistency and maintain high agreement even where human experts significantly disagree. We further find that simple aggregation strategies across 3-5 runs dramatically improve consistency. We also find that aggregation may come with an additional benefit of improved accuracy for sentiment analysis when using newer models. Simulation analysis reveals that despite measurable inconsistency in LLM outputs, downstream statistical inferences remain remarkably robust. These findings address concerns about what we term"G-hacking,"the selective reporting of favorable outcomes from multiple Generative AI runs, by demonstrating that such risks are relatively low for finance and accounting tasks.

---

Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison
10.2139/ssrn.5275418
Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions but often suffers from the scarcity of diverse, labeled datasets that reflect real-world conversational nuances. This paper presents an approach for generating synthetic ABSA data using Large Language Models (LLMs) to address this gap. We detail the generation process aimed at producing data with consistent topic and sentiment distributions across multiple domains using GPT-4o. The quality and utility of the generated data were evaluated by assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification tasks. Our results demonstrate the effectiveness of the synthetic data, revealing distinct performance trade-offs among the models: DeepSeekR1 showed higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall, and Gemini 1.5 Pro offered significantly faster inference. We conclude that LLM-based synthetic data generation is a viable and flexible method for creating valuable ABSA resources, facilitating research and model evaluation without reliance on limited or inaccessible real-world labeled data.

---

Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach
10.2139/ssrn.5468389
InfluenceMap's LobbyMap Platform monitors the climate policy engagement of over 500 companies and 250 industry associations, assessing each entity's support or opposition to science-based policy pathways for achieving the Paris Agreement's goal of limiting global warming to 1.5{\deg}C. Although InfluenceMap has made progress with automating key elements of the analytical workflow, a significant portion of the assessment remains manual, making it time- and labor-intensive and susceptible to human error. We propose an AI-assisted framework to accelerate the monitoring of corporate climate policy engagement by leveraging Retrieval-Augmented Generation to automate the most time-intensive extraction of relevant evidence from large-scale textual data. Our evaluation shows that a combination of layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies yields the best performance in extracting and classifying evidence from multilingual corporate documents. We conclude that while the automated RAG system effectively accelerates evidence extraction, the nuanced nature of the analysis necessitates a human-in-the-loop approach where the technology augments, rather than replaces, expert judgment to ensure accuracy.

---

Sustainability Bias in Utility and Infrastructure Related Large Language Model Queries
10.2175/193864718825159252
Sustainability Bias in Utility and Infrastructure Related Large Language Model QueriesAbstractIntroduction Large Language Models, such as ChatGPT, have become the fastest growing web platforms to date, exceeding one hundred million users in record time less than two months and as the capabilities and use of ChatGPT to inform decisions increases, it is crucial to understand the inherent biases of the platform regarding sustainability in the context of infrastructure and utilities. Sustainability, defined for the purpose of this paper as the intersection of social, environmental, and economic factors which will ensure a better future for tomorrow, is a critical factor to consider when making decisions for infrastructure and utilities. The intent of this paper is to determine and assess what biases, if any, ChatGPT expresses when responding to prompts regarding infrastructure and utilities. If advice is sought from a platform like ChatGPT in the context of utilities and infrastructure, does it prioritize Sustainability? What about regenerative development? Perhaps more interestingly, since ChatGPT is essentially a representation of an amalgam of human thought, what does it show about our own societal biases in terms of sustainability in utility data? These were the guiding questions for our analysis, which is intended to provide perspective on how the incredible new tools coming out today require us to be aware of their eccentricities to utilize them appropriately. Methodology A list of over one hundred questions were developed for use as ChatGPT prompts. These questions were intended to fall along three axes: public-private, discipline (water resources, transportation, site development), and function (operations and maintenance, planning, design, financing, and construction). Each question was developed selectively as to not directly lead ChatGPT to mention sustainability in the response, but to be open ended and to inherently require considerations regarding economics, the environment, and society. Context was provided in the form of a preamble for each question, and ChatGPT was asked to respond using a list format and limit its token response to 150 for each question this helped weigh each question evenly and control cost. The engine selected for this analysis was ChatGPT 3.5-Turbo accessible via OpenAI's Application Programming Interface (API). This engine was selected for its similarity to OpenAI's non-premium and publicly available interface, as well as its advanced ability to understand and respond to text prompts. Each of ChatGPT's responses to provided prompts were scored as follows: 0 not directly addressed, 1 indirectly addressed, and 2 directly addressed. The scores were used to assess whether the response adequately addressed the following criteria: 1) social aspect of sustainability, 2) economic aspect of sustainability 3) environmental aspect of sustainability 4) sustainability in general 5) regenerative development. Two different methods were used to score the ChatGPT provided responses. The first method utilized ChatGPT to score itself, where ChatGPT was fed the questions and corresponding answers and was then instructed to score the answers based on the criteria listed above. The ChatGPT scores were then reviewed by a human being for quality assurance and control. The second method required a human being to evaluate the questions and responses and provide a score manually. Results Sustainability was addressed to some degree across all criteria. However, the breakdown in how sustainability was addressed varies between the social, environmental, and economic components. The social component was addressed directly more than addressed indirectly or not addressed. Both the economic and environmental components had far more indirect responses than direct responses, and far more responses that did not address the respective topic. This indicates that social and communal aspects of infrastructure are well represented by ChatGPT, but environmental and economic components require further prompting or weighting from decision makers to make balanced and informed decisions. All results were then broken down by functional group, public/private, and discipline to explore more specific biases, all results are given within Figures 4-8. A deeper analysis will be performed between human scores and ChatGPT generated scores to determine potential bias in scoring and to assess whether there is a different bias between generating text and understanding text when it comes to ChatGPT and sustainability. Conclusion It is expected that ChatGPT will contain bias. It is trained on human generated data which will inheritently contain some level of skew, but understanding this bias qualitatively and quantitatively will provide decision-makers with more context needed to evaluate the reliability of results from ChatGPT, especially in the context of sustainability in infrastructure a complex, evolving field that lends itself to the powers of AI and ChatGPT. The bias observed seems to skew overall towards a social component and often does not directly or at all address economic and environmental components when it is expected these aspects would be incorporated.This paper was presented at the WEF/AWWA Utility Management Conference, February 13-16, 2024.SpeakerKuehne, WilliamPresentation time09:30:0010:00:00Session time08:30:0010:00:00SessionManagement and Oversight of New Frontiers in Artificial IntelligenceSession number16Session locationOregon Convention Center, Portland, OregonTopicUtility Sustainability, Environmental and Regulatory IssuesTopicUtility Sustainability, Environmental and Regulatory IssuesAuthor(s)Kuehne, WilliamAuthor(s)W. Kuehne1, L. Basler1Author affiliation(s)Ardurra Group 1;SourceProceedings of the Water Environment FederationDocument typeConference PaperPublisherWater Environment FederationPrint publication date Feb 2024DOI10.2175/193864718825159252Volume / Issue Content sourceUtility Management ConferenceWord count12

---

Comparing the Validity of Automated and Human Scoring of Essays
10.2190/cx92-7wkv-n7wc-jl0a
Automated, or computer-based, scoring represents one promising possibility for improving the cost effectiveness (and other features) of complex performance assessments (such as direct tests of writing skill) that require examinees to construct responses rather than select them from a set of multiple choices. Indeed, significant advances have been made in applying natural language processing techniques to the automatic scoring of essays. Thus far, most of the validation of automated scoring has focused appropriately (but too narrowly, we contend) on the correspondence between computer-generated scores and those assigned by human readers. Far less effort has been devoted to assessing the relation of automated scores to independent indicators of examinees' writing skills. This study examined the relationship of scores from a graduate level writing assessment to several independent, non-test indicators of examinees' writing skills—both for automated scores and for scores assigned by trained human readers. The extent to which automated and human scores exhibited similar relations with the non-test indicators was taken as evidence of the degree to which the two methods of scoring reflect similar aspects of writing proficiency. Analyses revealed significant, but modest, correlations between the non-test indicators and each of the two methods of scoring. These relations were somewhat weaker for automated scores than for scores awarded by human readers. Overall, however, the results provide some evidence of the validity of one specific procedure for automated scoring.

---

Analyzing Trends of Loneliness Through Large-Scale Analysis of Social Media Postings: Observational Study (Preprint)
10.2196/preprints.17188
<sec> <title>BACKGROUND</title> Loneliness has become a public health problem described as an epidemic, and it has been argued that digital behavior such as social media posting affects loneliness. </sec> <sec> <title>OBJECTIVE</title> The aim of this study is to expand knowledge of the determinants of loneliness by investigating online postings in a social media forum devoted to loneliness. Specifically, this study aims to analyze the temporal trends in loneliness and their associations with topics of interest, especially with those related to mental health determinants. </sec> <sec> <title>METHODS</title> We collected a total of 19,668 postings from 11,054 users in the loneliness forum on Reddit. We asked seven crowdsourced workers to imagine themselves as writing 1 of 236 randomly chosen posts and to answer the short-form UCLA Loneliness Scale. After showing that these postings could provide an assessment of loneliness, we built a predictive model for loneliness scores based on the posts’ text and applied it to all collected postings. We then analyzed trends in loneliness postings over time and their correlations with other topics of interest related to mental health determinants. </sec> <sec> <title>RESULTS</title> We found that crowdsourced workers can estimate loneliness (interclass correlation=0.19) and that predictive models are correlated with reported loneliness scores (Pearson &lt;i&gt;r&lt;/i&gt;=0.38). Our results show that increases in loneliness are strongly associated with postings to a suicidality-related forum (hazard ratio 1.19) and to forums associated with other detrimental behaviors such as depression and illicit drug use. Clustering demonstrates that people who are lonely come from diverse demographics and from a variety of interests. </sec> <sec> <title>CONCLUSIONS</title> The results demonstrate that it is possible for unrelated individuals to assess people’s social media postings for loneliness. Moreover, our findings show the multidimensional nature of online loneliness and its correlated behaviors. Our study shows the advantages of studying a hard-to-reach population through social media and suggests new directions for future studies. </sec>

---

Aspect-Based Sentiment Analysis in Bromo Tengger Semeru National Park Indonesia Based on Google Maps User Reviews
10.22146/ijccs.77354
Technology can influence and shape a person's behavior patterns when planning tours, traveling, and after traveling. Visitors' reviews can be used as evaluation material to improve the quality of tourist destinations and become a determining factor for other tourists to visit or revisit the destinations. The process of utilizing these reviews can be done by assessing the aspects of tourist destinations based on reviews from visitors. This study aims to conduct an aspect-based sentiment analysis on one of the tourist destinations in Indonesia, namely Bromo Tengger Semeru National Park, based on reviews of Google Maps users. The aspects consist of attractions, facilities, access, and price. The sentiment classification model used is a machine learning model consisting of SVM, Complement Naïve Bayes, Logistic Regression, and transfer learning from pre-trained BERT, IndoBERT, and mBERT. Based on the experimental results, transfer learning from the IndoBERT model achieved the best performance with accuracy and F1-Score of 91.48% and 71.56%, respectively. In addition, among the machine learning models used, the SVM model gives the best results with an accuracy of 89.16% and an F1-Score of 62.23%.

---

Optimizing Aspect Term Extraction and Sentiment Classification through Attention Mechanism and Sparse Attention Techniques
10.22266/ijies2024.1031.75
: Aspect-based sentiment analysis (ABSA) has become an essential field in Natural Language Processing (NLP) in recent years. ABSA not only categorizes sentiment as positive, negative, or neutral but also understands the specific aspects or topics discussed in the text review. This study focuses on two important elements of ABSA: Aspect Term Extraction (ATE) and Aspect Sentiment Classification (ASC). This study uses a combination of Sentence Embedding (SBERT) techniques, Part-of-Speech tagging, cosine-similarity calculation to assess words with their respective aspect labels, and the sparse attention mechanism (BIGBIRD) method, which has been proven to increase accuracy effectively and is effective in terms of time and memory usage. By applying this method to two hotel review datasets, Traveloka Review and Semeval 2016 dataset, it is proven to work well on two ABSA tasks, namely ATE and ASC. The results of the ATE test obtained an accuracy of 0.99, and the ASC test obtained an accuracy of 0.89. This study contributes to the advancement of ABSA by introducing a new methodology that improves the accuracy of aspect term extraction and sentiment classification. Additionally, it identifies avenues for future research, including exploring additional techniques to improve model performance and address potential limitations.

---

Measuring Political Preferences
10.2307/2111702
Theory: When analysts adopt surrogates of actors' political preferences for purposes unanticipated by the inventors of those measures, they often stretch (but not explicitly assess) the range of reliability and validity. Hypotheses: The consequences pushing measures beyond their intended purposes may significantly impact research findings, as well as the conclusions drawn from those findings. Methods: "Methodological audit" of measures developed by Segal and Cover (1989) to represent the political preferences of justices on the United States Supreme Court. Mainly regression analysis using the Segal/Cover scores and vote data drawn from the United States Supreme Court Judicial Database. Results: Analysts would be well advised to weigh carefully whether adequate tests have been performed before adopting others' preference measures for their own research. More specific conclusions are: 1) scholars should invoke the Segal/Cover scores in the set of circumstances indicated by their developers: aggregated individual-level decisions in civil liberties cases; and 2) students of the judicial process who seek to explore phenomena other than aggregated individual-level voting in civil liberties cases ought to give serious thought to devising new surrogates for judicial preferences.

---

Auditor Response to Negative Media Coverage of Client Environmental, Social, and Governance Practices
10.2308/acch-52450
SYNOPSIS We use new data to examine auditor response to negative media coverage of client environmental, social, and governance (ESG) practices. This coverage can be indicative of an increased risk...

---

The current state and future implications of ESG assurance
10.2308/ciia-2022-012
This study examines environmental, social, and governance (ESG) reports of companies on the Wall Street Journal’s and Investor Business Daily’s top 100 sustainable companies. We collect information on whether the reports are assured, the type of assurance and standards cited, the assurance provider, and whether or not the provider also audits the financial statements. 58 percent of sample companies voluntarily sought external assurance services for some portion of these reports, with limited assurance primarily provided. Big 4 firms provide assurance for most international companies, but assure only 16 percent of U.S companies. We discuss practical implications and future research areas, including cross-country comparisons, ESG assurance, and standard-setting.

---

ESG Fraud Risks: A Research Agenda
10.2308/jfar-2024-021
ABSTRACT Environmental, social, and governance (ESG) criteria are often used to screen investment and business decisions and have become the basis for sustainability reporting. Although ESG coverage is far more encompassing than financial reporting, it remains in its infancy. In the absence of robust measurement frameworks and standards, adequately trained personnel for reporting and auditing ESG disclosures, clarity regarding the demand and supply of ESG information, and strong regulation and oversight, ESG remains susceptible to fraud and abuse. Building on a supply-and-demand framework, we systematically identify and describe the “gaps” that characterize the ESG reporting environment and how they lead to various fraud risks, including misreporting of ESG performance, greenwashing, data manipulation, and the misrepresentation, mis-selling, and manipulation of carbon credits. We develop a research agenda relevant to ESG reporting and offer suggestions for future academic work. Data Availability: Not applicable. JEL Classifications: H32; L20; M14; M49.

---

Sustainability Reports as a Legitimacy Tool: A Qualitative Perspective from Financial Accounting Practices
10.23887/ijssb.v9i3.101573
Although the number of sustainability reports in Indonesia has increased—particularly after the enactment of the Financial Services Authority’s Regulation No. 51/POJK.03/2017—the substance, honesty, and motivations behind these reports remain questionable. This study explores sustainability report practices as a means of organizational legitimacy within financial accounting. Using an interpretive, qualitative approach with a single-case study design, the research explores the subjective experiences of six key informants directly involved in preparing sustainability reports at a publicly listed company in Indonesia. The findings indicate that sustainability reports are employed to comply with regulatory obligations and market demands and serve as a symbolic communication tool and strategic representation of the company. Selective disclosure is not merely technical but also carries political and social significance because it is used to build organizational legitimacy. In this case study, selective disclosure is evident in how companies emphasize achievements that align with the Sustainable Development Agenda, such as carbon emission reduction programs, social responsibility activities in the community, and environmentally friendly innovations. In contrast, sensitive issues such as program failures, negative operational impacts on the environment, or dissatisfaction among some community members tend to be minimized or even omitted from the narrative. Selective disclosure of information that supports a positive corporate image and the construction of sustainability narratives aligned with managerial interests. This study reinforces the relevance of legitimacy theory and sociological approaches in understanding accounting reporting, revealing that sustainability reports constitute a discursive arena in which the meaning of sustainability is socially and politically constructed.

---

End-to-end Reinforcement Learning for Autonomous Racing: Bridging the sim-to-real gap
10.23919/acc63710.2025.11107738
Deep reinforcement learning is a promising technique that can help create autonomous agents. However, it is still an open problem how one can create a controller with robust operation for real-world automotive systems. The difficulty lies in either sample efficiency for real-world learning or developing a good enough simulator for training. This paper addresses the latter, proposing a method that provides a solution to the sim-to-real gap through domain randomization, learning with disturbances, and observation preprocessing. The method is validated on a small-scale F1TENTH-type test vehicle, that is trained to race autonomously in a fully end-to-end manner. It is demonstrated that the training process results in a policy that can drive the car safely even over the grip limit.

---

A Survey on Interpretable Clustering
10.23919/ccc52363.2021.9549986
Clustering is the process of dividing a collection of physical or abstract objects into several classes composed of similar objects. Now there are many clustering algorithms with superior performance, but the clusters generated by them are difficult for human to understand. Thus, some interpretable clustering methods are proposed, which make the clustering results have good interpretability without much impact on the clustering accuracy. This paper reviews the interpretable clustering algorithms, introduces and summarizes the previous work in this field according to the different interpretative ways, including rules, rectangular bounds and decision trees, and explores the development of interpretable clustering algorithms in the future.

---

DeepRacing: a framework for autonomous racing
10.23919/date48585.2020.9116486
We consider the challenging problem of high speed autonomous racing in realistic dynamic environments. DeepRacing is a novel end-to-end framework, and a virtual testbed for training and evaluating algorithms for autonomous racing. The virtual testbed is implemented using the realistic Formula One (F1) Codemasters game, which is used by many F1 drivers for training. We present AdmiralNet - a Convolution Neural Network (CNN) integrated with Long Short-Term Memory (LSTM) cells that can be tuned for the autonomous racing task in the highly realistic F1 game. We evaluate AdmiralNet’s performance on unseen race tracks, and also evaluate the degree of transference between the simulation and the real world by implementing end-to-end racing on a physical 1/10 scale autonomous racecar.

---

Artificial Intelligence in ESG and Sustainable Finance: A Bibliometric Analysis of Research Trends
10.2478/picbe-2025-0117
Abstract The integration of Artificial Intelligence (AI), including Machine Learning (ML) and Natural Language Processing (NLP), into Environmental, Social, and Governance (ESG) frameworks and sustainable finance has gained significant academic and industry attention. This study presents a bibliometric analysis of research at the intersection of AI, ESG, and sustainable finance, using Web of Science data from 2004 to 2025. Our analysis maps the evolution of research trends, identifies key authors, influential publications, and collaboration networks, and explores the intellectual structure of the field. Using co-citation analysis, keyword co-occurrence mapping, and thematic clustering, we reveal how AI-driven methodologies have shaped ESG assessments, sustainable investment strategies, and financial decision-making. Findings highlight a sharp rise in AI applications in ESG post-2015, particularly in ESG risk modelling, climate analytics, and AI-driven reporting. This study provides a comprehensive overview of the evolving academic discourse, identifying emerging research directions and challenges related to data standardization, transparency, and ethical AI applications in sustainability.

---

Using Social Media as A Legitimation Tool in Sustainability Reporting: Evidence from SOEs Listed on the Indonesia Stock Exchange
10.24815/jdab.v10i2.27550
The use of social media changes the dynamics of reporting, leading to interactive communication processes where dialogue and engagement with stakeholders can be used to accompany disclosure of information to seek legitimacy. The purpose of this study is to determine the use of social media in the sustainability report in state owned enterprises (SOEs) listed on the Indonesia Stock Exchange as a means of seeking legitimacy from stakeholders. The data obtained can be parsed, checked, and compared with open coding (open coding). The results of data processing are displayed in excel spreadsheet, and processed into diagrams and tables. The results of this study indicate that social media allows companies to build a dialogue about current issues that affect society and the environment. The issues raised on social media by companies from the three platforms mostly consist of community, employees, diversity, gender equality, health, and corruption issues. The findings implicate that social media has been largely used even by SOEs as a legitimation tool for its stakeholders in constructing a dialogue regarding current issues affecting people and the environment.

---

Organisational Metamorphosis: Tracing Sustainability Integration through Sentiment Evolution in Corporate Reporting in Turkish Companies
10.24818/ea/2025/70/1052
This study examines the evolution of sentiment in the sustainability reports of Turkish companies from 2014 to 2023, serving as an indicator of organisational change in response to shifting sustainability regulations. A longitudinal sample of corporate sustainability reports was subjected to sentiment analysis and linguistic pattern recognition, encompassing three distinct regulatory periods. By comparing the tone and language across these periods, the study investigates how companies adapt their sustainability communication strategies under increasing institutional pressures. The study is grounded in organisational change theories and regulatory response models. It sheds light on whether heightened disclosure requirements and global sustainability norms have driven substantive shifts in corporate narrative or merely superficial compliance. The analysis reveals notable changes in sentiment, with an overall trend toward more optimistic and assertive sustainability disclosures over time. These linguistic adaptations correspond to key regulatory milestones. The findings suggest a gradual internalisation of sustainability principles, reflecting organisational learning and strategic legitimisation efforts. The findings contribute to the extant literature on corporate sustainability reporting by linking textual sentiment trends to institutional change dynamics, offering a rare developing-country perspective. The insights are particularly relevant for stakeholders and policymakers seeking to understand and enhance the impact of sustainability reporting in emerging economies.

---

Earlier Attention? Aspect-Aware LSTM for Aspect Sentiment Analysis
10.24963/ijcai.2019/738
Aspect-based sentiment analysis (ABSA) aims to predict fine-grained sentiments of comments with respect to given aspect terms or categories. In previous ABSA methods, the importance of aspect has been realized and verified. Most existing LSTM-based models take aspect into account via the attention mechanism, where the attention weights are calculated after the context is modeled in the form of contextual vectors. However, aspect-related information may be already discarded and aspect-irrelevant information may be retained in classic LSTM cells in the context modeling process, which can be improved to generate more effective context representations. This paper proposes a novel variant of LSTM, termed as aspect-aware LSTM (AA-LSTM), which incorporates aspect information into LSTM cells in the context modeling stage before the attention mechanism. Therefore, our AA-LSTM can dynamically produce aspect-aware contextual representations. We experiment with several representative LSTM-based models by replacing the classic LSTM cells with the AA-LSTM cells. Experimental results on SemEval-2014 Datasets demonstrate the effectiveness of AA-LSTM.

---

Evaluating and Aggregating Feature-based Model Explanations
10.24963/ijcai.2020/417
A feature-based model explanation denotes how much each input feature contributes to a model's output for a given data point. As the number of proposed explanation functions grows, we lack quantitative evaluation criteria to help practitioners know when to use which explanation function. This paper proposes quantitative evaluation criteria for feature-based explanations: low sensitivity, high faithfulness, and low complexity. We devise a framework for aggregating explanation functions. We develop a procedure for learning an aggregate explanation function with lower complexity and then derive a new aggregate Shapley value explanation function that minimizes sensitivity.

---

Impact of Microinsurance Affordability on Profitability of Micro, Small, and Medium Enterprises (MSMEs) in Lagos State, Nigeria
10.25105/ijsmebs.v9i2.24208
Nigeria has one of the lowest insurance penetration rates in the world possibly owing to the cost of insurance products. The study investigated the impact of microinsurance affordability on MSME's profitability in Nigeria. The study is quantitative employing both survey and cross-sectional research design. A total of four hundred (400) copies of the questionnaire were administered, but only three hundred forty-five (345) copies of the questionnaire were duly filled and used for this study representing an 86.3% response rate. According to the findings, microinsurance affordability has a favorable and considerable impact on MSME's profitability in Nigeria. The study recommended that insurance companies should improve on claims processing, be sincere, and promptly attend to customer's claims because some insurance companies do not always give full details about policies to lure people into buying them.

---

Sentiment Analysis of Tweets Before the 2024 Elections in Indonesia Using Bert Language Models
10.26555/jiteki.v9i3.26490
General election is one of the crucial moments for a democratic country, e.g., Indonesia. Good election preparation can increase people's participation in the general election. In this study, we conduct a sentiment analysis of Indonesian public opinion on the upcoming 2024 election using Twitter data and IndoBERT model. This study is aimed at helping the government and related institutions to understand public perception. Therefore, they could obtain valuable insights to better prepare for elections, including evaluating the election policies, developing campaign strategies, increasing voter engagement, addressing issues and conflicts, and increasing transparency and public trust. The main contribution of this study is threefold: (i) the application of state-of-the-art transformer-based model IndoBERT for sentiment analysis on political domain; (ii) the empirical evaluation of IndoBERT model against machine learning and lexicon-based models; and (iii) the new dataset creation for sentiment analysis in political domain. Our Twitter data shows that Indonesian public mostly reacts neutrally (83.7%) towards the upcoming 2024 election. Then, the experimental results demonstrate that IndoBERT large-p1 is the best-performing model that achieves an accuracy of 83.5%. It improves our baseline systems by 48.5% and 46.49% for TextBlob, 2.5% and 14.49% for Multinomial Naïve Bayes, and 3.5% and 13.49% for Support Vector Machine in terms of accuracy and F-1 score, respectively.

---

TEASER: Towards Efficient Aspect-based SEntiment Analysis and Recognition
10.26615/978-954-452-072-4_013
Sentiment analysis aims to detect the overall sentiment, i.e., the polarity of a sentence, paragraph, or text span, without considering the entities mentioned and their aspects. Aspect-based sentiment analysis aims to extract the aspects of the given target entities and their respective sentiments. Prior works formulate this as a sequence tagging problem or solve this task using a span-based extract-then-classify framework where first all the opinion targets are extracted from the sentence, and then with the help of span representations, the targets are classified as positive, negative, or neutral. The sequence tagging problem suffers from issues like sentiment inconsistency and colossal search space. Whereas, Span-based extract-then-classify framework suffers from issues such as half-word coverage and overlapping spans. To overcome this, we propose a similar span-based extract-then-classify framework with a novel and improved heuristic. Experiments on the three benchmark datasets (Restaurant14, Laptop14, Restaurant15) show our model consistently outperforms the current state-of-the-art. Moreover, we also present a novel supervised movie reviews dataset (Movie20) and a pseudo-labeled movie reviews dataset (moviesLarge) made explicitly for this task and report the results on the novel Movie20 dataset as well.

---

Essays on Corporate Climate Risk Disclosures
10.26686/wgtn.30020038
&lt;p&gt;&lt;strong&gt;This thesis explores voluntary climate risk disclosures and consists of five chapters. Chapter 1 introduces the thesis, Chapters 2 to 4 present three independent but related studies, and Chapter 5 provides a discussion.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Chapter 1 introduces the thesis by highlighting the importance of corporate voluntary climate risk disclosures and outlining the three key research questions explored in the main chapters. It also provides brief overviews of the motivation, research setting, and main findings for each research topic.&lt;/p&gt;&lt;p&gt;Chapter 2 examines how investors assess the credibility of voluntary climate disclosures. The empirical results show that investors understand the signal conveyed by green patents, which certify firms’ climate disclosure credibility. Exploiting the variation in the leniency of patent examiners’ grant decisions for identification, I show that firms with more green patents: (i) are more likely to issue voluntary climate disclosures, (ii) experience higher stock returns around these disclosures, and (iii) see a greater subsequent increase in institutional ownership, especially among climate-conscious investors. Additionally, (iv) the influence of green patents is less pronounced when climate disclosures are externally assured, report bad news, or when reporting firms possess high reputational capital.&lt;/p&gt;&lt;p&gt;Chapter 3 investigates whether greenwashing activities generate a negative externality. To address this question, I focus on the impact of the 2015 Volkswagen emissions fraud on the voluntary climate disclosures of Volkswagen’s industry peers. Analyzing automakers across OECD countries, I find that these firms demonstrate diminished incentives to engage in voluntary disclosures following the fraud. This reduction in disclosure propensity is primarily attributed to a loss of disclosure credibility due to the negative spillover effects of Volkswagen’s deceptive behavior. Notably, the reduction is most pronounced among automakers without green patents, those in countries with low social capital, and climate disclosures without external ESG assurance. The adverse externality of greenwashing activities revealed in this study may provide a rationale for the regulatory mandate of climate disclosures.&lt;/p&gt;&lt;p&gt;Chapter 4 explores how firms’ environmental performance influences their decisions to provide voluntary climate disclosures. Using verified carbon emissions reported under the UK mandatory emissions disclosure regulation, I find that firms with higher Scope 1 emissions or worse environmental performance are more likely to provide voluntary climate disclosures before the regulation. Consistent with the explanation that firms under-report emissions in voluntary climate disclosures, I find a significant increase in Scope 1 emissions in the first year of the disclosure regulation for firms that previously disclosed these emissions voluntarily. This increase in emissions is more pronounced among firms without external ESG assurance. Furthermore, I find less under-reporting for Scope 2 emissions, which rely on external data and are harder to manipulate.&lt;/p&gt;&lt;p&gt;Finally, Chapter 5 summarizes the main findings of the three research topics and discusses their potential implications.&lt;/p&gt;

---

An Overview of Current Research on Automated Essay Grading
10.28945/331
Introduction Assessment is considered to play a central role in the educational process. The interest in the development and in use of Computer-based Assessment Systems (CbAS) has grown exponentially in the last few years, due both to the increase of the number of students attending universities and to the possibilities provided by e-learning approaches to asynchronous and ubiquitous education. According to our findings (Valenti, Cucchiarelli, & Panti., 2002) more than forty commercial CbAS are currently available on the market. Most of those tools are based on the use of the so-called objective-type questions: i.e. multiple choice, multiple answer, short answer, selection/association, hot spot and visual identification (Valenti et al., 2000). Most researchers in this field agree on the thesis that some aspects of complex achievement are difficult to measure using objective-type questions. Learning outcomes implying the ability to recall, organize and integrate ideas, the ability to express oneself in writing and the ability to supply merely than identify interpretation and application of data, require less structuring of response than that imposed by objective test items (Gronlund, 1985). It is in the measurement of such outcomes, corresponding to the higher levels of the Bloom's (1956) taxonomy (namely evaluation and synthesis) that the essay question serves its most useful purpose. One of the difficulties of grading essays is the subjectivity, or at least the perceived subjectivity, of the grading process. Many researchers claim that the subjective nature of essay assessment leads to variation in grades awarded by different human assessors, which is perceived by students as a great source of unfairness. Furthermore essay grading is a time consuming activity. According to Mason (2002), about 30% of teachers' time in Great Britain is devoted to marking. "So, if we want to free up that 30% (worth 3 billion UK Pounds/year to the taxpayer by the way) then we must find an effective way, that teacher will trust, to mark essays and short text responses." This issue may be faced through the adoption of automated assessment tools for essays. A system for automated assessment would at least be consistent in the way it scores essays, and enormous cost and time savings could be achieved if the system can be shown to grade essays within the range of those awarded by human assessor. Furthermore, according to Hearst (2000) using computers to increase our understanding of the textual features and cognitive skills involved in the creation and in the comprehension of written texts, will provide a number of benefits to the educational community. In fact "it will help us develop more effective instructional materials for improving reading, writing and other communication abilities. It will also help us develop more effective technologies such as search engines and question answering systems for providing universal access to electronic information." Purpose of this paper is to present a survey of current approaches to the automated assessment of free text answers. Thus, in the next section, the following systems will be discussed: Project Essay Grade (PEG), Intelligent Essay Assessor (IEA), Educational Testing service I, Electronic Essay Rater (E-Rater), C-Rater, BETSY, Intelligent Essay Marking System, SEAR, Paperless School free text Marking Engine and Automark. All these systems are currently available either as commercial systems or as the result of research in this field. For each system, the general structure and the performance claimed by the authors are presented. In the last section, we will try to compare these systems and to identify issues that may foster the research in the field. Current Tools for Automated Essay Grading Project Essay Grade (PEG) PEG is one of the earliest and longest-lived implementations of automated essay grading. It was developed by Page and others (Hearst, 2000; Page, 1994, 1996) and primarily relies on style analysis of surface linguistic features of a block of text. …

---

LEVERAGING BLOCKCHAIN FOR ESG DISCLOSURE: ENHANCING TRANSPARENCY AND ACCOUNTABILITY IN SUSTAINABILITY REPORTING
10.29121/ijetmr.v12.i3.2025.1546
The growing need for Environmental, Social, and Governance (ESG) disclosure accountability and transparency has brought to light serious issues with conventional reporting structures. Problems including data discrepancies, greenwashing, and a lack of real-time verification make ESG disclosures less reliable. With its decentralized and unchangeable ledger, blockchain technology provides a revolutionary answer to these problems. Blockchain increases the legitimacy of ESG reporting by guaranteeing data integrity, improving transparency, and automating compliance through smart contracts.The potential of blockchain technology to overcome current constraints and enhance stakeholder trust is highlighted in this study's exploration of the technology's role in ESG disclosure. Several case studies where blockchain has been used to improve the accuracy of ESG data have been analyzed using a descriptive and exploratory research methodology. According to the results, blockchain reduces fraud reporting and expedites verification procedures, which lowers expenses and boosts operational effectiveness. But issues including excessive energy use, unclear regulations, and the requirement for defined ESG criteria continue to be major obstacles to broad implementation.Notwithstanding these obstacles, there are encouraging prospects for more developments in ESG reporting due to the integration of blockchain with cutting-edge technologies like artificial intelligence (AI) and the Internet of Things (IoT). In order to establish a strong framework for blockchain-driven ESG disclosures, this study emphasizes the need for cooperation between regulators, companies, and technology developers. Future studies should concentrate on resolving issues with scalability and creating laws that support the use of blockchain technology in sustainability reporting.

---

Comparative Evaluation of IndoBERT, IndoBERTweet, and mBERT for Multilabel Student Feedback Classification
10.29207/resti.v8i6.6100
Student feedback plays a crucial role in enhancing the quality of educational programs, yet analyzing this feedback, especially in informal contexts, remains challenging. In Indonesia, where student comments often include colloquial language and vary widely in content, effective multilabel classification is essential to accurately identify the aspects of courses being critiqued. Despite the development of several BERT-based models, the effectiveness of these models for classifying informal Indonesian text remains underexplored. Here we evaluate the performance of three BERT variants—IndoBERT, IndoBERTweet, and mBERT—on the task of multilabel classification of student feedback. Our experiments investigate the impact of different sequence lengths and truncation strategies on model performance. We find that IndoBERTweet, with a macro F1-score of 0.8462, outperforms IndoBERT (0.8243) and mBERT (0.8230) when using a sequence length of 64 tokens and truncation at the end. These findings suggest that IndoBERTweet is well-suited for handling the informal, abbreviated text common in Indonesian student feedback, providing a robust tool for educational institutions aiming for actionable insights from student comments.

---

Exploring a Large Language Model on the ChatGPT Platform for Indonesian Text Preprocessing Tasks
10.29244/ijsa.v9i1p100-116
Preprocessing is a crucial step in Natural Language Processing, especially for informal languages like Indonesian, which contain complex morphology, slang, abbreviations, and non-standard expressions. Traditional rule-based tools such as regex, IndoNLP, and Sastrawi are commonly used but often fall short in handling noisy, user-generated text. This study explores the capability of Large Language Model, particularly ChatGPT-o3, in performing Indonesian text preprocessing tasks, namely text cleaning, normalization, stopword removal, and stemming/lemmatization, and compares it to conventional rule-based approaches. Using two types of datasets, consisting of a small example dataset of five manually constructed sentences and a real-world dataset of 100 tweets about the Indonesian “Makan Bergizi Gratis” program, both preprocessing methods were applied and evaluated. Results show that ChatGPT-o3 performs equally well in text cleaning and significantly better in normalization. However, rule-based methods like IndoNLP and Sastrawi still outperform ChatGPT-o3 in stopword removal and stemming. These findings indicate that while ChatGPT-o3 demonstrates strong contextual understanding and linguistic flexibility, they may underperform in rigid, token-based operations without fine-tuning. This study provides initial insights into using Large Language Models as an alternative preprocessing engine for Indonesian text and highlights the need for hybrid approaches or improved prompt design in future applications.

---

Proposed Aspect Based Sentiment Analysis system for English reviews
10.29304/jqcm.2019.11.2.559
Reviews are a crucial source of opinions that may influence the decision in many areas. So there is a need for an algorithm that is efficient in understanding the aspects that the reviewers have focused on in their reviews and comments on social networks or other web applications. This paper submits a proposed approach for aspect-based sentiment analysis that consists of two steps; the firststep is by a proposedp_chunker algorithm for aspect extraction using Latent Dirchilet Analysis and noun phrase chunking, the second step is sentiment analysis using a proposed hybrid algorithm that depending on both lexicon and supervised sentiment analysis to specify the sentiment for extracted aspects. The proposed paradigm is tested using standard datasets from kaggle for both aspect extraction and sentiment analysis, the result show efficacy in the proposed method.

---

A Comparative Study of Attention Mechanisms in Deep Learning Models for Aspect-based Sentiment Analysis of Customer Reviews
10.30534/ijatcse/2025/011422025
Aspect-Based Sentiment Analysis (ABSA) refines conventional sentiment analysis by targeting specific aspects within customer reviews. However, existing deep learning approaches often fall short when multiple aspects such as price, quality, or service are discussed in a single review, leading to misclassifications. In this paper, we present a comparative study of four baseline models (Recurrent Neural Network, Convolutional Neural Network, Recursive Neural Network, and Memory Network) alongside four attention mechanisms (Self, Multihead, Global, and Hierarchical). Experiments on publicly available datasets reveal that while the RNN baseline achieves the best accuracy (73.1%) among non-attention models, incorporating attention substantially enhances performance. In particular, RNN with Hierarchical Attention attains the highest accuracy of 84.7%, highlighting its superiority in capturing both local (word-level) and global (sentence-level) dependencies. Global, Self and Multihead Attention boosts performance, but offer moderate gains. The study shows that adding attention mechanisms greatly improves multi-aspect sentiment classification. Self-Attention increased performance by an average of 2.5%, Multi-Head Attention by 6.8%, Global Attention by 10.2%, and Hierarchical Attention by 15.9%, making it the most effective. These findings underscore the importance of attention mechanisms for fine-grained sentiment tasks and inform researchers on selecting the most effective attention strategy for ABSA. We conclude by suggesting avenues for future work, including aspect-specific attention enhancements to further refine model accuracy.

---

Aspect-Based Sentiment Analysis for Enhanced Understanding of 'Kemenkeu' Tweets
10.30871/jaic.v8i2.8558
The perceptions and expressions shared by the public on social media play a crucial role in shaping the reputation of government institutions, such as the Ministry of Finance MOF (Kemenkeu) in Indonesia which also has faced increased scrutiny, particularly on Twitter. This study analyzes public sentiment towards the Indonesian Ministry of Finance (MoF) through Aspect-Based Sentiment Analysis (ABSA) on Twitter data. Using a dataset of 10,099 tweets from January to July 2024, this study combines IndoBERT for sentiment classification and Latent Dirichlet Allocation (LDA) for topic modeling. Here, LDA was tested across four scenarios that considered various combinations of stopwords removal and stemming techniques, resulting in coherence scores of 0.314256, 0.369636, 0.350285, and 0.541752. The most optimal results were achieved in the scenario of stopwords removal without stemming (with 0.314256 coherence score). The main results show: 1) Identification of four main topics related to MoF: Economy, Budget, Employees, and Tax; 2) The dominance of negative sentiment (6,837 tweets) compared to positive sentiment (198 tweets) across all topics; 3) The effectiveness of IndoBERT in handling the complexity of the Indonesian language, especially in interpreting context and language nuances; 4) The importance of proper preprocessing, with a scenario of removing stopwords without stemming resulting in the most relevant topics. This study provides valuable insights for MoF to understand public perception and identify areas that require special attention in public communication and policy.

---

SemEval-2014 Task 4: Aspect Based Sentiment Analysis
10.3115/v1/s14-2004
Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.

---

A Vector Space Approach for Aspect Based Sentiment Analysis
10.3115/v1/w15-1516
Vector representations for language has been shown to be useful in a number of Natural Language Processing tasks. In this paper, we aim to investigate the effectiveness of word vector representations for the problem of Aspect Based Sentiment Analysis. In particular, we target three sub-tasks namely aspect term extraction, aspect category detection, and aspect sentiment prediction. We investigate the effectiveness of vector representations over different text data and evaluate the quality of domain-dependent vectors. We utilize vector representations to compute various vectorbased features and conduct extensive experiments to demonstrate their effectiveness. Using simple vector based features, we achieve F1 scores of 79.91% for aspect term extraction, 86.75% for category detection, and the accuracy 72.39% for aspect sentiment prediction.

---

Using AI to Detect AI-Generated Research Papers
10.31219/osf.io/7rbds
On February 20, 2023, science fiction magazine Clarkesworld was forced to stop accepting author submissions. In the preceding weeks, magazine's editors had seen a sharp increase in submissions--approximately 70\% more than normal. The reason for this drastic increase in submissions was the consumerization of large language models such as ChatGPT and Bard. Clarkesworld is not alone in being subjected to this onslaught of AI-generated content churned out by industrious people hoping to turn a buck. Easy accessibility to powerful AI services have opened new workflows for generating content at scale with minimal effort and knowledge. For many in academia and scientific research, publication is the path to promotion, reputation and perhaps funding. Just as those who used AI as their personal science fiction ghostwriters, some unscrupulous researchers have and likely will continue to submit mostly AI-generated material as their own work. The research community deserves to know the linage and origin of content.

---

A Review of Financial Data Analysis Techniques for Unstructured Data in the Deep Learning Era: Methods, Challenges, and Applications
10.31219/osf.io/gdvbj_v1
Financial institutions are increasingly leveraging---such as text, audio, and images---to gain insights and competitive advantage. Deep learning (DL) has emerged as a powerful paradigm for analyzing these complex data types, transforming tasks like financial news analysis, earnings call interpretation, and document parsing. This paper provides a comprehensive academic review of deep learning techniques for unstructured financial data. We present a taxonomy of data types and DL methods, including natural language processing models, speech and audio processing frameworks, multimodal fusion approaches, and transformer-based architectures. We survey key applications ranging from sentiment analysis and market prediction to fraud detection, credit risk assessment, and beyond, highlighting recent advancements in each domain. Additionally, we discuss major challenges unique to financial settings, such as data scarcity and annotation cost, model interpretability and regulatory compliance, and the dynamic, non-stationary nature of financial data. We enumerate prominent datasets and benchmarks that have accelerated research, and identify research gaps and future directions. The review emphasizes the latest developments up to 2025, including the rise of large pre-trained models and multimodal learning, and outlines how these innovations are shaping the next generation of financial analytics.

---

Automated Classification and Trend Analysis of Large Language Model Survey Papers Using Machine Learning and Natural Language Processing Techniques
10.31224/3984
This study investigates the application of machine learning (ML) and natural language processing (NLP) techniques to classify academic survey papers into predefined taxonomy categories. The dataset, consisting of paper titles, summaries, release dates, taxonomy labels, and categories, was analyzed to uncover trends and patterns in the publication of research papers. Exploratory data analysis (EDA) revealed important insights through visualizations, such as publication trends over time, the distribution of taxonomy categories, and the most common terms used in paper summaries. Key NLP techniques, including Term Frequency-Inverse Document Frequency (TF-IDF), were employed to transform the textual data into numerical features, while one-hot encoding was applied to the categorical data. A Random Forest Classifier was trained on the extracted feature matrix to predict the taxonomy category of each paper. The model achieved promising accuracy, effectively capturing patterns in the dataset. The study also identified areas for future improvement, including addressing class imbalance and exploring more sophisticated models. These findings demonstrate the potential of ML and NLP for automating the classification of academic papers, providing a scalable solution for managing large collections of research literature while offering insights into publication dynamics and trends.

---

Exploring and Improving Robustness of Multi Task Deep Neural Networks via Domain Agnostic Defenses
10.31224/osf.io/du2vs
In this paper I explore the robustness of the Multi-Task Deep Neural Networks (MT-DNN) againstnon-targeted adversarial attacks across Natural Language Understanding (NLU) tasks as well assome possible ways to defend against them. Liu et al., have shown that the Multi-Task Deep NeuralNetwork, due to the regularization effect produced when training as a result of it’s cross task data, ismore robust than a vanilla BERT model trained only on one task (1.1%-1.5% absolute difference).I then show that although the MT-DNN has generalized better, making it easily transferable acrossdomains and tasks, it can still be compromised as after only 2 attacks (1-character and 2-character)the accuracy drops by 42.05% and 32.24% for the SNLI and SciTail tasks. Finally I propose a domainadaptable defense which restores the model’s accuracy (36.75% and 25.94% respectively) as opposedto a general purpose defense or an off-the-shelf spell checker.

---

The Performance of Large Language Models on Quantitative and Verbal Ability Tests: Initial Evidence and Implications for Unproctored High-stakes Testing
10.31234/osf.io/9cs23
Abstract. Unproctored assessments are widely used in pre-employment assessment. However, the recent emergence of widely accessible large language models (LLMs) poses challenges for unproctored personnel assessments, given that applicants may use them to artificially inflate their scores beyond their true abilities. This may be particularly concerning in cognitive ability testing, which is widely used and is less fakeable by humans than personality tests. Thus, this study compares the performance of LLMs on two common types of cognitive tests: quantitative ability and verbal ability. The particular tests investigated are used in real-world, high-stakes selection. We also examine the performance of the LLMs across different test formats (i.e., open-ended vs. multiple choice). Further, we contrast the performance of two LLMs (GPT 3.5 and GPT 4) across multiple prompt approaches and temperature settings. We find that the LLMs score much better, in terms of percentile scores, on the verbal ability test than the quantitative ability test, even when accounting for the test format. GPT 4 outperforms GPT 3.5 across both types of tests. Notably, although prompt approaches and temperature settings do affect LLM test performance, the effects are minor relative to differences across tests and language models. We provide recommendations for securing pre-employment testing against LLM influences. Additionally, we call for rigorous research investigating the prevalence of LLM usage in pre-employment testing as well as on how LLM usage influences selection test validity.

---

Trends and Forms of Greenwashing in the Fashion Industry: A Systematic Review
10.31274/itaa.17331
With its extensive network, supply chain, and manufacturing process, the fashion industry is accountable for actions that pollute the environment. To attract environmentally conscious consumers and remain environment friendly in every perspective, fashion brands' activities sometimes go against the sustainability concept (greenwashing) and continue to be controversial in many published publications. To understand the manifestations of greenwashing in the fashion sector, we used PRISMA to evaluate journal articles (n=103) from six databases between 2000 and 2023. Our study results, categorized into eight themes, indicate no trend in greenwashing. Varied types, methods, and traits common to the fashion sector occur at the product, service, and firm levels. We provide a framework demonstrating how many definition dimensions relate to unfavorable criticism in each content area and, ultimately, define greenwashing in the fashion industry. Furthermore, via several parts, we present future research agendas based on our content research deficit.&nbsp

---

A Short Guidance for SME Sustainability Reporting at the EU Level
10.31410/itema.2023.167
Environment, Social and Governance (ESG) related regulations such as the Non-Financial Reporting Directive (NFRD) or the upcoming Tax­onomy Regulation of the European Union (EU) combined with the Corporate Sustainability Reporting Directive (CSRD) had and will have a lasting impact on various market participants including small and medium-sized enterpris­es (SMEs). Among these market participants, SMEs face difficulties in comply­ing with sustainability reporting due to limited resources and the absence of SME-specific guidelines. To address this issue, a framework for SME sustain­ability reporting in the EU is proposed in this study. The framework consists of a seven-step process that helps companies and stakeholders prioritize rele­vant sustainability topics. The proposed framework is easy to implement and cost-effective, considering the unique characteristics and capabilities of SMEs. The study targets managing directors and decision-makers of German SMEs to assist them in preparing their sustainability reports. The minimum content elements for SME sustainability reporting include general disclosures, climate change, own workforce, and materiality assessment. The European Financial Reporting Advisory Group (EFRAG) plans to provide a digital questionnaire for SMEs to gather the necessary information. Proactive sustainability reporting can help SMEs improve stakeholder relations and optimize their internal pro­cesses, leading to competitive advantages and cost savings. However, it is im­portant to note that authenticity, high quality, and transparency in disclos­ing sustainability aspects are crucial to avoid reputational damage resulting from greenwashing.

---

Automatic Question Generation Monolingual Multilingual pre-trained Models using RNN and Transformer in Low Resource Indonesian Language
10.31449/inf.v46i7.4236
Although Indonesian is the fourth most frequently used language on the internet, the development of NLP in Indonesian has not been studied intensively. One form of NLP application classified as an NLG task is the Automatic Question Generation task. Generally, the task has proven well, using rule-based and cloze tests, but these approaches depend heavily on the defined rules. While this approach is suitable for automated question generation systems on a small scale, it can become less efficient as the scale of the system grows. Many NLG model architectures have recently proven to have significantly improved performance compared to previous architectures, such as generative pre-trained transformers, text-to-text transfer transformers, bidirectional autoregressive transformers, and many more. Previous studies on AQG in Indonesian were built on RNN-based architecture such as GRU, LSTM, and Transformer. The performance of models in previous studies is compared with state-of-the-art models, such as multilingual models mBART and mT5, and monolingual models such as IndoBART and IndoGPT. As a result, the fine-tuned IndoBART performed significantly higher than either BiGRU and BiLSTM on the SQuAD dataset. Fine-tuned IndoBART on most of the metrics also performed better on the TyDiQA dataset only, which has fewer population than the SQuAD dataset.

---

Public Perception of IKN as the Capital City of Indonesia After the Budget Blockage Issue through Mention and Sentiment Trend Analysis
10.31637/epsir-2026-1903
Introduction: IKN has become a concern for the Indonesian people both when it was a discourse and when it was implemented, even when the government changed. This study was conducted to examine the perception of the Indonesian people towards IKN. Methodology: This study is a qualitative descriptive study using the netnography method. The netnography method chosen is virtual ethnography, which is a type of research that focuses on social behavior in the digital realm via the internet. Result: Between January 24 and February 7, 2025, IKN received 10.7 thousand mentions with a reach of 110.1 million. Compared to the previous period (January 9-23, 2025), mentions decreased slightly (from 11.3 thousand), but the reach increased significantly (from 77.4 million). Positive sentiment remained stable at 4% of mentions, while negative sentiment rose from 10% to 12%. The peak of mentions occurred on February 3 (1.2 thousand mentions) and February 6 (1.1 thousand mentions), with February 6 also marking the highest daily reach of 22 million.

---

Development of a Novel Integrated Ontology-Based ESG Assessment Tool with AI Assistance for SMEs
10.3233/faia250736
Existing environmental, social, and governance (ESG) standards and guidelines are difficult to assess and compare the ESG performance without a standardized and integrated framework. A comprehensive assessment tool is required to integrate these standards and guidelines into a consistent list for assessment; this is especially true for SMEs. The problem arises as to how these ESG standards and guidelines can be connected and interrelated to each other. This paper therefore aims to review the existing ESG standards and guidelines, and to propose an integrated ontology-based ESG assessment tool. The research method involves empirical review, content analysis and pilot testing. The proposed instrument includes three modules of the UN Sustainable Development Group (UNSDG) model, an UNSDG maturity model, and an ESG ontology A questionnaire and a rule-based AI recommendation assistant are developed. The instrument’s validity and reliability testing are done with five companies (30 samples). The results indicate that the content validity has values of 0.909 for S-CVR, 0.964 for S-CVI/UA, and 0.982 for S-CVI/Ave. The construct validity has a Pearson’s R of 0.7737, convergent validity of 0.983, divergent validity of −0.1, and construct validity of 0.883. The face validity has a Pearson’s R of 0.7147 at a confidence level of 95%. The internal reliability has an alpha value of 0.902. Prepost reliability testing of the instrument shows no significant difference between the two periods (r = 0.965, p &lt; 0.05, N = 30), indicating that the instrument is well designed. The future assessment of the instrument will be extended to 30 company cases and 77 industries.

---

Bridging the gap between data mining and decision support: A case-based reasoning and ontology approach
10.3233/ida-2008-12205
Nowadays, decision makers invariably need to use decision support technology (DS) such as data mining (DM) methodologies and tools in order to tackle complex decision making problems. However the successful application of DM technology requires that one possess specific DM decision-making skills. For instance, the effective application of a data mining process is littered with many difficult and technical decisions (i.e. data cleansing, feature transformations, algorithms, parameters, evaluation, etc.) In essence, this contentious problem and burden for decision makers clearly stems from a poor DM-DS integration. As a result, we have strived to improve on this problem by proposing an intelligent DM assistant that can potentially empower decision makers to better leverage DM technology and achieve their intended business objectives. Nonetheless, as this paper will strive to demonstrate, the realization of an intelligent data mining assistant for the decision maker or non-specialist data miner is a challenging and complex endeavour. Hence, in what follows we present the key design considerations (i.e. knowledge representation and reasoning, knowledge elicitation and reuse efforts, etc.) that were addressed during the implementation of a hybrid data mining assistant, based on the case-based reasoning (CBR) paradigm and the use of a formal OWL-DL ontology.

---

A systematic review and research contributions on aspect-based sentiment analysis using twitter data
10.3233/idt-220063
Recently, Aspect-based Sentiment Analysis (ABSA) is considered a more demanding research topic that tries to discover the sentiment of particular aspects of the text. The key issue of this model is to discover the significant contexts for diverse aspects in an accurate manner. There will be variation among the sentiment of a few contexts based on their aspect, which stands as another challenging point that puts off the high performance. The major intent of this paper is to plan an analysis of ABSA using twitter data. The review is concentrated on a detailed analysis of diverse models performing the ABSA. Here, the main challenges and drawbacks based on ABSA baseline approaches are analyzed from the past 10 years’ references. Moreover, this review will also focus on analyzing different tools, and different data utilized by each contribution. Additionally, diverse machine learning is categorized according to their existence. This survey also points out the performance metrics and best performance values to validate the effectiveness of entire contributions. Finally, it highlights the challenges and research gaps to be addressed in modeling and learning about effectual, competent, and vigorous deep-learning algorithms for ABSA and pays attention to new directions for effective future research.

---

Aspect-based sentiment analysis with metaphorical information
10.3233/jifs-233077
Aspect-Based Sentiment Analysis (ABSA) has been the focus of increasing study in recent years. Previous research has demonstrated that incorporating syntactic information, such as dependency trees, can enhance ABSA performance. Despite the widespread use of metaphors in daily life to express emotions more vividly, few studies have integrated this literary device into ABSA. In this paper, we propose a novel ABSA model that utilizes Metaphor Identification Procedure (MIP) to encode both the sentence and aspect word as a single unit, thereby overcoming these limitations. Our experimental results demonstrate that our model achieves competitive performance in ABSA.

---

On the multiple roles of ontologies in explanations for neuro-symbolic AI
10.3233/nai-240754
There has been a renewed interest in symbolic AI in recent years. Symbolic AI is indeed one of the key enabling technologies for the development of neuro-symbolic AI systems, as it can mitigate the limited capabilities of black box deep learning models to perform reasoning and provide support for explanations. This paper discusses the different roles that explicit knowledge, in particular ontologies, can play in drawing intelligible explanations in neuro-symbolic AI. We consider three main perspectives in which ontologies can contribute significantly, namely reference modelling, common-sense reasoning, and knowledge refinement and complexity management. We overview some of the existing approaches in the literature, and we position them according to these three proposed perspectives. The paper concludes by discussing some open challenges related to the adoption of ontologies in explanations.

---

Who Is This Explanation for? Human Intelligence and Knowledge Graphs for eXplainable AI
10.3233/ssw200024
eXplainable AI focuses on generating explanations for the output of an AI algorithm to a user, usually a decision-maker. Such user needs to interpret the AI system in order to decide whether to trust the machine outcome. When addressing this challenge, therefore, proper attention should be given to produce explanations that are interpretable by the target community of users. In this chapter, we claim for the need to better investigate what constitutes a human explanation, i.e. a justification of the machine behaviour that is interpretable and actionable by the human decision makers. In particular, we focus on the contributions that Human Intelligence can bring to eXplainable AI, especially in conjunction with the exploitation of Knowledge Graphs. Indeed, we call for a better interplay between Knowledge Representation and Reasoning, Social Sciences, Human Computation and Human-Machine Cooperation research -- as already explored in other AI branches -- in order to support the goal of eXplainable AI with the adoption of a Human-in-the-Loop approach.

---

Aspect Based Sentiment Analysis
10.32628/ijsrst2293123
Sentiment analysis, which addresses the computational treatment of opinion, sentiment, and subjectivity in text, has received considerable attention in recent years. In contrast to the traditional coarse-grained sentiment analysis tasks, such as document-level sentiment classification, we are interested in the fine-grained aspect-based sentiment analysis that aims to identify aspects that users comment on and these aspects’ polarities. Aspect-based sentiment analysis relies heavily on syntactic features. However, the reviews that this task focuses on are natural and spontaneous, thus posing a challenge to syntactic parsers. In this paper, we address this problem by proposing a framework of adding a sentiment sentence compression (Sent Comp) step before performing the aspect-based sentiment analysis. We apply a discriminative conditional random field model, with certain special features, to automatically compress sentiment sentences. Sentiment analysis is contextual mining of text which identifies and extracts subjective information in textual data. sentiment analysis proves to be an incredible asset for users to extract essential information and assists organizations with understanding the social sentiment of their brand, product or service while monitoring online conversations.

---

Performance management systems for sustainability in SMEs: An interventionist approach
10.3280/maco2024-003003
To address global sustainability, engagement from every social sector is necessary. In this context, businesses can play a pivotal role. Therefore, it is urgent to establish proper management tools in order to support or-ganizations' top management in addressing these goals. While larger firms are making progress in measuring, managing and reporting sustainability performance, small and medium enterprises (SMEs) are lagging behind for various reasons – such as the lack of regulatory frameworks, standards, managerial competence and more. In OECD countries, SMEs account for 40% of private sector GDP. As such, their relevance and role in pursuing sustainability cannot be overlooked. This research aims to advance understanding of the development and implemen-tation of performance measurement and management (PMM) systems taking into account the sustainability dimensions of performance in SMEs. This is key to al-lowing businesses and their management to measure and monitor their impact at the economic, social and environmental levels. In order to do this, the research is based on action research and the incorporation of an interventionist approach. In particular, the authors have developed a sus-tainability PMM for a medium citrus company operating in the B-to-B market and based in Southern Italy.

---

An Empirical Study of AI-Generated Text Detection Tools
10.33140/amlai.04.02.03
"Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial intelligence (AI) text identification systems, including ""GPTkit

---

Open Information Extraction for Knowledge Representation
10.33540/1739
The field of Natural Language Processing (NLP) focuses on developing computational techniques to analyze and extract information from human language. With the exponential growth of unstructured textual data, NLP-based techniques have become essential for extracting valuable insights from this data. However, existing information extraction systems have limitations in terms of extracting valuable information without predefined relations or ontology and storing the extracted knowledge effectively. This Ph.D. thesis aims to enhance open information extraction methods to represent unstructured textual data efficiently and effectively. The first part of the research focuses on Open Information Extraction (OIE) systems and their challenges. Existing OIE methods, including pattern-based and machine learning-based approaches, as well as neural techniques, are analyzed to understand their limitations. A Bidirectional Gated Recurrent Unit (Bi-GRU) OIE model is proposed in Chapter 3, which utilizes contextualized word embeddings to extract relevant triples from unstructured text. Experimental results demonstrate the effectiveness of this model in generating high-quality relation triples. Chapter 4 addresses the lack of labeled data, a common problem in NLP tasks. The research extends the OIE model from Chapter 3 by using learned features to generate relation triples and explores the transferability of these features across different OIE domains and the related task of Relation Extraction (RE). The results show comparable performance with traditional training, indicating the potential of OIE in achieving NLP performance without labeled data. In Chapter 5, the focus shifts to enhancing pre-trained language models for taxonomy classification. Pre-trained language models often struggle with unseen patterns during inference, and the limited size of annotated data poses a challenge. A two-stage fine-tuning procedure, incorporating data augmentation techniques, is proposed to improve the generalizability of pre-trained models. Experimental results demonstrate strong generalizability on unseen data, with an F1 score of 91.25%. Chapter 6 explores the use of OIE for constructing a knowledge graph, specifically in the context of cyber threat intelligence. Open-CyKG, an open cyber threat intelligence knowledge graph framework, is designed using an attention-based neural OIE model and a Named Entity Recognition (NER) model. Refinement and canonicalization techniques are employed to overcome ambiguity and data redundancy during knowledge graph construction. The results show that querying the constructed knowledge graph can be done efficiently, highlighting the support of OIE in knowledge graph development. The proposed components achieve beyond-state-of-the-art results in terms of OIE performance, NER performance, and knowledge graph canonicalization. The research presented in the previous chapters demonstrates significant improvements in the efficiency and effectiveness of open information extraction methods for representing unstructured textual data. These advancements leverage techniques such as data augmentation, multi-stage fine-tuning, and pre-trained language models. The construction of knowledge graphs, enabled by OIE, has the potential to mimic human intelligence and benefit various complex applications, including recommender systems, search engines, and dialog systems.

---

Bringing Automatic Scoring into the Classroom - Measuring the Impact of Automated Analytic Feedback on Student Writing Performance
10.3384/ecp190008
While many methods for automatically scoring student writings have been proposed, few studies have inquired whether such scores constitute effective feedback improving learners’ writing quality. In this paper, we use an EFL email dataset annotated according to five analytic assessment criteria to train a classifier for each criterion, reaching human-machine agreement values (kappa) between .35 and .87. We then perform an intervention study with 112 lower secondary students in which participants in the feedback condition received stepwise automatic feedback for each criterion while students in the control group received only a description of the respective scoring criterion. We manually and automatically score the resulting revisions to measure the effect of automated feedback and find that students in the feedback condition improved more than in the control group for 2 out of 5 criteria. Our results are encouraging as they show that even imperfect automated feedback can be successfully used in the classroom.

---

Explainable Automated Essay Scoring: Deep Learning Really Has Pedagogical Value
10.3389/feduc.2020.572367
Automated essay scoring (AES) is a compelling topic in Learning Analytics for the primary reason that recent advances in AI find it as a good testbed to explore artificial supplementation of human creativity. However, a vast swath of research tackles AES only holistically; few have even developed AES models at the rubric level, the very first layer of explanation underlying the prediction of holistic scores. Consequently, the AES black box has remained impenetrable. Although several algorithms from Explainable Artificial Intelligence have recently been published, no research has yet investigated the role that these explanation models can play in: (a) discovering the decision-making process that drives AES, (b) fine-tuning predictive models to improve generalizability and interpretability, and (c) providing personalized, formative, and fine-grained feedback to students during the writing process. Building on previous studies where models were trained to predict both the holistic and rubric scores of essays, using the Automated Student Assessment Prize’s essay datasets, this study focuses on predicting the quality of the writing style of Grade-7 essays and exposes the decision processes that lead to these predictions. In doing so, it evaluates the impact of deep learning (multi-layer perceptron neural networks) on the performance of AES. It has been found that the effect of deep learning can be best viewed when assessing the trustworthiness of explanation models. As more hidden layers were added to the neural network, the descriptive accuracy increased by about 10%. This study shows that faster (up to three orders of magnitude) SHAP implementations are as accurate as the slower model-agnostic one. It leverages the state-of-the-art in natural language processing, applying feature selection on a pool of 1592 linguistic indices that measure aspects of text cohesion, lexical diversity, lexical sophistication, and syntactic sophistication and complexity. In addition to the list of most globally important features, this study reports (a) a list of features that are important for a specific essay (locally), (b) a range of values for each feature that contribute to higher or lower rubric scores, and (c) a model that allows to quantify the impact of the implementation of formative feedback.

---

Digital-First Learning and Assessment Systems for the 21st Century
10.3389/feduc.2022.857604
In the past few years, our lives have changed due to the COVID-19 pandemic; many of these changes resulted in pivoting our activities to a virtual environment, forcing many of us out of traditional face-to-face activities into digital environments. Digital-first learning and assessment systems (LAS) are delivered online, anytime, and anywhere at scale, contributing to greater access and more equitable educational opportunities. These systems focus on the learner or test-taker experience while adhering to the psychometric, pedagogical, and validity standards for high-stakes learning and assessment systems. Digital-first LAS leverage human-in-the-loop artificial intelligence to enable personalized experience, feedback, and adaptation; automated content generation; and automated scoring of text, speech, and video. Digital-first LAS are a product of an ecosystem of integrated theoretical learning and assessment frameworks that align theory and application of design and measurement practices with technology and data management, while being end-to-end digital. To illustrate, we present two examples—a digital-first learning tool with an embedded assessment, the Holistic Educational Resources and Assessment (HERA) Science, and a digital-first assessment, the Duolingo English Test.

---

Developing valid assessments in the era of generative artificial intelligence
10.3389/feduc.2024.1399377
Generative Artificial Intelligence (GAI) holds tremendous potential to transform the field of education because GAI models can consider context and therefore can be trained to deliver quick and meaningful evaluation of student learning outcomes. However, current versions of GAI tools have considerable limitations, such as social biases often inherent in the data sets used to train the models. Moreover, the GAI revolution comes during a period of moving away from memorization-based education systems toward supporting learners in developing the ability to apply knowledge and skills to solve real-world problems and explain real-world phenomena. A challenge in using GAI tools for scoring assessments aimed at fostering knowledge application is ensuring that these algorithms are scoring the same construct attributes (e.g., knowledge and skills) as a trained human scorer would score when evaluating student performance. Similarly, if using GAI tools to develop assessments, one needs to ensure that the goals of GAI-generated assessments are aligned with the vision and performance expectations of the learning environments for which these assessments are developed. Currently, no guidelines have been identified for assessing the validity of AI-based assessments and assessment results. This paper represents a conceptual analysis of issues related to developing and validating GAI-based assessments and assessment results to guide the learning process. Our primary focus is to investigate how to meaningfully leverage capabilities of GAI for developing assessments. We propose ways to evaluate the validity evidence of GAI-produced assessments and assessment scores based on existing validation approaches. We discuss future research avenues aimed at establishing guidelines and methodologies for assessing the validity of AI-based assessments and assessment results. We ground our discussion in the theory of validity outlined in the Standards for Educational and Psychological Testing by the American Educational Research Association and discuss how we envision building on the standards for establishing the validity of inferences made from the test scores in the context of GAI-based assessments.

---

Applying artificial intelligence techniques for predicting the environment, social, and governance (ESG) pillar score based on balance sheet and income statement data: A case of non-financial companies of USA, UK, and Germany
10.3389/fenvs.2022.975487
Due to globalization, environment, social, and governance (ESG) issues have gained importance over the last few decades. ESG is a worldwide issue, which clarifies that organizations throughout the world are lacking in contribution to the environment, society, and corporate governance characteristics for sustainable development. The problem of ESG spread over all stakeholders needs to be addressed. In this regard, rating agencies also have a close eye on ESG issues and have developed the methodology of score that aims to provide disclosure on ESG metrics which, in return, help investors and asset managers better differentiate between responsible and irresponsible companies. The ESG score has become an important tool among asset managers but is highly questioned in terms of reliability. The study objective was to develop machine learning algorithms to assess how balance sheet and income statement data impact the Thomson Reuters ESG score for non-financial public companies of USA, UK, and Germany from 2008 to 2020. In addition, the study also has an objective to assess which machine learning (ML) algorithm better predicts the ESG score using structural data, that is, return on assets (ROA), return on equity (ROE), earning per share (EPS), earnings before interest and taxes (EBIT), dividend yield, and net sales. The results concluded that balance sheet and income statement data are critical in explaining the ESG score, and the ANN algorithm outperforms with minimum RMSE and MAE values. All in all, the results of the study, based on the concept of artificial intelligence, bring suggestion for improvement to regulatory bodies, researchers, academia, practitioners, publicly listed companies around the globe, and last but not the least to the US, UK, and Germany markets. Moreover, it also provides suggestions for up-to-date compliance of ESG-relevant activities for boosting the firm performance.

---

Multi-level governance of low-carbon tourism in rural China: policy evolution, implementation pathways, and socio-ecological impacts
10.3389/fenvs.2024.1482713
Low-carbon rural tourism development presents a critical challenge for environmental governance in emerging economies, yet the multi-level dynamics of policy implementation remain underexplored. This study examines China’s approach to this challenge, employing a mixed-methods approach including analysis of 16 central and 559 provincial policy documents, case studies in 15 rural villages across five provinces (Hunan, Guangdong, Zhejiang, Shanxi, and Hainan), and a survey of 637 stakeholders. Our findings reveal a complex policy landscape characterized by evolving national frameworks, varied provincial adoption patterns influenced by economic, environmental, and institutional factors, and three distinct local implementation pathways: technology-driven, community-based, and policy-led. We uncover significant variations in stakeholder perceptions and socio-economic impacts across different contexts, highlighting the critical role of adaptive governance mechanisms and local contextual factors in determining policy effectiveness. This study contributes to policy diffusion and multi-level governance theories by demonstrating the intricate interplay between top-down directives and bottom-up innovations in shaping sustainable tourism outcomes. Based on these insights, we propose evidence-based policy recommendations emphasizing flexible, context-sensitive approaches and improved stakeholder engagement to enhance low-carbon rural tourism governance. This research provides valuable guidance for policymakers and practitioners working towards sustainable rural development and environmental conservation in China and other developing countries.

---

Financial Risk Management and Explainable, Trustworthy, Responsible AI
10.3389/frai.2022.779799
This perspective paper is based on several sessions by the members of the Round Table AI at FIRM1, with input from a number of external and international speakers. Its particular focus lies on the management of the model risk of productive models in banks and other financial institutions. The models in view range from simple rules-based approaches to Artificial Intelligence (AI) or Machine learning (ML) models with a high level of sophistication. The typical applications of those models are related to predictions and decision making around the value chain of credit risk (including accounting side under IFRS9 or related national GAAP approaches), insurance risk or other financial risk types. We expect more models of higher complexity in the space of anti-money laundering, fraud detection and transaction monitoring as well as a rise of AI/ML models as alternatives to current methods in solving some of the more intricate stochastic differential equations needed for the pricing and/or valuation of derivatives. The same type of model is also successful in areas unrelated to risk management, such as sales optimization, customer lifetime value considerations, robo-advisory, and other fields of applications. The paper refers to recent related publications from central banks, financial supervisors and regulators as well as other relevant sources and working groups. It aims to give practical advice for establishing a risk-based governance and testing framework for the mentioned model types and discusses the use of recent technologies, approaches, and platforms to support the establishment of responsible, trustworthy, explainable, auditable, and manageable AI/ML in production. In view of the recent EU publication on AI, also referred to as the EU Artificial Intelligence Act (AIA), we also see a certain added value for this paper as an instigator of further thinking outside of the financial services sector, in particular where “High Risk” models according to the mentioned EU consultation are concerned.

---

A generative AI-driven interactive listening assessment task
10.3389/frai.2024.1474019
Introduction Assessments of interactional competence have traditionally been limited in large-scale language assessments. The listening portion suffers from construct underrepresentation, whereas the speaking portion suffers from limited task formats such as in-person interviews or role plays. Human-delivered tasks are challenging to administer at large scales, while automated assessments are typically very narrow in their assessment of the construct because they have carried over the limitations of traditional paper-based tasks to digital formats. However, computer-based assessments do allow for more interactive, automatically administered tasks, but come with increased complexity in task creation. Large language models present new opportunities for enhanced automated item generation (AIG) processes that can create complex content types and tasks at scale that support richer assessments. Methods This paper describes the use of such methods to generate content at scale for an interactive listening measure of interactional competence for the Duolingo English Test (DET), a large-scale, high-stakes test of English proficiency. The Interactive Listening task assesses test takers’ ability to participate in a full conversation, resulting in a more authentic assessment of interactive listening ability than prior automated assessments by positing comprehension and interaction as purposes of listening. Results and discussion The results of a pilot of 713 tasks with hundreds of responses per task, along with the results of human review, demonstrate the feasibility of a human-in-the-loop, generative AI-driven approach for automatic creation of complex educational assessments at scale.

---

Testing the applicability of a governance checklist for high-risk AI-based learning outcome assessment in Italian universities under the EU AI act annex III
10.3389/frai.2025.1718613
Background The EU AI Act classifies AI-based learning outcome assessment as high-risk (Annex III, point 3b), yet sector-specific frameworks for institutional self-assessment remain underdeveloped. This creates accountability gaps affecting student rights and educational equity, as institutions lack systematic tools to demonstrate that algorithmic assessment systems produce valid and fair outcomes. Methods This exploratory study tests whether ALTAI’s trustworthy AI requirements can be operationalized for educational assessment governance through the XAI-ED Consequential Assessment Framework, which integrates three educational evaluation theories (Messick’s consequential validity, Kirkpatrick’s four-level model, Stufflebeam’s CIPP). Following pilot testing with three institutions, four independent coders applied a 27-item checklist to policy documents from 14 Italian universities (13% with formal AI policies plus one baseline case) using four-point ordinal scoring and structured consensus procedures. Results Intercoder reliability analysis revealed substantial agreement (Fleiss’s κ = 0.626, Krippendorff’s α = 0.838), with higher alpha reflecting predominantly adjacent-level disagreements suitable for exploratory validation. Analysis of 14 universities reveals substantial governance heterogeneity among early adopters (Institutional Index: 0.00–60.32), with Technical Robustness and Safety showing lowest implementation (M = 19.64, SD = 21.08) and Societal Well-being highest coverage (M = 52.38, SD = 29.38). Documentation prioritizes aspirational statements over operational mechanisms, with only 13% of Italian institutions having adopted AI policies by September 2025. Discussion The framework demonstrates feasibility for self-assessment but reveals critical misalignment: universities document aspirational commitments more readily than technical safeguards, with particularly weak capacity for validity testing and fairness monitoring. Findings suggest three interventions: (1) ministerial operational guidance translating EU AI Act requirements into educational contexts, (2) inter-institutional capacity-building addressing technical-pedagogical gaps, and (3) integration of AI governance indicators into national quality assurance systems to enable systematic accountability. The study contributes to understanding how educational evaluation theory can inform the translation of abstract trustworthy AI principles into outcome-focused institutional practices under high-risk classifications.

---

A Topic Modeling Comparison Between LDA, NMF, Top2Vec, and BERTopic to Demystify Twitter Posts
10.3389/fsoc.2022.886498
The richness of social media data has opened a new avenue for social science research to gain insights into human behaviors and experiences. In particular, emerging data-driven approaches relying on topic models provide entirely new perspectives on interpreting social phenomena. However, the short, text-heavy, and unstructured nature of social media content often leads to methodological challenges in both data collection and analysis. In order to bridge the developing field of computational science and empirical social research, this study aims to evaluate the performance of four topic modeling techniques; namely latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), Top2Vec, and BERTopic. In view of the interplay between human relations and digital media, this research takes Twitter posts as the reference point and assesses the performance of different algorithms concerning their strengths and weaknesses in a social science context. Based on certain details during the analytical procedures and on quality issues, this research sheds light on the efficacy of using BERTopic and NMF to analyze Twitter data.

---

On Assessing the Performance of LLMs for Target-Level Sentiment Analysis in Financial News Headlines
10.3390/a18010046
The importance of sentiment analysis in the rapidly evolving financial markets is widely recognized for its ability to interpret market trends and inform investment decisions. This study delves into the target-level financial sentiment analysis (TLFSA) of news headlines related to stock. The study compares the performance in the TLFSA task of various sentiment analysis techniques, including rule-based models (VADER), fine-tuned transformer-based models (DistilFinRoBERTa and Deberta-v3-base-absa-v1.1) as well as zero-shot large language models (ChatGPT and Gemini). The dataset utilized for this analysis, a novel contribution of this research, comprises 1476 manually annotated Bloomberg headlines and is made publicly available (due to copyright restrictions, only the URLs of Bloomberg headlines with the manual annotations are provided; however, these URLs can be used with a Bloomberg terminal to reconstruct the complete dataset) to encourage future research on this subject. The results indicate that the fine-tuned Deberta-v3-base-absa-v1.1 model performs better across all evaluation metrics than other evaluated models in TLFSA. However, LLMs such as ChatGPT-4, ChatGPT-4o, and Gemini 1.5 Pro provide similar performance levels without the need for task-specific fine-tuning or additional training. The study contributes to assessing the performance of LLMs for financial sentiment analysis, providing useful insights into their possible application in the financial domain.

---

Analyzing and Visualizing Text Information in Corporate Sustainability Reports Using Natural Language Processing Methods
10.3390/app12115614
Sustainability is a major contemporary issue that affects everyone. Many companies now produce an annual sustainability report, mainly intended for their stakeholders and the public, enumerating their goals and degrees of achievement regarding sustainable development. Although sustainability reports are an important resource to understand a company’s sustainability strategies and practices, the difficulty of extracting key information from dozens or hundreds of pages with sustainability and business jargon has highlighted the need for metrics to effectively measure the content of such reports. Accordingly, many researchers have attempted to analyze the concepts and messages from sustainability reports using various natural language processing (NLP) methods. In this study, we propose a novel approach that overcomes the shortcomings of previous studies. Using the sentence similarity method and sentiment analysis, the study clearly shows thematic practices and trends, as well as a significant difference in the balance of positive and negative information in the reports across companies. The results of sentiment analysis prove that the new approach of this study is very useful. It confirms that companies actively use the sustainability report to improve their positive image when they experience a crisis. It confirms that companies actively use the sustainability report to improve their positive image when they experience a crisis. The inferences gained from this method will not only help companies produce better reports that can be utilized effectively, but also provide researchers with ideas for further research. In the concluding section, we summarize the implications of our approach and discuss limitations and future research areas.

---

A Survey of Information Extraction Based on Deep Learning
10.3390/app12199691
As a core task and an important link in the fields of natural language understanding and information retrieval, information extraction (IE) can structure and semanticize unstructured multi-modal information. In recent years, deep learning (DL) has attracted considerable research attention to IE tasks. Deep learning-based entity relation extraction techniques have gradually surpassed traditional feature- and kernel-function-based methods in terms of the depth of feature extraction and model accuracy. In this paper, we explain the basic concepts of IE and DL, primarily expounding on the research progress and achievements of DL technologies in the field of IE. At the level of IE tasks, it is expounded from entity relationship extraction, event extraction, and multi-modal information extraction three aspects, and creates a comparative analysis of various extraction techniques. We also summarize the prospects and development trends in DL in the field of IE as well as difficulties requiring further study. It is believed that research can be carried out in the direction of multi-model and multi-task joint extraction, information extraction based on knowledge enhancement, and information fusion based on multi-modal at the method level. At the model level, further research should be carried out in the aspects of strengthening theoretical research, model lightweight, and improving model generalization ability.

---

Automatic Essay Scoring Method Based on Multi-Scale Features
10.3390/app13116775
Essays are a pivotal component of conventional exams; accurately, efficiently, and effectively grading them is a significant challenge for educators. Automated essay scoring (AES) is a complex task that utilizes computer technology to assist teachers in scoring. Traditional AES techniques only focus on shallow linguistic features based on the grading criteria, ignoring the influence of deep semantic features. The AES model based on deep neural networks (DNN) can eliminate the need for feature engineering and achieve better accuracy. In addition, the DNN-AES model combining different scales of essays has recently achieved excellent results. However, it has the following problems: (1) It mainly extracts sentence-scale features manually and cannot be fine-tuned for specific tasks. (2) It does not consider the shallow linguistic features that the DNN-AES cannot extract. (3) It does not contain the relevance between the essay and the corresponding prompt. To solve these problems, we propose an AES method based on multi-scale features. Specifically, we utilize Sentence-BERT (SBERT) to vectorize sentences and connect them to the DNN-AES model. Furthermore, the typical shallow linguistic features and prompt-related features are integrated into the distributed features of the essay. The experimental results show that the Quadratic Weighted Kappa of our proposed method on the Kaggle ASAP competition dataset reaches 79.3%, verifying the efficacy of the extended method in the AES task.

---

TwinStar: A Novel Design for Enhanced Test Question Generation Using Dual-LLM Engine
10.3390/app15063055
In light of the remarkable success of large language models (LLMs) in natural language understanding and generation, a trend of applying LLMs to professional domains with specialized requirements stimulates interest across various fields. It is desirable to further understand the level of intelligence that can be achieved by LLMs in solving domain-specific problems, as well as the resources that need to be invested accordingly. This paper studies the problem of generating high-quality test questions with specified knowledge points and target cognitive levels in AI-assisted teaching and learning. Our study shows that LLMs, even those as immense as GPT-4 or Bard, can hardly fulfill the design objectives, lacking clear focus on cognitive levels pertaining to specific knowledge points. In this paper, we explore the opportunity of enhancing the capability of LLMs through system design, instead of training models with substantial domain-specific data, consuming mass computing and memory resources. We propose a novel design scheme that orchestrates a dual-LLM engine, consisting of a question generation model and a cognitive-level evaluation model, built with fine-tuned, lightweight baseline models and prompting technology to generate high-quality test questions. The experimental results show that the proposed design framework, TwinStar, outperforms the state-of-the-art LLMs for effective test question generation in terms of cognitive-level adherence and knowledge relevance. TwinStar implemented with ChatGLM2-6B improves the cognitive-level adherence by almost 50% compared to Bard and 21% compared to GPT-4.0. The overall improvement in the quality of test questions generated by TwinStar reaches 12.0% compared to Bard and 2% compared with GPT-4.0 while our TwinStar implementation consumes only negligible memory space compared with that of GPT-4.0. An implementation of TwinStar using LLaMA2-13B shows a similar trend of improvement.

---

Toward Transparent Modeling: A Scoping Review of Explainability for Arabic Sentiment Analysis
10.3390/app151910659
The increasing prevalence of Arabic text in digital media offers significant potential for sentiment analysis. However, challenges such as linguistic complexity and limited resources make Arabic sentiment analysis (ASA) particularly difficult. In addition, explainable artificial intelligence (XAI) has become crucial for improving the transparency and trustworthiness of artificial intelligence (AI) models. This paper addresses the integration of XAI techniques in ASA through a scoping review of developments. This study critically identifies trends in model usage, examines explainability methods, and explores how these techniques enhance the explainability of model decisions. This review is crucial for consolidating fragmented efforts, identifying key methodological trends, and guiding future research in this emerging area. Online databases (IEEE Xplore, ACM Digital Library, Scopus, Web of Science, ScienceDirect, and Google Scholar) were searched to identify papers published between 1 January 2016 and 31 March 2025. The last search across all databases was conducted on 1 April 2025. From these, 19 peer-reviewed journal articles and conference papers focusing on ASA with explicit use of XAI techniques were selected for inclusion. This time frame was chosen to capture the most recent decade of research, reflecting advances in deep learning and the transformer-based and explainable AI methods. The findings indicate that transformer-based models and deep learning approaches dominate in ASA, achieving high accuracy, and that local interpretable model-agnostic explanations (LIME) is the most widely used explainability tool. However, challenges such as dialectal variation, small or imbalanced datasets, and the black box nature of advanced models persist. To address these challenges future research directions should include the creation of richer Arabic sentiment datasets, the development of hybrid explainability models, and the enhancement of adversarial robustness.

---

Explainable Aspect-Based Sentiment Analysis Using Transformer Models
10.3390/bdcc8110141
An aspect-based sentiment analysis (ABSA) aims to perform a fine-grained analysis of text to identify sentiments and opinions associated with specific aspects. Recently, transformers and large language models have demonstrated exceptional performance in detecting aspects and determining their associated sentiments within text. However, understanding the decision-making processes of transformers remains a significant challenge, as they often operate as black-box models, making it difficult to interpret how they arrive at specific predictions. In this article, we examine the performance of various transformers on ABSA and we employ explainability techniques to illustrate their inner decision-making processes. Firstly, we fine-tune several pre-trained transformers, including BERT, RoBERTa, DistilBERT, and XLNet, on an extensive set of data composed of MAMS, SemEval, and Naver datasets. These datasets consist of over 16,100 complex sentences, each containing a couple of aspects and corresponding polarities. The models were fine-tuned using optimal hyperparameters and RoBERTa achieved the highest performance, reporting 89.16% accuracy on MAMS and SemEval and 97.62% on Naver. We implemented five explainability techniques, LIME, SHAP, attention weight visualization, integrated gradients, and Grad-CAM, to illustrate how transformers make predictions and highlight influential words. These techniques can reveal how models use specific words and contextual information to make sentiment predictions, which can improve performance, address biases, and enhance model efficiency and robustness. These also point out directions for further focus on the analysis of models’ bias in combination with explainability methods, ensuring that explainability highlights potential biases in predictions.

---

IndoGovBERT: A Domain-Specific Language Model for Processing Indonesian Government SDG Documents
10.3390/bdcc8110153
Achieving the Sustainable Development Goals (SDGs) requires collaboration among various stakeholders, particularly governments and non-state actors (NSAs). This collaboration results in but is also based on a continually growing volume of documents that needs to be analyzed and processed in a systematic way by government officials. Artificial Intelligence and Natural Language Processing (NLP) could, thus, offer valuable support for progressing towards SDG targets, including automating the government budget tagging and classifying NSA requests and initiatives, as well as helping uncover the possibilities for matching these two categories of activities. Many non-English speaking countries, including Indonesia, however, face limited NLP resources, such as, for instance, domain-specific pre-trained language models (PTLMs). This circumstance makes it difficult to automate document processing and improve the efficacy of SDG-related government efforts. The presented study introduces IndoGovBERT, a Bidirectional Encoder Representations from Transformers (BERT)-based PTLM built with domain-specific corpora, leveraging the Indonesian government’s public and internal documents. The model is intended to automate various laborious tasks of SDG document processing by the Indonesian government. Different approaches to PTLM development known from the literature are examined in the context of typical government settings. The most effective, in terms of the resultant model performance, but also most efficient, in terms of the computational resources required, methodology is determined and deployed for the development of the IndoGovBERT model. The developed model is then scrutinized in several text classification and similarity assessment experiments, where it is compared with four Indonesian general-purpose language models, a non-transformer approach of the Multilabel Topic Model (MLTM), as well as with a Multilingual BERT model. Results obtained in all experiments highlight the superior capability of the IndoGovBERT model for Indonesian government SDG document processing. The latter suggests that the proposed PTLM development methodology could be adopted to build high-performance specialized PTLMs for governments around the globe which face SDG document processing and other NLP challenges similar to the ones dealt with in the presented study.

---

Aspect-Based Sentiment Analysis of Patient Feedback Using Large Language Models
10.3390/bdcc8120167
Online medical forums have emerged as vital platforms for patients to share their experiences and seek advice, providing a valuable, cost-effective source of feedback for medical service management. This feedback not only measures patient satisfaction and improves health service quality but also offers crucial insights into the effectiveness of medical treatments, pain management strategies, and alternative therapies. This study systematically identifies and categorizes key aspects of patient experiences, emphasizing both positive and negative sentiments expressed in their narratives. We collected a dataset of approximately 15,000 entries from various sections of the widely used medical forum, patient.info. Our innovative approach integrates content analysis with aspect-based sentiment analysis, deep learning techniques, and a large language model (LLM) to analyze these data. Our methodology is designed to uncover a wide range of aspect types reflected in patient feedback. The analysis revealed seven distinct aspect types prevalent in the feedback, demonstrating that deep learning models can effectively predict these aspect types and their corresponding sentiment values. Notably, the LLM with few-shot learning outperformed other models. Our findings enhance the understanding of patient experiences in online forums and underscore the utility of advanced analytical techniques in extracting meaningful insights from unstructured patient feedback, offering valuable implications for healthcare providers and medical service management.

---

Large Language Models in Bio-Ontology Research: A Review
10.3390/bioengineering12111260
Biomedical ontologies are critical for structuring domain knowledge and enabling integrative analyses in the life sciences. Traditional ontology development is labor-intensive, requiring extensive expert curation. Recent advances in artificial intelligence, particularly large language models (LLMs), have opened new possibilities to automate and enhance various aspects of bio-ontology research. This review article synthesizes findings from recent studies on LLM-assisted ontology creation, mapping, integration, and semantic search, while addressing challenges such as bias, reliability, and ethical concerns. We also discuss promising future directions and emerging trends that may further transform the way biomedical ontologies are developed, maintained, and used.

---

Adoption of Green Building Assessment Systems to Existing Buildings under Kazakhstani Conditions
10.3390/buildings11080325
The construction industry is an enormous economic sector with a profound economic, social, and environmental impact. The building sector is responsible for one-third of total energy consumption and, notably, construction activities account for 39% of the total carbon emissions in the world. Therefore, nowadays, the promotion of green building concepts is essential for all countries. Typically, the sustainability level of a building is evaluated by specified certification systems through rating assessment tools. The development of national assessment tools is necessary for the developing world due to environmental, social, and economic issues; consequently, a national assessment tool adopted under specific local conditions would provide a more precise assessment. This paper analyzes the rating system of BREEAM, LEED, CASBEE, and Green Globes certification systems and discusses their adoption with assessment measures for the existing buildings in Kazakhstan’s reality. The following main criteria were discussed during six roundtable sessions: sustainable site and landscape, energy and carbon footprint reduction, water and wastewater management, indoor environmental quality, sustainable building materials, commissioning, and maintenance. A set of assessment criteria and measures were suggested, and 43 existing buildings were assessed. Only eight buildings reached a high rating level. The “sustainable site and landscape” and “indoor environmental air quality” categories were the categories with the highest scores; otherwise, “energy and carbon footprint reduction” with “water and wastewater management” had the lowest average scores. One of the buildings was evaluated separately by several experts to check the consistency of the suggested assessment measures. This evaluation also provided insight into how the assessors’ knowledge and experience may change the overall rating scores obtained. The most critical issues for the existing buildings in Kazakhstan’s reality were discussed. Despite the widespread adoption of green certification methodology, the application of global certification systems in Kazakhstan remains complicated due to the lack of knowledge and limited awareness.

---

Replication of Real-World Evidence in Oncology Using Electronic Health Record Data Extracted by Machine Learning
10.3390/cancers15061853
Simple Summary Obtaining and structuring information about the characteristics, treatments, and outcomes of people living with cancer for research purposes is difficult and resource-intensive. Oftentimes, this information can only be found in electronic health records (EHRs). In response, researchers use natural language processing with machine learning (ML extraction) techniques to extract information at scale. This study evaluated the quality and fitness-for-use of EHR-derived oncology data curated using ML extraction, relative to the standard approach, abstraction by trained experts. Using patients with lung cancer from a real-world database, we performed replication analyses demonstrating common analyses conducted in observational research. Eligible patients were selected into biomarker- and treatment-defined cohorts, first with expert-abstracted then with ML-extracted data. The study’s results and conclusions were similar regardless of the data curation method used. These results demonstrate that high-performance ML-extracted variables trained on expert-abstracted data can achieve similar results as when using abstracted data, unlocking the ability to perform oncology research at scale. Abstract Meaningful real-world evidence (RWE) generation requires unstructured data found in electronic health records (EHRs) which are often missing from administrative claims; however, obtaining relevant data from unstructured EHR sources is resource-intensive. In response, researchers are using natural language processing (NLP) with machine learning (ML) techniques (i.e., ML extraction) to extract real-world data (RWD) at scale. This study assessed the quality and fitness-for-use of EHR-derived oncology data curated using NLP with ML as compared to the reference standard of expert abstraction. Using a sample of 186,313 patients with lung cancer from a nationwide EHR-derived de-identified database, we performed a series of replication analyses demonstrating some common analyses conducted in retrospective observational research with complex EHR-derived data to generate evidence. Eligible patients were selected into biomarker- and treatment-defined cohorts, first with expert-abstracted then with ML-extracted data. We utilized the biomarker- and treatment-defined cohorts to perform analyses related to biomarker-associated survival and treatment comparative effectiveness, respectively. Across all analyses, the results differed by less than 8% between the data curation methods, and similar conclusions were reached. These results highlight that high-performance ML-extracted variables trained on expert-abstracted data can achieve similar results as when using abstracted data, unlocking the ability to perform oncology research at scale.

---

Design and Implementation of Aspect-Based Sentiment Analysis Task
10.3390/cmsf2023008056
This paper presents the design and implementation of a deep-learning-based aspect-level sentiment analysis model for the aspect term extraction and sentiment polarity classification of Chinese text comments. The model utilizes two layers of BERT models and fully connected neural networks for feature extraction and classification. It also incorporates a context feature dynamic weighting strategy to focus on aspect words and enhance the model’s performance. Experimental results demonstrate that the proposed model performs well in aspect-level sentiment analysis tasks and effectively extracts sentiment information from the text. Additionally, to facilitate user interaction, a lightweight system is built, which enables model invocation and the visualization of analysis results, offering practical value.

---

An Interpretable Artificial Intelligence Approach for Reliability and Regulation-Aware Decision Support in Power Systems
10.3390/computation14010002
Modern medium-voltage (MV) distribution networks face increasing reliability challenges driven by aging assets, climate variability, and evolving operational demands. In Colombia and across Latin America, reliability metrics, such as the System Average Interruption Frequency Index (SAIFI), standardized under IEEE 1366, serve as key indicators for regulatory compliance and service quality. However, existing analytical approaches struggle to jointly deliver predictive accuracy, interpretability, and traceability required for regulated environments. Here, we introduce CRITAIR (Criticality Analysis through Interpretable Artificial Intelligence-based Recommendations), an integrated framework that combines predictive modeling, explainable analytics, and regulation-aware reasoning to enhance reliability management in MV networks. CRITAIR unifies three components: (i) a TabNet-based predictive module that estimates SAIFI using outage, asset, and meteorological data while producing global and local attributions; (ii) an agentic retrieval-and-reasoning stage that grounds recommendations in regulatory evidence from RETIE and NTC 2050; and (iii) interpretable reasoning graphs that map decision pathways. Evaluations conducted on real operational data demonstrate that CRITAIR achieves competitive predictive performance—comparable to Random Forest and XGBoost—while maintaining transparency through sparse attention and sequential feature explainability. Also, our regulation-aware reasoning module exhibits coherent and verifiable recommendations, achieving high semantic alignment scores (BERTScore) and expert-rated interpretability. Overall, CRITAIR bridges the gap between predictive analytics and regulatory governance, offering a transparent, auditable, and deployment-ready solution for digital transformation in electric distribution systems.

---

The Explainability of Transformers: Current Status and Directions
10.3390/computers13040092
An increasing demand for model explainability has accompanied the widespread adoption of transformers in various fields of applications. In this paper, we conduct a survey of the existing literature on the explainability of transformers. We provide a taxonomy of methods based on the combination of transformer components that are leveraged to arrive at the explanation. For each method, we describe its mechanism and survey its applications. We find out that attention-based methods, both alone and in conjunction with activation-based and gradient-based methods, are the most employed ones. A growing attention is also devoted to the deployment of visualization techniques to help the explanation process.

---

AI-Assisted Exam Variant Generation: A Human-in-the-Loop Framework for Automatic Item Creation
10.3390/educsci15081029
Educational assessment relies on well-constructed test items to measure student learning accurately, yet traditional item development is time-consuming and demands specialized psychometric expertise. Automatic item generation (AIG) offers template-based scalability, and recent large language model (LLM) advances promise to democratize item creation. However, fully automated approaches risk introducing factual errors, bias, and uneven difficulty. To address these challenges, we propose and evaluate a hybrid human-in-the-loop (HITL) framework for AIG that combines psychometric rigor with the linguistic flexibility of LLMs. In a Spring 2025 case study at Franklin University Switzerland, the instructor collaborated with ChatGPT (o4-mini-high) to generate parallel exam variants for two undergraduate business courses: Quantitative Reasoning and Data Mining. The instructor began by defining “radical” and “incidental” parameters to guide the model. Through iterative cycles of prompt, review, and refinement, the instructor validated content accuracy, calibrated difficulty, and mitigated bias. All interactions (including prompt templates, AI outputs, and human edits) were systematically documented, creating a transparent audit trail. Our findings demonstrate that a HITL approach to AIG can produce diverse, psychometrically equivalent exam forms with reduced development time, while preserving item validity and fairness, and potentially reducing cheating. This offers a replicable pathway for harnessing LLMs in educational measurement without sacrificing quality, equity, or accountability.

---

Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach
10.3390/electronics10182195
In recent years, the explainable artificial intelligence (XAI) paradigm is gaining wide research interest. The natural language processing (NLP) community is also approaching the shift of paradigm: building a suite of models that provide an explanation of the decision on some main task, without affecting the performances. It is not an easy job for sure, especially when very poorly interpretable models are involved, like the almost ubiquitous (at least in the NLP literature of the last years) transformers. Here, we propose two different transformer-based methodologies exploiting the inner hierarchy of the documents to perform a sentiment analysis task while extracting the most important (with regards to the model decision) sentences to build a summary as the explanation of the output. For the first architecture, we placed two transformers in cascade and leveraged the attention weights of the second one to build the summary. For the other architecture, we employed a single transformer to classify the single sentences in the document and then combine the probability scores of each to perform the classification and then build the summary. We compared the two methodologies by using the IMDB dataset, both in terms of classification and explainability performances. To assess the explainability part, we propose two kinds of metrics, based on benchmarking the models’ summaries with human annotations. We recruited four independent operators to annotate few documents retrieved from the original dataset. Furthermore, we conducted an ablation study to highlight how implementing some strategies leads to important improvements on the explainability performance of the cascade transformers model.

---

Ontology-Driven Architecture for Managing Environmental, Social, and Governance Metrics
10.3390/electronics13091719
The burgeoning significance of environmental, social, and governance (ESG) metrics in realms such as investment decision making, corporate reporting, and risk management underscores the imperative for a robust, comprehensive solution capable of effectively capturing, representing, and analysing the multifaceted and intricate ESG data landscape. Facing the challenge of aligning with diverse standards and utilising complex datasets, organisations require robust systems for the integration of ESG metrics with traditional financial reporting. Amidst this, the evolving regulatory landscape and the demand for transparency and stakeholder engagement present significant challenges, given the lack of standardized ESG metrics in certain areas. Recently, the use of ontology-driven architectures has gained attention for their ability to encapsulate domain knowledge and facilitate integration with decision-support systems. This paper proposes a knowledge graph in the ESG metric domain to assist corporations in cataloguing and navigating ESG reporting requirements, standards, and associated data. Employing a design science methodology, we developed an ontology that serves as both a conceptual foundation and a semantic layer, fostering the creation of an interoperable ESG Metrics Knowledge Graph (ESGMKG) and its integration within operational layers. This ontology-driven approach promises seamless integration with diverse ESG data sources and reporting frameworks, while addressing the critical challenges of metric selection, alignment, and data verification, supporting the dynamic nature of ESG metrics. The utility and effectiveness of the proposed ontology were demonstrated through a case study centred on the International Financial Reporting Standards (IFRS) framework that is widely used within the banking industry.

---

Differential Impacts of Environmental, Social, and Governance News Sentiment on Corporate Financial Performance in the Global Market: An Analysis of Dynamic Industries Using Advanced Natural Language Processing Models
10.3390/electronics13224507
This study examines how sentiment analysis of environmental, social, and governance (ESG) news affects the financial performance of companies in innovative sectors such as mobility, technology, and renewable energy. Using approximately 9828 general ESG articles from Google News and approximately 140,000 company-specific ESG articles, we performed term frequency-inverse document frequency (TF-IDF) analysis to identify key ESG-related terms and visualize their materiality across industries. We then applied models such as bidirectional encoder representations from transformers (BERT), the robustly optimized BERT pretraining approach (RoBERTa), and big bidirectional encoder representations from transformers (BigBird) for multiclass sentiment analysis, and distilled BERT (DistilBERT), a lite BERT (ALBERT), tiny BERT (TinyBERT), and efficiently learning an encoder that classifies token replacements accurately (ELECTRA) for positive and negative sentiment identification. Sentiment analysis results were correlated with profitability, cash flow, and stability indicators over a three-year period (2019–2021). ESG ratings from Morgan Stanley Capital International (MSCI), a prominent provider that evaluates companies’ sustainability practices, further enriched our analysis. The results suggest that sentiment impacts financial performance differently across industries; for example, positive sentiment correlates with financial success in mobility and renewable energy, while consumer goods often show positive sentiment even with low environmental ESG scores. The study highlights the need for industry-specific ESG strategies, especially in dynamic sectors, and suggests future research directions to improve the accuracy of ESG sentiment analysis.

---

Awareness of the Impact of IT/AI on Energy Consumption in Enterprises: A Machine Learning-Based Modelling Towards a Sustainable Digital Transformation
10.3390/en18215573
The integration of artificial intelligence (AI) and information technology (IT) is transforming business operations while increasing energy demand. A scalable and nonintrusive method for assessing the adoption of energy-conscious IT governance without direct measurements of energy use is lacking. To address this gap, a machine learning framework is developed and validated that infers the presence of energy-conscious IT governance from five indicators of digital maturity and AI adoption. Enterprise survey data were used to train five classification algorithms—support vector machine, logistic regression, decision tree, neural network, and k-nearest neighbors—to identify organizations implementing energy-efficient IT/AI management. All models achieved strong predictive performance, with SVM achieving 90% test accuracy and an F1 score of 89.8%. The findings demonstrate that an enterprise’s technological profile can serve as a reliable proxy for assessing sustainable IT/AI practices, enabling rapid assessment, benchmarking, and targeted support for green digital transformation. This approach offers significant implications for policy design, ESG reporting, and managerial decision-making in energy-conscious governance, supporting the alignment of digital innovation with environmental objectives.

---

Improving the Accuracy of Firm Failure Forecasting Using Non-Financial Variables: The Case of Croatian SME
10.3390/engproc2023039062
Empirical findings based on a bivariate logistic regression model with two SME categories (successful and failed) indicate that by adding non-financial indicators to the model based on financial variables, the accuracy of forecasting increases significantly. Namely, the total classification error decreases by an average of 26.99%, while the AUROC value increases by an average of 7.33%. In the additional model, with three firm categories (successful, sensitive, and failed), the findings reveal that one financial variable (self-financing) and three non-financial variables (orderly settlement of obligations, export, and age) significantly explain the occurrence of the early stage of SME failure.

---

Ontology-based Feature Selection: A Survey
10.3390/fi13060158
The Semantic Web emerged as an extension to the traditional Web, adding meaning (semantics) to a distributed Web of structured and linked information. At its core, the concept of ontology provides the means to semantically describe and structure information, and expose it to software and human agents in a machine and human-readable form. For software agents to be realized, it is crucial to develop powerful artificial intelligence and machine-learning techniques, able to extract knowledge from information sources, and represent it in the underlying ontology. This survey aims to provide insight into key aspects of ontology-based knowledge extraction from various sources such as text, databases, and human expertise, realized in the realm of feature selection. First, common classification and feature selection algorithms are presented. Then, selected approaches, which utilize ontologies to represent features and perform feature selection and classification, are described. The selective and representative approaches span diverse application domains, such as document classification, opinion mining, manufacturing, recommendation systems, urban management, information security systems, and demonstrate the feasibility and applicability of such methods. This survey, in addition to the criteria-based presentation of related works, contributes a number of open issues and challenges related to this still active research topic.

---

Detecting Topic and Sentiment Trends in Physician Rating Websites: Analysis of Online Reviews Using 3-Wave Datasets
10.3390/ijerph18094743
(1) Background: Physician rating websites (PRWs) are a rich resource of information where individuals learn other people response to various health problems. The current study aims to investigate and analyze the people top concerns and sentiment dynamics expressed in physician online reviews (PORs). (2) Methods: Text data were collected from four U.S.-based PRWs during the three time periods of 2018, 2019 and 2020. Based on the dynamic topic modeling, hot topics related to different aspects of healthcare were identified. Following the hybrid approach of aspect-based sentiment analysis, the social network of prevailing topics was also analyzed whether people expressed positive, neutral or negative sentiments in PORs. (3) Results: The study identified 30 dominant topics across three different stages which lead toward four key findings. First, topics discussed in Stage III were quite different from the earlier two stages due to the COVID-19 outbreak. Second, based on the keyword co-occurrence analysis, the most prevalent keywords in all three stages were related to the treatment, questions asked by patients, communication problem, patients’ feelings toward the hospital environment, disease symptoms, time spend with patients and different issues related to the COVID-19 (i.e., pneumonia, death, spread and cases). Third, topics related to the provider service quality, hospital servicescape and treatment cost were the most dominant topics in Stages I and II, while the quality of online information regarding COVID-19 and government countermeasures were the most dominant topics in Stage III. Fourth, when zooming into the topic-based sentiments analysis, hot topics in Stage I were mostly positive (joy be the dominant emotion), then negative (disgust be the dominant emotion) in Stage II. Furthermore, sentiments in the initial period of Stage III (COVID-19) were negative (anger be the dominant emotion), then transformed into positive (trust be the dominant emotion) later. The findings also revealed that the proposed method outperformed the conventional machine learning models in analyzing topic and sentiment dynamics expressed in PRWs. (4) Conclusions: Methodologically, this research demonstrates the ability and importance of computational techniques for analyzing large corpora of text and complementing conventional social science approaches.

---

Few-Shot Methods for Aspect-Level Sentiment Analysis
10.3390/info15110664
In this paper, we explore the approaches to the problem of cross-domain few-shot classification of sentiment aspects. By cross-domain few-shot, we mean a setting where the model is trained on large data in one domain (for example, hotel reviews) and is intended to perform on another (for example, restaurant reviews) with only a few labelled examples in the target domain. We start with pre-trained monolingual language models. Using the Polish language dataset AspectEmo, we compare model training using standard gradient-based learning to a zero-shot approach and two dedicated few-shot methods: ProtoNet and NNShot. We find both dedicated methods much superior to both gradient learning and zero-shot setup, with a small advantage held by NNShot. Overall, we find few-shot to be a compelling alternative, achieving a surprising amount of performance compared to gradient training on full-size data.

---

Aspect-Enhanced Prompting Method for Unsupervised Domain Adaptation in Aspect-Based Sentiment Analysis
10.3390/info16050411
This study proposes an Aspect-Enhanced Prompting (AEP) method for unsupervised Multi-Source Domain Adaptation in Aspect Sentiment Classification, where data from the target domain are completely unavailable for model training. The proposed AEP is based on two generative language models: one generates a prompt from a given review, while the other follows the prompt and classifies the sentiment of an aspect. The first model extracts Aspect-Related Features (ARFs), which are words closely related to the aspect, from the review and incorporates them into the prompt in a domain-agnostic manner, thereby directing the second model to identify the sentiment accurately. Our framework incorporates an innovative rescoring mechanism and a cluster-based prompt expansion strategy. Both are intended to enhance the robustness of the generation of the prompt and the adaptability of the model to diverse domains. The results of experiments conducted on five datasets (Restaurant, Laptop, Device, Service, and Location) demonstrate that our method outperforms the baselines, including a state-of-the-art unsupervised domain adaptation method. The effectiveness of both the rescoring mechanism and the cluster-based prompt expansion is also validated through an ablation study.

---

Ensemble Large Language Models: A Survey
10.3390/info16080688
Large language models (LLMs) have transformed the field of natural language processing (NLP), achieving state-of-the-art performance in tasks such as translation, summarization, and reasoning. Despite their impressive capabilities, challenges persist, including biases, limited interpretability, and resource-intensive training. Ensemble learning, a technique that combines multiple models to improve performance, presents a promising avenue for addressing these limitations in LLMs. This review explores the emerging field of ensemble LLMs, providing a comprehensive analysis of current methodologies, applications across diverse domains, and existing challenges. By reviewing ensemble strategies and evaluating their effectiveness, this paper highlights the potential of ensemble LLMs to enhance robustness and generalizability while proposing future research directions to advance the field.

---

Fiscal Management and Artificial Intelligence as Strategies to Combat Corruption in Colombia
10.3390/info16110998
Corruption in Colombia remains a critical barrier to development, institutional trust, and equitable access to public services, despite legislative efforts such as the Anti-Corruption Statute. This article explores the intersection between fiscal management and artificial intelligence (AI) as integrated strategies for enhancing transparency, accountability, and risk assessment in public administration. Drawing on theoretical frameworks and empirical data from 2020 to 2022, this study analyzes the scale and impact of corruption and the effectiveness of oversight mechanisms led by the Comptroller General of the Republic (CGR). A key innovation examined is the implementation of a GPT-based scoring model that automates the evaluation of internal accounting controls in 219 public entities. By leveraging AI to support fiscal audits, Colombia demonstrates a scalable approach to modernizing anti-corruption practices. The study concludes with policy recommendations that emphasize digital transformation, institutional strengthening, citizen engagement, and capacity building to improve fiscal governance and reduce corruption.

---

Secure and Transparent Banking: Explainable AI-Driven Federated Learning Model for Financial Fraud Detection
10.3390/jrfm18040179
The increasing sophistication of fraud has rendered rule-based fraud detection obsolete, exposing banks to greater financial risk, reputational damage, and regulatory penalties. Financial stability, customer trust, and compliance are increasingly threatened as centralized Artificial Intelligence (AI) models fail to adapt, leading to inefficiencies, false positives, and undetected detection. These limitations necessitate advanced AI solutions for banks to adapt properly to emerging fraud patterns. While AI enhances fraud detection, its black-box nature limits transparency, making it difficult for analysts to trust, validate, and refine decisions, posing challenges for compliance, fraud explanation, and adversarial defense. Effective fraud detection requires models that balance high accuracy and adaptability to emerging fraud patterns. Federated Learning (FL) enables distributed training for fraud detection while preserving data privacy and ensuring legal compliance. However, traditional FL approaches operate as black-box systems, limiting the analysts to trust, verify, or even improve the decisions made by AI in fraud detection. Explainable AI (XAI) enhances fraud analysis by improving interpretability, fostering trust, refining classifications, and ensuring compliance. The integration of XAI and FL forms a privacy-preserving and explainable model that enhances security and decision-making. This research proposes an Explainable FL (XFL) model for financial fraud detection, addressing both FL’s security and XAI’s interpretability. With the help of Shapley Additive Explanations (SHAP) and LIME, analysts can explain and improve fraud classification while maintaining privacy, accuracy, and compliance. The proposed model is trained on a financial fraud detection dataset, and the results highlight the efficiency of detection and successful elimination of false positives and contribute to the improvement of the existing models as the proposed model attained 99.95% accuracy and a miss rate of 0.05%, paving the way for a more effective and comprehensive AI-based system to detect potential fraudulence in banking.

---

The Impact of AI-Integrated ESG Reporting on Firm Valuation in Emerging Markets: A Multimodal Analytical Approach
10.3390/jrfm18120675
This study examines the impact of Artificial Intelligence (AI)-enhanced Environmental, Social, and Governance (ESG) reporting on firm valuation in emerging markets. It aims to explore how AI integration enhances the interpretability and predictive accuracy of ESG metrics in shaping market perceptions and investor decisions. This study employs a panel dataset from 2018 to 2024, analysing publicly listed non-financial firms across five major sectors: manufacturing, energy, telecommunications, consumer goods, and industrials. This study contributed by employing AI-powered multimodal analysis with conventional ESG scoring methods and integrating Fixed-Effects Regression with machine learning (ML) algorithms including Extreme Gradient Boosting (XGBoost) and Random Forest to identify complex, non-linear relationships within ESG data and firm valuation. The results show empirical evidence that integrating ML enhances the explanatory power of ESG data. Findings indicate that ESG performance is positively correlated with higher market valuations, particularly in Environmental and Social dimensions. Governance metrics are more inconsistent, which may be due to heterogeneity in governance practices, regulatory enforcement and the challenges of quantifying governance quality beyond compliance indicators across the focus emerging markets. Firms identified in ESG controversies tend to face valuation penalties, which stresses market sensitivity to reputational risks. ML algorithms outperform conventional techniques in predictive accuracy, revealing complex, non-linear interactions within ESG data. This study contributes to both the academic literature and practice showing how next-generation ESG reporting can robust valuation models, address limitations of conventional ESG scoring, and ensure a reliable outlook for investors and policymakers of industries in emerging markets.

---

Mapping ESG Trends by Distant Supervision of Neural Language Models
10.3390/make2040025
The integration of Environmental, Social and Governance (ESG) considerations into business decisions and investment strategies have accelerated over the past few years. It is important to quantify the extent to which ESG-related conversations are carried out by companies so that their impact on business operations can be objectively assessed. However, profiling ESG language is challenging due to its multi-faceted nature and the lack of supervised datasets. This research study aims to detect historical trends in ESG discussions by analyzing the transcripts of corporate earning calls. The proposed solution exploits recent advances in neural language modeling to understand the linguistic structure in ESG discourse. In detail, firstly we develop a classification model that categorizes the relevance of a text sentence to ESG. A pre-trained language model is fine-tuned on a small corporate sustainability reports dataset for this purpose. The semantic knowledge encoded in this classification model is then leveraged by applying it to the sentences in the conference transcripts using a novel distant-supervision approach. Extensive empirical evaluations against various pretraining techniques demonstrate the efficacy of the proposed transfer learning framework. Our analysis indicates that in the last 5 years, nearly 15% of the discussions during earnings calls pertained to ESG, implying that ESG factors are integral to business strategy.

---

Effective Methods of Categorical Data Encoding for Artificial Intelligence Algorithms
10.3390/math12162553
It is known that artificial intelligence algorithms are based on calculations performed using various mathematical operations. In order for these calculation processes to be carried out correctly, some types of data cannot be fed directly into the algorithms. In other words, numerical data should be input to these algorithms, but not all data in datasets collected for artificial intelligence algorithms are always numerical. These data may not be quantitative but may be important for the study under consideration. That is, these data cannot be thrown away. In such a case, it is necessary to transfer categorical data to numeric type. In this research work, 14 encoding methods of transforming of categorical data were considered. At the same time, conclusions are given about the general conditions of using these methods. During the research, categorical data in the dataset that were collected in order to assess whether it is possible to give credit to customers will be transformed based on 14 methods. After applying each encoding method, experimental tests are conducted based on the classification algorithm, and they are evaluated. At the end of the study, the results of the experimental tests are discussed and research conclusions are presented.

---

Understanding Public Opinion towards ESG and Green Finance with the Use of Explainable Artificial Intelligence
10.3390/math12193119
This study leverages explainable artificial intelligence (XAI) techniques to analyze public sentiment towards Environmental, Social, and Governance (ESG) factors, climate change, and green finance. It does so by developing a novel multi-task learning framework combining aspect-based sentiment analysis, co-reference resolution, and contrastive learning to extract nuanced insights from a large corpus of social media data. Our approach integrates state-of-the-art models, including the SenticNet API, for sentiment analysis and implements multiple XAI methods such as LIME, SHAP, and Permutation Importance to enhance interpretability. Results reveal predominantly positive sentiment towards environmental topics, with notable variations across ESG categories. The contrastive learning visualization demonstrates clear sentiment clustering while highlighting areas of uncertainty. This research contributes to the field by providing an interpretable, trustworthy AI system for ESG sentiment analysis, offering valuable insights for policymakers and business stakeholders navigating the complex landscape of sustainable finance and climate action. The methodology proposed in this paper advances the current state of AI in ESG and green finance in several ways. By combining aspect-based sentiment analysis, co-reference resolution, and contrastive learning, our approach provides a more comprehensive understanding of public sentiment towards ESG factors than traditional methods. The integration of multiple XAI techniques (LIME, SHAP, and Permutation Importance) offers a transparent view of the subtlety of the model’s decision-making process, which is crucial for building trust in AI-driven ESG assessments. Our approach enables a more accurate representation of public opinion, essential for informed decision-making in sustainable finance. This paper paves the way for more transparent and explainable AI applications in critical domains like ESG.

---

Nested Sentiment Analysis for ESG Impact: Leveraging FinBERT to Predict Market Dynamics Based on Eco-Friendly and Non-Eco-Friendly Product Perceptions with Explainable AI
10.3390/math12213332
In the current era, the environmental component of ESG is recognized as a major driver due to the pressing challenges posed by climate change, population growth, global warming, and shifting weather patterns. The environment must be considered a critical factor, and as evidenced by existing research, it is regarded as the dominant component within ESG. In this study, the ESG score is derived primarily from the environmental score. The increasing importance of the environmental, social, and governance (ESG) factors in financial markets, along with the growing need for sentiment analysis in sustainability, has necessitated the development of advanced sentiment analysis techniques. A predictive model has been introduced utilizing a nested sentiment analysis framework, which classifies sentiments towards eco-friendly and non-eco-friendly products, as well as positive and negative sentiments, using FinBERT. The model has been optimized with the AdamW optimizer, L2 regularization, and dropout to assess how sentiments related to these product types influence ESG metrics. The “black-box” nature of the model has been addressed through the application of explainable AI (XAI) to enhance its interpretability. The model demonstrated an accuracy of 91.76% in predicting ESG scores and 99% in sentiment classification. The integration of XAI improves the transparency of the model’s predictions, making it a valuable tool for decision-making in making sustainable investments. This research is aligned with the United Nations’ Sustainable Development Goals (SDG 12 and SDG 13), contributing to the promotion of sustainable practices and fostering improved market dynamics.

---

Sparse Attention-Based Residual Joint Network for Aspect-Category-Based Sentiment Analysis
10.3390/math13152437
Aspect-based sentiment analysis (ABSA) aims at identifying the sentiment polarity for a particular aspect in a review. ABSA studies based on deep learning models have exploited the attention mechanism to detect aspect-related parts. Conventional softmax-based attention mechanisms generate dense distributions, which may limit performance in tasks that inherently require sparsity. Recent studies on sparse attention transformation functions have demonstrated their effectiveness over the conventional softmax function. However, these studies primarily focus on highly sparse tasks based on self-attention architectures, leaving their applicability to the ABSA domain unexplored. In addition, most ABSA research has focused on leveraging aspect terms despite the usefulness of aspect categories. To address these issues, we propose a sparse-attention-based residual joint network (SPA-RJ Net) for the aspect-category-based sentiment analysis (ACSA) task. SPA-RJ Net incorporates two aspect-guided sparse attentions—sparse aspect-category attention and sparse aspect-sentiment attention—that introduce sparsity in attention via a sparse distribution transformation function, enabling the model to selectively focus on aspect-related information. In addition, it employs a residual joint learning framework that connects the aspect category detection (ACD) task module and the ACSA task module via residual connections, enabling the ACSA module to receive explicit guidance on relevant aspect categories from the ACD module. Our experiment validates that SPA-RJ Net consistently outperforms existing models, demonstrating the effectiveness of sparse attention and residual joint learning for aspect category-based sentiment classification.

---

ESG Narrative Quality in Green Bond Disclosures: Implications for Risk Perception, Transparency, and Market Trust
10.3390/risks14010001
This research evaluates the extent to which firms’ “green” bond disclosures create and convey a meaningful representation of their Environmental, Social, and Governance (“ESG”) commitments. Additionally, this research explores how investors distinguish between disclosures that represent genuine commitment to sustainability and those that may be indicative of “greenwashing,” and how such distinctions impact their assessment of an issuer’s credibility as well as the issuer’s performance subsequent to the issuance of a “green” bond. The methodology employed in this research employs a convergent mixed-methods approach that combines quantitative methods (Natural Language Processing (“NLP”), financial modeling, etc.) with qualitative methodologies (case studies, interviews). The NLP methodology employed in this research includes sentiment analysis, topic modeling, and ambiguity measurement in order to determine the tone, thematic content, and linguistic clarity of the disclosure texts. Subsequently, the results of the NLP methodologies are correlated with firm level outcomes using cross validated partial least squares regression (“PLS-R”), event study methodologies, and one way ANOVA to test for temporal and industrial variability. Finally, the results of the computational and financial methodologies are supplemented by qualitative case studies and interviews to provide context for the patterns identified in the computational and financial methodologies. In summary, the results of this research demonstrate that firms that communicate in a clear, balanced, and verifiable manner experience better market reaction and more favorable accounting results subsequent to the issuance of a “green” bond than do firms whose communications are vague, overly optimistic, or lacking in consistency. Conversely, the findings suggest that investors have become increasingly sensitive to potential “greenwashing” and therefore are less likely to respond favorably to communications characterized by the aforementioned characteristics.

---

Hierarchical Fusion Network with Enhanced Knowledge and Contrastive Learning for Multimodal Aspect-Based Sentiment Analysis on Social Media
10.3390/s23177330
Aspect-based sentiment analysis (ABSA) is a task of fine-grained sentiment analysis that aims to determine the sentiment of a given target. With the increased prevalence of smart devices and social media, diverse data modalities have become more abundant. This fuels interest in multimodal ABSA (MABSA). However, most existing methods for MABSA prioritize analyzing the relationship between aspect–text and aspect–image, overlooking the semantic gap between text and image representations. Moreover, they neglect the rich information in external knowledge, e.g., image captions. To address these limitations, in this paper, we propose a novel hierarchical framework for MABSA, known as HF-EKCL, which also offers perspectives on sensor development within the context of sentiment analysis. Specifically, we generate captions for images to supplement the textual and visual features. The multi-head cross-attention mechanism and graph attention neural network are utilized to capture the interactions between modalities. This enables the construction of multi-level aspect fusion features that incorporate element-level and structure-level information. Furthermore, for this paper, we integrated modality-based and label-based contrastive learning methods into our framework, making the model learn shared features that are relevant to the sentiment of corresponding words in multimodal data. The results, based on two Twitter datasets, demonstrate the effectiveness of our proposed model.

---

Sustainability Reporting as a Mixture of CSR and Sustainable Development. A Model for Micro-Enterprises within the Romanian Forestry Sector
10.3390/su12020603
In the last decades, the issue of the behavior geared towards society and the environment of small and medium-sized enterprises (SMEs) has created a new niche for economic researches. Most studies point out that entities operating in the forestry sector, despite having difficulties in applying valid corporate social responsibility (CSR) instruments, are concerned about the role they play in society. Therefore, they tend to develop their business by giving importance to the principles of sustainable development. The aim of the paper was to propose an econometric model to report the sustainability of non-financial performance for the companies operating in the forestry field. The main objectives of the study focused on defining and analyzing the studied problem through the specialized literature, defining and conceptualizing the statistical model in order to identify the risk factors and vulnerability, influencing the forestry sector in Romania. In this context, based on a sample of 248 Romanian active companies in the forestry sector in four distinct sectors, we calculated a number of indicators specific to the forestry sector in order to identify the risks and vulnerabilities and analyze the entities associated with this sector. Our research led us to the conclusion that, as far as the forestry sector is concerned, the companies that operate in Statistical classification of economic activities in the European Community (NACE) 240 and NACE 210 have registered superior results compared to the average in regards to the vulnerability of the sector, while those that operate in NACE 220 and NACE 230 focus mostly on those vulnerabilities regarding the risk zone of their sustainable development. The study could be useful both to stakeholders by giving them the possibility to identify those entities, classified according to the NACE code, taking into account the sector vulnerabilities and the risks associated with the profile market, as well as to the state that could influence through economic policies the sectors in which vulnerabilities are manifested.

---

Factors Affecting the Use of Balanced Scorecard in Measuring Company Performance
10.3390/su12031178
The paper presents the results from the research on the factors influencing the use of the Balanced Scorecard methodology in measuring company performance in the engineering sector. The primary objective of the research was to verify the importance of using non-financial factors in managing businesses in connection to the use of the Balanced Scorecard methodology and to verify the dependence between the use of the given methodology and the lack of human and financial resources for its usage. The research focusing on the given issue was conducted over a period of six months. The research was based on the hypotheses that were verified with statistical methods using the methodology of a Chi-square test. To identify the factors that hinder the usage of the Balanced Scorecard methodology in the addressed enterprises, the method of standard deviation was used. The main result of the research is a finding that there is a statistically significant relationship between the enterprises considering the non-financial indicators and the use of the Balanced Scorecard methodology to be important. This relationship is confirmed also by the calculation using a test with p = 0.0422. The research verified one of the main research goals, i.e., the importance of non-financial indicators in connection to the Balanced Scorecard concept (BSC). Other hypotheses are related to the issue of the lack of human and financial resources. Using the Chi-square test in these cases once again, the study also found out the existence of the dependence between the lack of these resources and the use of the Balanced Scorecard methodology. The final value p = 0.0446 relating to human resources and the value p = 0.0377 relating to financial sources define the barriers as being important in implementing the BSC methodology into corporate practice. These values confirm other research results related to the barriers of using BSC. The presented paper assesses the research results that confirm the importance of using non-financial indicators and define the barriers that hinder this usage. The research contributed to the extension of the knowledge of the BSC concept that we consider being a modern managerial future-oriented tool and supported its implementation in companies so that they could operate within the framework of sustainable development.

---

ESG Scores and the Credit Market
10.3390/su12083456
This study analyzes the relationship between Environmental, Social and Governance (ESG) scores and bond returns using the corporate bond data in Korea during the period of 2010 to 2015. We find that ESG scores include valuable information about the downside risk of firms. This effect is particularly salient for the firms with high information asymmetry such as small firms. Interestingly, of the three ESG criteria, only environmental scores show a significant impact on bond returns when interacted with the firm size, suggesting that high environmental scores lower the cost of debt financing for small firms. Finally, ESG is complementary to credit ratings in assessing credit quality as credit ratings cannot explain away ESG effects in predicting future bond returns. This result suggests that credit rating agencies should either integrate ESG scores into their current rating process or produce separate ESG scores which bond investors integrate with the existing credit ratings by themselves.

---

Does Audit Improve the Quality of ESG Scores? Evidence from Corporate Misconduct
10.3390/su12145670
One of the main controversial aspects of sustainability metrics relies on the accuracy, transparency, and reliability of the information at the basis of environmental, social and governance (ESG) scores. This paper investigates whether firms that have their ESG reporting audited by independent firms exhibit a higher quality of ESG scores. We performed an analysis investigating the change in ESG scores following the unveiling of a corporate misconduct. We documented that, overall, no significant ESG score adjustment occurs after the scandal becomes public, thus, implying that rating agencies provide an accurate interpretation of the firm’s sustainability. However, our results differed when we distinguished between audited and unaudited reports. Firms whose reports are audited by third parties did not exhibit significant changes in their scores after a scandal, whereas for companies whose reports are not audited, we detected a worsening of the ESG scores that are statistically significant. Our findings were also confirmed in a multivariate analysis. Overall, our results suggest that the reliability of ESG scores can benefit from the auditing of sustainability reporting by third parties, which has an assurance effect on the quality of the company’s ESG information.

---

Automated Sustainability Assessment System for Small and Medium Enterprises Reporting
10.3390/su12145687
Sustainability assessment is a mainstream business activity that demonstrates the link between the organization’s strategy for and commitment to a sustainable global economy, and the prevention of economic crises. Small- and medium-sized businesses/enterprises (SMBs/SMEs) have significant effects on the European economy. However, because of a lot of restriction factors, like business risk, the high expenses of data collection and management, and the lack of resources, sustainability reporting is considered a superfluous and burdensome activity for them. The aim of this research is to propose an automated, comprehensive and simplified system for the sustainability assessment of SMEs. This system is achieved by implementing three main phases. The first phase includes key performance indicators design, which starts with the identification of various key performance indicators for comprehensive sustainability assessment, and ends with proposing an optimal set of KPIs (Key Performance Indicators) that can encompass long-term issues and be applicable to SMEs in the EU. The second phase involves a new comprehensive method of sustainability assessment for all KPIs designed in the first phase. Therefore, a multi-criteria model, which involves four main pillars of sustainability assessment (economic, environmental, social and governance), is proposed. It gives different enterprises the ability to verify and compare their efficiency and sustainability with other companies within the same sector in an almost automated manner. In the final phase, a simple and an automated information system (WEBRIS), which provides a suitable environment for SME sustainability reporting, is developed. Finally, this system is verified in a case study of the Czech breweries sector.

---

Indicators and Framework for Measuring Industrial Sustainability in Italian Footwear Small and Medium Enterprises
10.3390/su13105472
As small and medium enterprises (SMEs) have limited resources, they need a manageable number of indicators that are simple and easy to use for measuring sustainability performance. However, the lack of suitable indicators tailored to industry needs, particularly for SMEs, has been a major challenge in measuring and managing industrial sustainability. Our study aims to empirically analyze and select the useful and applicable indicators to measure sustainability performance in Italian footwear SMEs. To achieve this objective, we proposed a methodological approach to identify, analyze and select sustainability indicators. First, we carried out a systematic review to identify potential sustainability indicators from the literature. Then, we developed a questionnaire based on the identified indicators and pre-tested it with selected industrial experts, scholars, and researchers to further refine the indicators before collecting data. We applied the fuzzy Delphi method to analyze and select the final indicators. Based on a sample of 48 Italian footwear SMEs, the results of our study show that product quality, material consumption, and customer satisfaction were the top priorities among the selected indicators for measuring the economic, environmental, and social dimensions of industrial sustainability, respectively. The selected indicators stressed the measuring of industrial sustainability performance associated with financial benefits, costs, market competitiveness, resources, customers, employees, and the community. Our study proposed a framework that helps to apply the selected indicators for measuring sustainability performance in SMEs. Finally, our study contributes to the existing theory and knowledge of industrial sustainability performance measurement by providing indicators supported by empirical evidence and a framework to put the indicators into practice in the context of SMEs.

---

Multi-Stakeholder Impact Environmental Indexes: The Case of NeXt
10.3390/su132212364
The design of proper environmental and social indicators is one of the most critical challenges when monitoring and implementing corporate and government policy measures toward ecological transitions and sustainable development. In our paper we outline and discuss the characteristics of a new vintage of “living” multi-stakeholder community-based indicators based on the principles of self-evaluation, dialogue and simplification with a specific focus on the NeXt index. We explain the main differences between them and the opposite extreme of static expert-based indicators, how they integrate firm-level scores with compliance with macro multidimensional wellbeing indicators (such as the UN Sustainable Development Goals) and how they complement with ongoing regulatory standards currently under development. As well, we discuss caveats, policy implications and impact in terms of subjective wellbeing.

---

Environmental, Social, and Governance Integration into the Business Model: Literature Review and Research Agenda
10.3390/su14052959
Environmental, social, and governance (ESG) integration as a socially responsible investment (SRI) from a financial perspective has been discussed extensively. However, few studies discuss its impact on firms’ internal operations from the perspective of sustainable development (SD). This study aims to examine the integration of ESG into the currently prevailing business model. Twenty-nine studies were systematically reviewed. Our analysis used an input–process–output model to identify the integration process and the outcomes. The findings show that only two papers explain the implementation steps or transition process of ESG integration, while 27 papers discuss ESG integration as an outcome, including integration behaviors, advantages, practices, and critical views. Our research aims to highlight that firms adopt ESG as a response to pressure from financial markets rather than as a serious effort to integrate sustainability into their core operations. We state the need for more research into the integration process to motivate firms to reform their business models, foster sustainability, and enhance financial performance.

---

Barriers to Using ESG Data for Investment Decisions
10.3390/su14095157
Institutional investors who commit to integrating environmental, social and governance (ESG) aspects into investment decisions require ESG data of sufficient quality. However, concerns have risen over a lack of quality in ESG data, as outlined by the Global Reporting Initiative. The lack of quality in ESG data deters institutional investors from using the data for investment decisions. This study outlines the ESG data reporting process and explores where in the process quality concerns emerge. Semi-structured interviews are applied with professionals involved in ESG data analysis and reporting of listed companies, a rating agency and institutional investors. The results show that current barriers to using ESG data include a lack of materiality, accuracy and reliability. Interviewees agree that access to data collected by governmental institutions is lacking, and that companies’ purchase of carbon credits raise questions about the reliability of ESG data. Companies hold contrasting views to the institutional investors on the useability of the data they disclose. The results enhance our understanding of the common and contrasting concerns about the lack of quality in ESG data. The results can be used as guide for companies, investors and regulators for actions to mitigate barriers related to the lack of quality in ESG reporting.

---

Socially Responsible Activity of Micro-, Small-, and Medium-Sized Enterprises—Benefits for the Enterprise
10.3390/su14159603
As a research hypothesis, it was assumed that micro-, small-, and medium-sized enterprise SMEs undertake Corporate Social Responsibility (CSR) mainly guided by the benefits obtainable from this activity. The aim of the study was to identify the achievable and achieved benefits of undertaking CSR by SME enterprises. SMEs are not obliged to report this activity. The exploration included literature studies and empirical research, according to the expert method, in three stages: (1) selection of experts, (2) collection of information using the CAWI and CATI method, (3) development and interpretation of research results. Research has indicated that CSR is becoming closer to SMEs. The research has shown that experts are implementing CSR and gaining benefits from it. It has also shown that experts’ knowledge of CSR is insufficient, which results in many opportunities resulting from the implementation of this activity being overlooked. The European Commission (EC) guidelines introduced an obligation of reporting CSR only for listed companies; indirectly, this obligation will also affect other companies. Voluntary reporting will increase the knowledge of CSR and bring many benefits to SMEs, provided that a legal framework is created to facilitate the implementation of CSR and its reporting.

---

Does the Tone in Corporate Social Responsibility Reports Misdirect Analysts’ Forecasts in China?
10.3390/su142416631
With increasing emphasis being placed on corporate social responsibility, the number of companies furnishing corporate social responsibility (CSR) reports is increasing. This study investigates the impact of abnormal positive tone in CSR reports on analysts’ earnings forecast bias. The textual analysis of CSR reports of Chinese listed companies between 2006 and 2016 reveals that an abnormal positive tone significantly and positively relates to an optimistic bias in analysts’ forecasts. This effect is pronounced among companies with poor financial transparency and those operating in regions where culture is stakeholder-oriented. Further analysis confirms that the poorer the company’s CSR performance, the more it tends to mislead analysts using an abnormal positive tone in its CSR report. Based on these findings, this study suggests that firms may greenwash using an abnormally positive tone in their CSR reports.

---

Environmental, Social and Governance (ESG) Disclosure and the Small and Medium Enterprises (SMEs) Sustainability Performance
10.3390/su15010200
This paper aims to evaluate the impact of environmental, social and governance (ESG) disclosure practices on the sustainability performance of small and medium enterprises (SMEs) in Saudi Arabia. It adopts qualitative research methods to answer the research questions through interviews, using a sample of 30 interviewees, to direct and moderate the relationship between SMEs’ sustainability performance and their disclosure of ESG practices. The results indicated that SMEs in Saudi Arabia lack awareness of ESG practices and disclosures and, therefore, the extent of their importance to sustainability performance. The findings of this research have several practical implications for different stakeholders, internally and externally, such as managers, consultants, investors, credit agencies, lenders, policymakers, government, and the overall community in the context of the potential effects of ESG disclosure practices on SMEs.

---

Technologies Empowered Environmental, Social, and Governance (ESG): An Industry 4.0 Landscape
10.3390/su15010309
Currently, sustainability is a vital aspect for every nation and organization to accomplish Sustainable Development Goals (SDGs) by 2030. Environmental, social, and governance (ESG) metrics are used to evaluate the sustainability level of an organization. According to the statistics, 53% of respondents in the BlackRock survey are concerned about the availability of low ESG data, which is critical for determining the organization’s sustainability level. This obstacle can be overcome by implementing Industry 4.0 technologies, which enable real-time data, data authentication, prediction, transparency, authentication, and structured data. Based on the review of previous studies, it was determined that only a few studies discussed the implementation of Industry 4.0 technologies for ESG data and evaluation. The objective of the study is to discuss the significance of ESG data and report, which is used for the evaluation of the sustainability of an organization. In this regard, the assimilation of Industry 4.0 technologies (Internet of Things (IoT), artificial intelligence (AI), blockchain, and big data for obtaining ESG data by an organization is detailed presented to study the progress of advancement of these technologies for ESG. On the basis of analysis, this study concludes that consumers are concerned about the ESG data, as most organizations develop inaccurate ESG data and suggest that these digital technologies have a crucial role in framing an accurate ESG report. After analysis a few vital conclusions are drawn such as ESG investment has benefited from AI capabilities, which previously relied on self-disclosed, annualized company information that was susceptible to inherent data issues and biases. Finally, the article discusses the vital recommendations that can be implemented for future work.

---

Greenwashing, Sustainability Reporting, and Artificial Intelligence: A Systematic Literature Review
10.3390/su15021481
The rise of stakeholder interest globally in sustainable business practices has resulted in a rise in demands from stakeholders that companies report on the environmental and social impacts of their business activities. In certain cases, however, companies have resorted to the practice of providing inaccurate disclosures regarding sustainability as part of their corporate communications and sustainability reporting—commonly referred to as “greenwashing”. Concurrently, technological improvements in artificial intelligence have presented the means to rapidly and accurately analyze large volumes of text-based information, such as that contained in sustainability reports. Despite the possible impacts of artificial intelligence and machine learning on the fields of greenwashing and sustainability reporting, no literature to date has comprehensively and holistically addressed the interrelationship between these three important topics. This paper contributes to the body of knowledge by using bibliometric and thematic analyses to systematically analyze the interrelationship between those fields. The analysis is also used to conjecture a conceptual and thematic framework for the use of artificial intelligence with machine learning in relation to greenwashing and company sustainability reporting. This paper finds that the use of artificial intelligence in relation to greenwashing, and greenwashing within sustainability reporting, is an underexplored research field.

---

Community Governance Based on Sentiment Analysis: Towards Sustainable Management and Development
10.3390/su15032684
The promotion of community governance by digital means is an important research topic in developing smart cities. Currently, community governance is mostly based on reactive response, which lacks timely and proactive technical means for emergency monitoring. The easiest way for residents to contact their properties is to call the property call center, and the call centers of many properties store many speech data. However, text sentiment classification in community scenes still faces challenges such as small corpus size, one-sided sentiment feature extraction, and insufficient sentiment classification accuracy. To address such problems, we propose a novel community speech text sentiment classification algorithm combining two-channel features and attention mechanisms to obtain effective emotional information and provide decision support for the emergency management of public emergencies. Firstly, text vectorization based on word position information is proposed, and a SKEP-based community speech–text enhancement model is constructed to obtain the corresponding corpus. Secondly, a dual-channel emotional text feature extraction method that integrates spatial and temporal sequences is proposed to extract diverse emotional features effectively. Finally, an improved cross-entropy loss function suitable for community speech text is proposed for model training, which can achieve sentiment analysis and obtain all aspects of community conditions. The proposed method is conducive to improving community residents’ sense of happiness, satisfaction, and fulfillment, enhancing the effectiveness and resilience of urban community governance.

---

Corporate Sustainability Communication as ‘Fake News’: Firms’ Greenwashing on Twitter
10.3390/su15086683
Fake news on social media has engulfed the world of politics in recent years and is now posing the same threat in other areas, such as corporate social responsibility communications. This study examines this phenomenon in the context of firms’ deceptive communications concerning environmental sustainability, usually referred to as greenwashing. We first develop and validate a new method for automatically identifying greenwashing, using linguistic cues in a sample of tweets from a diverse set of firms in two highly polluting industries. We then examine the relationship between greenwashing and financial market performance for the firms in our sample. Prior research has identified these issues as some of the most important gaps in the extant literature. By addressing them, we make several important contributions to corporate sustainability research and practice, as well as introducing notable improvements to automatic greenwashing detection methods.

---

Is Sustainability Reporting Promoting a Circular Economy? Analysis of Companies’ Sustainability Reports in the Agri-Food Sector in the Scope of Corporate Sustainability Reporting Directive and EU Taxonomy Regulation
10.3390/su15097498
Circular economy has the potential to contribute significantly to sustainable development. Despite its popularity, implementation in Europe is still low. Through more stringent sustainability reporting, the circular economy should be increasingly implemented by companies, which is currently pursued by the EU through two new legal acts. Therefore, we need a more integrated understanding of existing practices of corporate sustainability reporting to identify weak points and possibilities for further improvement. This article aims to (i) investigate whether companies in the agri-food sector have reported on the circular economy so far, (ii) to what extent future legal obligations are already being met, and (iii) if the two new EU legal acts hold significance for the promotion of circular economy through corporate reporting. To assess the current reporting practices, a qualitative content analysis and a mapping approach of 20 selected sustainability reports from key players in the agri-food sector have been conducted. Additionally, seven semi-structured expert interviews were carried out to review the future role of the legal acts. Results show that reporting in the agri-food sector on circular economy has increased considerably as of 2016, but it is still lacking in terms of the two new legal acts. Although the Global Reporting Initiative (GRI) can be seen as a good basis for reporting, there is a large number of new obligations, which means that companies should start preparing at an early stage. This is especially true for those agri-food companies that have not yet been subjected to any reporting obligations. Experts have agreed with this view, considering the legal acts as an important vehicle for promoting the concept. However, they also recognize the weaknesses, such as the existing scope for interpretation, which still need to be addressed before the final publication of the standard and the technical criteria. Future research should analyze the final commitments of the reports (including for small and medium-sized companies), compare them with established reporting standards, seek expert opinions on them, and quantitatively examine sustainability reports in this and other industries.

---

esg2go: A Method to Reduce Bias, Improve Coherence, and Increase Practicality of ESG Rating and Reporting
10.3390/su152416872
Rating agencies that assess a company’s environmental, social, and corporate governance (ESG) impact have been subject to public and academic scrutiny due to divergent and often biased rating outcomes. Concurrently, an evolving regulatory environment mandates publicly listed companies to report on ESG and climate emissions, taking into account supply chain risks as well. As a result, small and medium-sized enterprises (SMEs) are increasingly asked as suppliers to present a credible sustainability certificate. The esg2go rating and reporting system aims at improving the credibility and practicality of corporate sustainability assessment. It was jointly developed with its users and relevant stakeholders and is based on a calibrated benchmarking system from verifiable data. The rating method enables the measurement and comparison of sector- and firm size-specific sustainability performance. Its underlying adaptive parametrization is derived from a coherent and pragmatic definition of SME sustainability as the ‘ability to co-exist’. Our data analyses indicate that our scoring function is able to minimize bias and deliver a fair comparability between SMEs. We conclude that esg2go represents a pragmatic and innovative approach to enhance the fairness and accuracy of corporate sustainability assessment.

---

Peeking into Corporate Greenwashing through the Readability of ESG Disclosures
10.3390/su16062571
Faced with the widespread issue of greenwashing, there is a pressing need for an effective approach to assess the extent of corporate involvement in such hypocritical practices. This study aims to address this concern by examining the association between corporate ESG disclosures’ readability and greenwashing. We gauge the readability using a modified Fog Index and construct a company’s peer-relative greenwashing score based on data from third-party databases. The empirical analysis reveals a negative relationship between the level of corporate greenwashing and the readability of its ESG disclosures, suggesting that companies whose ESG disclosures are more readable are less likely to engage in greenwashing. This negative relationship is particularly pronounced in companies characterized by higher levels of information asymmetry. However, the relationship is weaker after 2018, when the “Code of Corporate Governance for Listed Companies” was implemented. In conclusion, our research highlights the significance of ESG disclosure readability in effectively conveying and predicting corporate greenwashing practices. This study provides valuable insights for investors seeking to evaluate corporate performance and make well-informed investment decisions.

---

Textual Attributes of Corporate Sustainability Reports and ESG Ratings
10.3390/su16219270
While the textual attributes of corporate financial documents, such as annual reports, have been extensively analyzed in the academic literature, those of corporate sustainability reports, which serve as a critical channel for nonfinancial disclosure, are relatively under-explored. Given the increasing importance of Environmental, Social, and Governance (ESG) factors in corporate strategy and stakeholder evaluation, understanding the role of textual attributes in sustainability reporting is crucial. This study examines 10,021 hand-collected sustainability reports from Chinese firms between 2009 and 2021, focusing on six key textual attributes: length, readability, tone, boilerplate language, redundancy, and completeness. Using computational linguistics, we analyze how these attributes evolve over time and their impact on ESG ratings provided by both international (MSCI, FTSE) and domestic (SNSI) agencies. Our findings reveal that the length and completeness of sustainability reports significantly influence ESG scores across agencies, demonstrating a shared appreciation for detailed and transparent disclosures. However, international and domestic rating agencies exhibit differing responses to attributes like tone, boilerplate language, and redundancy. These differences highlight variations in evaluation standards, methodologies, and value orientations between global and local stakeholders. The results emphasize the need for firms to tailor their sustainability disclosures to meet diverse stakeholder expectations. This study contributes to the growing body of literature on nonfinancial reporting by providing empirical evidence on how specific textual characteristics of sustainability reports can shape ESG evaluations, offering insights for both corporate communicators and policymakers.

---

A New Approach to Assess Sustainable Corporate Reputation with Citizen Comments Using Machine Learning and Natural Language Processing
10.3390/su16229610
This study investigates the assessment of sustainable corporate reputation through citizen comments and how it can be measured by sentiment analysis methods based on machine learning and text mining. The research analyses citizen feedback on municipalities in the field of public services and examines their impact on the social reputation of the services provided by municipalities. Support vector machines, one of the machine learning methods, was used for sentiment analysis. In the study, Google Maps comments of the citizens receiving services from the municipality were used. The results of the sentiment analysis reveal that sustainable corporate reputation is directly related to citizen satisfaction and feedback. In this context, municipalities should continuously receive feedback and make strategic improvements based on citizens’ comments to ensure sustainable service quality. Municipalities are especially appreciated by citizens for their fast, effective, and high-quality services. However, some negative comments focus on issues such as the slowness of services, cleaning problems, and staff attitudes, indicating that certain improvements are needed. This feedback emphasises the need for continuous improvement in service quality.

---

Credit Risk Assessment of Green Supply Chain Finance for SMEs Based on Multi-Source Information Fusion
10.3390/su17041590
As an important pillar of the national economy, the green transformation of SMEs is the key to promoting sustainable economic development. However, SMEs generally face issues such as information opacity and high operational risks, which make it difficult for them to obtain traditional financing support, thereby hindering green development. Green Supply Chain Finance has opened up new financing channels for SMEs, but the accuracy of credit risk evaluation remains a bottleneck that limits its widespread application. This paper constructs a credit risk evaluation index system that integrates multiple sources of information, covering factors such as the situations of SMEs themselves, stakeholder feedback, and expert ratings. It compares and analyzes the performance of the genetic algorithm-optimized random forest model (GA-RF), the BP neural network, the support vector machine, and the logistic regression model in credit risk evaluation. The empirical results indicate that the GA-RF model is significantly better than the other models in terms of accuracy, precision, and F1 score, and has the highest AUC value, making it more effective in identifying credit risk. In addition, the GA-RF model reveals that the asset–liability ratio, the time of establishment, the growth rate of operating revenue, the time of collection of accounts receivable, the return on net assets, and daily shipments are the key indicators affecting the credit risk assessment.

---

From the Corporate Social Responsibility (CSR) and the Environmental, Social and Governance (ESG) Criteria to the Greenwashing Phenomenon: A Comprehensive Literature Review About the Causes, Consequences and Solutions of the Phenomenon with Specific Case Studies
10.3390/su17052222
Greenwashing, the phenomenon of misleading stakeholders concerning the environmental sustainability efforts of a company, may undermine the trust of people to a company or to a whole industry and the progress toward sustainability. This paper provides an extensive Literature Review about the evolution of Corporate Social Responsibility (CSR) into Environmental, Social and Governance (ESG) criteria and their relationship with the Greenwashing Phenomenon. It also examines the historical and regulatory contexts, causes, consequences and mitigation strategies of this phenomenon. By analyzing the market distortions and the environmental harm that may be linked to the Greenwashing Phenomenon, the study highlights the need for enhanced regulation, improved transparency and stakeholder vigilance. The methods employed in this paper include a thematic analysis of the literature and qualitative case study comparisons to derive insights into the multifarious impacts of greenwashing. Such case studies provided in this paper concern companies such as Volkswagen, Zara, Coca-Cola and BP.

---

Development of a Global Framework for an Integrated Life Cycle Assessment (LCA) Model in Quality, Safety and Environmental (QSE) Management Systems: Improving Environmental, Social and Economic Sustainability Performance
10.3390/su17083521
A framework to include life cycle assessment (LCA) in the importance assessment of quality, safety and environmental (QSE) aspects of a management system has been studied to improve the sustainable development performance in the environmental, social and economic dimensions. But there is a literature gap where impact assessment is a critical factor. This research follows a mixed-methods approach, including a survey of 127 Moroccan companies to assess the adoption and impact of LCA integration. The survey’s findings show that 40% of companies have integrated LCA through significant advances in operational quality, regulatory compliance and sustainability performance. The findings also demonstrate how integration has enhanced long-term strategic decision-making, process optimization and environmental impact assessment. The proposed model aligns the requirements of sustainable LCA standards (ISO 14040/44, ISO 26000 and ISO 15686-5) with certifiable standards (ISO 14001, ISO 9001 and ISO 45001), addresses the opportunities and limitations of organizations during integration, and includes indicators for sustainability analysis. The study highlights how implementing LCA in QSE management creates a systematic approach to sustainability, particularly in terms of employee training, regular performance monitoring and regulatory compliance. In light of changing laws and industry norms, these findings provide a means for sectors to enhance their sustainability performance.

---

An Empirical Analysis of the Impact of ESG Management Strategies on the Long-Term Financial Performance of Listed Companies in the Context of China Capital Market
10.3390/su17135778
In the evolving landscape of China’s capital markets, the integration of Environmental, Social, and Governance (ESG) considerations has become increasingly crucial for investors and decision-makers. Traditional financial performance metrics often fall short in capturing the multidimensional and long-term impacts of ESG factors. This study introduces a novel computational framework that combines domain-adapted pre-trained language models with structured financial regression analysis, aiming to empirically assess the correlation between ESG disclosures and long-term financial performance. This approach allows for the simultaneous processing of both structured and unstructured ESG data, using graph-based modeling and reinforcement learning to guide sustainability aligned policy optimization. Our empirical results show that firms with consistent and well-structured ESG strategies exhibit significantly superior long-term financial outcomes compared to those with weak or inconsistent ESG engagement. This study not only confirms the value of ESG engagement in enhancing financial resilience but also offers practical recommendations for investors, regulators, and corporate decision-makers, emphasizing consistent disclosure, sector-aligned ESG investment, and proactive adaptation to policy shifts.

---

Data Requests in Value Chains: The Effects of Corporate Sustainability Reporting on SMEs in The Netherlands
10.3390/su17178029
This study examines the effects of sustainability-related data requests—spurred by the EU Corporate Sustainability Reporting Directive (CSRD)—on small and medium-sized enterprises (SMEs) in the Netherlands. Using a representative survey of 431 SMEs and 48 qualitative interviews with SME representatives and business stakeholders, the research provides a comprehensive overview of their experiences in late 2024. A key finding is that most Dutch SMEs (72%) have not yet received sustainability data requests. However, SMEs embedded in international value chains report more frequent and complex data demands, particularly concerning environmental indicators like CO2 emissions and material use. Ratings of perceived relevance reveal a disconnect between external data requests and SMEs’ internal priorities, with many SMEs prioritizing health and safety over climate metrics. While some SMEs see data requests as opportunities for improved sustainability performance and market positioning, many also experience challenges, including limited resources, fragmented IT systems, and regulatory uncertainty. The implementation of CSRD highlights the urgency of supporting SMEs in building data management capacities and standardized processes. The study recommends clearer communication of data relevance, targeted support measures, and further research into cross-national and longitudinal dynamics to foster an effective sustainability transition across value chains.

---

From AI Adoption to ESG in Industrial B2B Marketing: An Integrated Multi-Theory Model
10.3390/su17198595
Artificial intelligence is transforming industrial marketing by reshaping processes, decision-making, and inter-firm relationships. However, research remains fragmented, with limited evidence on how adoption drivers create new capabilities and sustainability outcomes. This study develops and empirically validates an integrated framework that combines technology, organization, environment, user acceptance, resource-based perspectives, dynamic capabilities, and explainability. A convergent mixed-methods design was applied, combining survey data from industrial firms with thematic analysis of practitioner insights. The findings show that technological readiness, organizational commitment, environmental pressures, and user perceptions jointly determine adoption breadth and depth, which in turn foster marketing capabilities linked to measurable improvements. These include shorter quotation cycles, reduced energy consumption, improved forecasting accuracy, and the introduction of carbon-based pricing mechanisms. Qualitative evidence further indicates that explainability and human–machine collaboration are decisive for trust and practical use, while sustainability-oriented investments act as catalysts for long-term transformation. The study provides the first empirical integration of adoption drivers, capability building, and sustainability outcomes in industrial marketing. By demonstrating that artificial intelligence advances competitiveness and sustainability simultaneously, it positions marketing as a strategic lever in the transition toward digitally enabled and environmentally responsible industrial economies. We also provide a simplified mapping of theoretical lenses, detail B2B-specific scale adaptations, and discuss environmental trade-offs of AI use. Given the convenience/snowball design, estimates should be read as upper-bound effects for mixed-maturity populations; robustness checks (stratification and simple reweighting) confirm sign and significance.

---

Advancing Energy Transition and Climate Accountability in Wisconsin Firms: A Content Analysis of Corporate Sustainability Reporting
10.3390/su17198935
Corporate ESG (Environmental, Social, and Governance) reporting is increasingly envisioned as evidence of accountability in the energy transition, yet persistent gaps remain between commitments and practices. This study applied the Global Reporting Initiative (GRI) framework—specifically indicators 302 (Energy) and 305 (Emissions)—to evaluate the credibility, scope, and strategic depth of disclosures by 20 Wisconsin (WI) firms in the energy, manufacturing, food, and service sectors. Guided by accountability and legitimacy theory, a comparative content analysis was conducted, complemented by Spearman correlation to examine associations between firm size and disclosure quality. Results show that while firms consistently report basic metrics such as total energy consumption and Scope 1 emissions, disclosures on Scope 3 emissions, renewable sourcing, and energy-efficiency achievements remain partial and selectively framed. Third-party assurance is inconsistently applied, and methodological transparency—such as external audit and coding protocols—is limited, weakening credibility. A statistically significant negative correlation was observed between annual revenue and disclosure quality, indicating that greater financial capacity does not necessarily translate into greater transparency. These findings highlight methodological and governance shortcomings, including reliance on generic ESG frameworks rather than climate-focused standards such as Task Force on Climate-related Financial Disclosures (TCFD). Integrated reporting approaches are recommended to improve comparability, credibility, and alignment with Wisconsin’s Clean Energy Transition Plan.

---

AI-Enabled ESG Compliance Audit for Stakeholders
10.3390/su17219513
Environmental, social, and governance (ESG) disclosures face credibility risks due to Scope 2 Greenhouse Gas (GHG) reports lacking standardized compliance checks, raising concerns about their reliability. This study therefore develops and evaluates an AI-enabled artefact for ESG compliance auditing. This artefact applies natural language processing (NLP) to extract reported values, implements rule-based checks grounded in the GHG Protocol, and produces transparent output. A design science research (DSR) approach guided the design, demonstration, and evaluation of the artefact, which was applied to sustainability reports from five technology companies. The results revealed that it replicates auditor judgments and reduces workload by over ninety percent in the sample. These findings serve as a proof-of-concept for automation in ESG compliance auditing. The theoretical contributions include extending the literature on AI in ESG auditing by reframing its role from producing interpretive scores to enabling transparent compliance verification. This study also demonstrates how DSR can help produce artefacts that embed rule-based logic into ESG assurance with rigor and practical relevance. The practical contributions include highlighting how a lightweight tool can enable auditors, regulators, boards, and investors to screen disclosures and benchmark credibility without sacrificing professional judgment.

---

Automating the Construction of Environmental Policy Knowledge Graph with Large Language Models
10.3390/su172210282
Enterprises engaged in transnational operations are confronted with increasingly complex environmental policies and compliance challenges. A critical hurdle to achieving sustainable development lies in rapidly and accurately extracting environmental protection requirements from vast volumes of policy. To address this, our study introduces an automated framework for knowledge graph construction, termed iteration–extraction–optimization (IEO), driven by a large language model (LLM). Diverging from conventional linear extraction methods, the IEO framework employs an iterative enhancement process to progressively build and refine a policy knowledge network, capturing the intricate relationships among legislation, institutions, and environmental obligations. As a case study, we applied the framework to Niger’s environmental policy, constructing a large-scale knowledge graph with 61,912 entities and 81,389 relations. Preliminary evaluations demonstrate the framework’s high performance in knowledge capture completeness, achieving a recall of 0.93 and an F1-score of 0.84. This research presents a novel paradigm for the intelligent parsing of environmental policy texts, providing a knowledge graph that serves as a vital decision-support tool for corporate environmental risk management and strategic sustainability planning.

---

From News to Knowledge: Leveraging AI and Knowledge Graphs for Real-Time ESG Insights
10.3390/su172411128
Traditional Environmental, Social, and Governance (ESG) assessments rely heavily on corporate disclosures and third-party ratings, which are often delayed, inconsistent, and prone to bias. These limitations leave stakeholders without timely visibility into rapidly evolving ESG events. These assessment frameworks also fail to capture the dynamic nature of ESG issues reflected in public news media. This research addresses these limitations by proposing and implementing an automated framework utilising Artificial Intelligence (AI), specifically Natural Language Processing (NLP) and Knowledge Graphs (KG), to analyse ESG news data for companies listed on major stock indices. The methodology involves several stages: collecting a registry of target companies; retrieving relevant news articles; applying Named Entity Recognition (NER), sentiment analysis, and ESG domain classification; and constructing a linked property knowledge graph to structure the extracted information semantically. The framework culminates in an interactive dashboard for visualising and querying the resulting graph database. The resulting knowledge graph supports comparative inferential analytics across indices and sectors, uncovering divergent ESG sentiment profiles and thematic priorities that traditional reports overlook. The analysis also reveals comparative insights into sentiment trends and ESG focus areas across different exchanges and sectors, offering perspectives often missing from traditional methods. Findings indicate differing ESG sentiment profiles and thematic focuses between the UK (FTSE) and Australian (ASX) indices within the analysed dataset. This study confirms AI/KG’s potential for a modular, dynamic, and semantically rich ESG intelligence approach, transforming unstructured news into interconnected insights. Limitations and areas for future work, including model refinement and integration of financial data, are also discussed. This proposed framework augments traditional ESG evaluations with automated, scalable, and context-rich analysis.

---

Using Generative Artificial Intelligence to Evaluate the Quality of Chinese Environmental Information Disclosure in Chemical Firms
10.3390/su172411348
Environmental information disclosure plays a critical role in corporate sustainability, yet existing evaluation approaches often rely on subjective judgment or limited textual features. This study proposes a structured framework for assessing the environmental information disclosure quality (EIDQ) of chemical enterprises and develops a generative artificial intelligence (GAI)-driven automated scoring system to enhance evaluation consistency. Using 190 Environmental, Social, and Governance (ESG) reports from 38 Chinese chemical firms between 2020 and 2024, we applied a multi-stage process combining indicator construction, DeepSeek-V3.2–based large language model (LLM) scoring, and cross-model validation. The results show that EIDQ exhibited a steady upward trend over the study period, reflecting a shift toward more quantitative and verifiable disclosure practices. The AI-generated scores demonstrated a high degree of alignment with human expert evaluations, and robustness tests confirmed the method’s transferability across different large language models. These findings provide methodological evidence for the feasibility of AI-assisted EIDQ assessment and offer practical implications for corporate sustainability reporting and regulatory oversight.

---

Evaluating Multimodal AI for Greenwashing Detection: A Comparative Analysis of ChatGPT, Claude, and Gemini in ESG Reports
10.3390/su18010236
The rapid expansion of sustainability reporting under the EU Corporate Sustainability Reporting Directive (CSRD) has intensified concerns about greenwashing, particularly in visual communication within ESG reports. Recent advances in multimodal artificial intelligence offer new possibilities for automated detection, yet their reliability in non-English corporate reporting contexts remains unclear. This study evaluates the greenwashing detection capabilities of three leading multimodal AI systems—ChatGPT 5.1, Claude 4.5 Sonnet, and Gemini 2.5 Flash—using a purposively selected sample of 20 Polish ESG reports benchmarked against ESRS-aligned performance scores from the national “Ranking ESG”. A standardized auditing prompt was applied across all tools to generate comparable assessments of visual greenwashing. Contrary to theoretical expectations and all four hypotheses, the models did not demonstrate negative correlations between performance and AI-detected greenwashing; instead, high-performing firms frequently received higher greenwashing scores. Dimensional analyses showed inconsistent and often contradictory evaluations across Environmental, Social, and Governance pillars, while inter-tool reliability proved extremely low (Krippendorff’s α ≈ 0). These findings indicate that current multimodal AI systems conflate communication sophistication with deceptive intent and lack sufficient contextual understanding for ESG assurance. The study highlights significant methodological limitations and outlines directions for developing domain-specific, ESRS-aligned AI tools for greenwashing detection.

---

Sustainability reporting in family firms: A panel data analysis
10.3390/su9010038
We analyze the largely unexplored differences in sustainability reporting within family businesses using a sample of 230 non-financial Italian listed firms for the period 2004–2013. Drawing on legitimacy theory and stakeholder theory, integrated with the socio-emotional wealth (SEW) approach, we study how family control, influence and identification shape a firm’s attitude towards disclosing its social and environmental behavior. Our results suggest that family firms are more sensitive to media exposure than their non-family counterparts and that family control enhances sustainability disclosure when it is associated to a family’s direct influence on the business, by the founder’s presence on the board or by having a family CEO. In cases of indirect influence, without family involvement on the board, the level of family ownership is negatively related to sustainability reporting. On the other hand, a formal identification of the family with the firm by business name does not significantly affect social disclosure.

---

Enhancing ESG Risk Assessment with Litigation Signals: A Legal-AI Hybrid Approach for Detecting Latent Risks
10.3390/systems13090783
Environmental, Social, and Governance (ESG) ratings are widely used for investment and regulatory decision-making, yet they often suffer from symbolic compliance and information asymmetry. To address these limitations, this study introduces a hybrid ESG risk assessment model that integrates court ruling data with traditional ESG ratings to detect latent sustainability risks. Using a dataset of 213 ESG-related U.S. court rulings from January 2023 to May 2025, we apply natural language processing (TF-IDF, Legal-BERT) and explainable AI (SHAP) techniques to extract structured features from legal texts. We construct and compare classification models—including Random Forest, XGBoost, and a Legal-BERT-based hybrid model—to predict firms’ litigation risk. The hybrid model significantly outperforms the baseline ESG-only model in all key metrics: F1-score (0.81), precision (0.79), recall (0.84), and AUC-ROC (0.87). SHAP analysis reveals that legal features such as regulatory sanctions and governance violations are the most influential predictors. This study demonstrates the empirical value of integrating adjudicated legal evidence into ESG modeling and offers a transparent, verifiable framework to enhance ESG risk evaluation and reduce information asymmetry in sustainability assessments.

---

Professional Determinants in ESG Reporting for Sustainable Financial Assessment
10.3390/systems13100898
This paper explores the key professional and institutional factors that influence the integration of environmental, social, and governance (ESG) considerations into financial evaluation and auditing processes. The study investigates the impact of legal familiarity, ESG experience, professional qualifications, and digital competencies on ESG readiness among financial analysts, auditors, and economists. By integrating a structured review of academic literature with an in-depth analysis of European regulatory instruments, the research identifies how dual materiality principles, standardized ESG metrics, and taxonomy-aligned disclosures reshape professional practices. A structured, ethics-approved survey (10 items) was administered nationally, and 145 responses were retained for analysis across economists, analysts, and auditors. Descriptive statistics, Pearson correlations, and linear/multiple regressions were used to test three hypotheses regarding ESG experience, legislative familiarity, and multifactor effects. The results reveal that familiarity with EU legislation is the strongest predictor of ESG integration capacity, while ESG-related experience and digitalization also show moderate to strong influence. The multiple regression model confirms the multifactorial nature of ESG implementation, though not all professional predictors contribute equally. Residual analysis confirms the statistical robustness of the models. The study highlights the need for regulatory literacy, targeted training, and digital adaptation as critical components of ESG competency.

---

The AI Annotator: Large Language Models’ Potential in Scoring Sustainability Reports
10.3390/systems13100899
To explore the potential of Large Language Models (LLMs) as AI Annotators in the domain of sustainability reporting, this study establishes a systematic evaluation methodology. We use the specific case of European football clubs, quantifying their sustainability reports based on the sport Positive matrix as a benchmark to compare the performance of three state-of-the-art models (i.e., GPT-4o, Qwen-2-72b-instruct, and Llama-3-70b-instruct) against human expert scores. The evaluation is benchmarked on dimensions including accuracy, mean absolute error (MAE), and hallucination rates. The results indicate that GPT-4o is the top performer, yet its average accuracy of approximately 56% shows it cannot fully replace human experts at present. The study also reveals significant issues with overconfidence and factual hallucinations in models like Qwen-2-72b-instructon. Critically, we find that by implementing further data processing, specifically a Chain-of-Verification (CoVe) self-correction method, GPT-4o’s initial hallucination rate is successfully reduced from 16% to 10%, while accuracy improved to 58%. In conclusion, while LLMs demonstrate immense potential to streamline and democratize sustainability ratings, inherent risks like hallucinations remain a primary obstacle. Adopting verification strategies such as CoVe is a crucial pathway to enhancing model reliability and advancing their effective application in this field.

---

Assessment of TCFD Voluntary Disclosure Compliance in the Spanish Energy Sector: A Text Mining Approach to Climate Change Financial Disclosures
10.3390/world6030092
This study investigates voluntary compliance with the Task Force on Climate-Related Financial Disclosures (TCFD) framework in 64 financial, Environmental, Social, and Governance (ESG) reports from six Spanish IBEX-35 energy firms (2020–2023) and explores the implications for intangible assets and corporate reputation, employing empirical quantitative text mining and Natural Language Processing (NLP) in Python. A validated scale-based taxonomy within the TCFD framework applies query-driven rules to extract relevant text. This enables an evaluation of aspects of the reports, facilitating the development of a compliance index measuring each company’s adherence to TCFD recommendations. All companies showed year-on-year improvements (2023 was the most comprehensive), yet none fully adhered due to information gaps. Disparities in the disclosures of Scope 1,2 and 3, persisted, suggesting reputational risks. A replicable methodological model generating a compliance index that assesses the ‘being’ (‘true performance’) versus ‘seeming’ (‘external perception’) dichotomy within sustainability reports and acts as a potential reputational barometer for stakeholders. By providing unprecedented evidence of TCFD reporting in the Spanish energy sector, this study closes a significant academic gap. Future research may analyze ESG reports using AI agents, study the impact of ESG on energy-intensive companies from AI data centers, supporting services like Copilot, ChatGPT, Claude, Gemini, and extend this methodology to other industrial sectors.

---

Aspect-Based Sentiment Analysis of Transportation Electrification Opinions on YouTube Comment Data
10.34123/jurnalasks.v16i2.790
Introduction/Main Objectives: This research aims to conduct an aspect-based sentiment analysis of transportation electrification opinions on YouTube comment data. Background Problems: It is difficult to summarize the sentiment of many YouTube user comments related to electric vehicles (EVs) based on their aspects; therefore, aspect-based sentiment analysis is needed to conduct further analysis. Novelty: This study identifies five aspects of EV and their sentiments at the same time. The aspects are usefulness, ease of use, comfort, cost, and incentive policies. One of this study’s methods is the transfer learning model. This model can be a solution to overcome the shortcomings of deep learning in classifying aspect-based sentiment classification on small datasets. Research Methods: The sentiment classification model used is a machine learning model, namely support vector machine (SVM) and transfer learning models from pre-trained IndoBERT and mBERT. Finding/Results: Based on the experimental results, transfer learning from the IndoBERT model achieved the best performance with accuracy and F1-Score of 89.17% and 52.66%, respectively. Furthermore, the best IndoBERT model was developed with input in the form of a combination of aspects and comment sentences. Experimental results show that there is an improvement in performance with accuracy and F1-Score of 90% and 60.70%, respectively.

---

Greenwashing in the FinTech sector: the role of the EU Corporate Sustainability Reporting Directive (CSRD)
10.34190/ecie.19.1.2953
FinTech companies, along with other financial institutions, face increasing pressure to take the lead in promoting a more environmentally sustainable future. The EU lately incorporated Corporate Sustainability Reporting Directive (CSRD) which offers both possibilities and challenges. This study examines the most recent modifications to Environmental, Social, and Governance (ESG) standards, identifying potential shifts that could affect FinTech firms. The problem of "greenwashing," a tactic used by businesses to misrepresent themselves as environmentally conscious, is especially concerning. Through literature analysis, this study attempts to explain terms related to greenwashing. While greenwashing instances were revealed so far within the confines of traditional banking, this study ventures beyond, including FinTech firms. FinTech companies are the focus of analysis because they are expected to play a significant role in the increased regulatory scrutiny starting in 2024. The paper addresses the opportunities and challenges that financial technology companies may face as they navigate this complex field. Relevant stakeholders looking for deep insights into the relationship between financial technology and environmental responsibility will find great value in the research's conclusions. This paper adds to an ongoing conversation on sustainability and regulation, enhancing the discussion of ethical financial activities.

---

Strategies for e-Assessments in the Era of Generative Artificial Intelligence
10.34190/ejel.22.7.3477
The rapid advancement of generative artificial intelligence (AI), particularly tools like ChatGPT, is reshaping educational landscapes by enabling students to generate responses that closely mimic human-written answers. This development presents both opportunities and challenges for e-assessments, especially concerning academic integrity and the authenticity of student learning outcomes. Traditional assessment methods, which often emphasize memorization and standardized testing, are proving insufficient in this new context, as they may not effectively measure higher-order skills like critical thinking, creativity, and problem-solving. This study employs a systematic literature review (SLR) to investigate adaptive e-assessment strategies in higher education that address the integrity challenges posed by generative AI while supporting meaningful learning. Through an in-depth analysis of recent literature on e-assessment practices and AI integration, this study identifies key adaptive strategies such as randomized questioning, project-based assessments, open-book exams, and AI-enhanced plagiarism detection. The findings reveal that while generative AI complicates the assessment process, it also provides an impetus for rethinking assessment design in ways that promote application-based knowledge and discourage cheating. By advocating for a shift towards assessments that evaluate critical skills rather than rote knowledge, this study proposes a framework that can support educators in creating robust, integrity-focused e-assessments. This research contributes to the evolving discourse on educational assessment, offering practical recommendations for institutions aiming to balance the benefits of AI-enhanced learning with the need for fair and accurate assessments.

---

DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation
10.36227/techrxiv.174837976.69904638/v1
Domain-specific QA systems require not just generative fluency but high factual accuracy grounded in structured expert knowledge. While recent Retrieval-Augmented Generation (RAG) frameworks improve context recall, they struggle with integrating heterogeneous data and maintaining reasoning consistency. To address these challenges, we propose DO-RAG, a scalable and customizable hybrid QA framework that integrates multi-level knowledge graph construction with semantic vector retrieval. Our system employs a novel agentic chain-of-thought architecture to extract structured relationships from unstructured, multimodal documents, constructing dynamic knowledge graphs that enhance retrieval precision. At query time, DO-RAG fuses graph and vector retrieval results to generate context-aware responses, followed by hallucination mitigation via grounded refinement. Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%. By combining traceability, adaptability, and performance efficiency, DO-RAG offers a reliable foundation for multi-domain, high-precision QA at scale.

---

Automated Measures of Sentiment via Transformer- and Lexicon-Based Sentiment Analysis (TLSA)
10.36227/techrxiv.21781109.v2
&lt;p&gt;The last decade witnessed the proliferation of automated content analysis in communication research. However, existing computational tools have been taken up unevenly, with powerful deep learning algorithms such as transformers rarely applied as compared to lexicon-based dictionaries. To enable social scientists to adopt modern computational methods for valid and reliable sentiment analysis of English text, we propose an open and free web service named transformer- and lexicon-based sentiment analysis (TLSA). TLSA integrates diverse tools and offers validation metrics, empowering users with limited computational knowledge and resources to reap the benefit of state-of-the-art computational methods. Two cases demonstrate the functionality and usability of TLSA. The performance of different tools varied to a large extent based on the dataset, supporting the importance of validating various sentiment tools in a specific context.&lt;/p&gt;

---

Blockchain-based approach to improve environmental, social, and governance (ESG) reporting in construction organizations
10.36680/j.itcon.2025.061
Existing ESG reporting tools in construction organizations often lack transparency and accountability, presenting significant challenges in effectively managing and reporting ESG data. This research addresses the gap in current reporting practices by proposing and validating a hybrid blockchain solution aimed at enhancing ESG reporting in the Architecture, Engineering, and Construction (AEC) industry. The primary objective is to develop a blockchain-based solution that automates ESG reporting, addressing issues such as data fragmentation, lack of verification, and inefficiencies. Adopting a design science approach, the study develops a conceptual framework that combines Ethereum and Hyperledger Fabric to create a hybrid blockchain model for the prototype. The comprehensive literature review highlights key challenges in ESG practices and emphasizes the potential of blockchain technology to overcome these barriers. The findings show that the hybrid blockchain model successfully automates the ESG reporting process, ensuring transparency, immutability, and accountability. The prototype, validated through a case study involving two construction organizations, demonstrates the feasibility of combining Ethereum and Hyperledger Fabric to manage ESG data, reducing errors, preventing manipulation, and enabling real-time reporting. This research enriches the theoretical understanding of blockchain applications in ESG practices. It provides practical implications by offering a tangible, blockchain-based solution that ensures transparent, reliable, and accountable ESG reporting in the construction industry, ultimately contributing to more sustainable practices.

---

Empowering Large Language Model Reasoning : Hybridizing Layered Retrieval Augmented Generation and Knowledge Graph Synthesis
10.36838/v6i12.11
: Retrieval Augmented Generation has improved LLM question answering significantly. However, this mechanism still produces hallucinations and structural incoherence in knowledge-intensive tasks. Additionally, many existing techniques neither holistically leverage multiple properties of text nor integrate diverse prompting and agenting frameworks. To address these limitations, this paper proposes a novel methodology that extracts and utilizes unstructured and structured properties of text to construct layered RAG pipelines designed to enhance complex LLM reasoning. Our approach synthesizes three distinct RAG methodologies, each specialized in various aspects: textual entity knowledge graph extraction (Textual Entity RAG); community summary and entity generation (Microsoft GraphRAG), and structural link navigation (MetaWiki RAG). By cumulatively layering these techniques along with advanced prompting and agentic evaluation, we aim to capture a more comprehensive context, enabling the model to generate well-structured responses that reflect all relevant attributes of the text. The proposed framework not only enhances existing RAG mechanisms but also demonstrates the effective integration of knowledge graphs. Additionally, it showcases the application of this framework to advanced answer generation using Wikipedia, with extensions to similar knowledge networks. This novel approach offers a robust solution for social recommender systems and other practical applications, delivering holistic outcomes by synthesizing diverse RAG techniques.

---

A Zero-Shot Aspect-Based Sentiment Analysis of Public Perception Toward AI Chatbots
10.38043/tiers.v6i1.6488
The rapid development of AI chatbots has sparked public discussion on social media regarding their performance, ethical implications, and related concerns. While past studies primarily focused on individual chatbot model using traditional sentiment analysis, this study implements a novel application of Zero-Shot Aspect-Based Sentiment Analysis (ABSA) on 17,562 tweets mentioning AI chatbots such as ChatGPT, Bard (now Gemini), and DeepSeek, utilizing an efficient sentiment extraction method without supervised training. Six aspects were analyzed to understand the sentiment pattern and the results show the discussion was dominated by negative sentiment, with Bard receiving the most positive sentiment, potentially shaped by brand trust and user familiarity. On the other hand, DeepSeek and ChatGPT attracted more criticism, especially related to performance and bias aspects. This study offers data-driven suggestions for developers, including improving response accuracy to shape user trust, reducing biased output, and developing real-time discourse analysis. Future work should incorporate multiple platforms to avoid bias, analyze more AI chatbot models, and include temporal sentiment for broader insights.

---

CORPORATE GOVERNANCE, CREDIT RISK, AND FINANCIAL LITERACY FOR SMALL MEDIUM ENTERPRISE IN INDONESIA
10.3846/btp.2021.13063
The problem of SMEs in Indonesia as a “high-risk borrower” that has not been resolved until today. The purposes of this study was to analyze the financial literacy as mediating between corporate governance and SMEs’ credit risk in Indonesia. This sample method used purposive sampling: 1273 units of Trade and Service SMEs fostered by Central Java that received credit in 2018. Twenty percent of the totals were taken, so the total was 255 samples. The data collection technique used questionnaires and interviews. The number of questionnaires distributed were 255 with a response rate of 95%, so resulting 242 respondents. Data analysis used descriptive and Regression Method. The corporate governance shown by responsibility, independence, and fairness did not affect SMEs’ credit risk. In other words, transparency and accountability is effective in reducing SMEs’ credit risk. Also, financial literacy can strengthen the influence of transparency, accountability, and responsibility in reducing credit risk for SMEs in Indonesia.

---

Building Machine Learning Systems for Automated ESG Scoring
10.3905/jesg.2021.1.010
Although investing in environmental, social, and governance (ESG)-driven portfolios is already a large and growing portion of global assets under management, applications of quantitative techniques to improve and standardize ESG scoring and the construction of ESG portfolios are underutilized. In this article, the authors propose an approach to automatically convert unstructured text data into ESG scores by using the latest advances in deep learning for natural language processing (NLP). They also show how a state-of-the-art NLP technique, BERT, can be incorporated to improve the accuracy of assessing relevance and content of documents in an ESG context using social media data as an example and discuss the relevance of this approach to automating ESG scoring and constructing ESG portfolios. TOPICS: ESG investing, big data/machine learning, portfolio construction Key Findings ▪ The authors demonstrate the feasibility and advantages to applying state-of-the art natural language processing (NLP) to identify environmental, social, governance (ESG) risks using social media data. ▪ The authors discuss how advances in modern NLP can be leveraged to continuously build up algorithmic capabilities for processing ESG-relevant documents, by leveraging the capabilities of deep-learning models for learning general representations of text data, which can then be applied across many tasks in the ESG domain. ▪ The authors discuss results of NLP models can be used for creating aggregated ESG scores, as well design considerations for creating fully or semi-autonomous ESG scoring systems.

---

Intrinsic impediments to category captainship collaboration
10.3934/jimo.2016007
Category captainship, the approach where retailers use manufacturer-retailer collaboration, is a common way to leverage resources and capabilities in order to improve the sales/shelf performance ratio. However, evidence suggests that the depth and effectiveness of category captains and collaboration in retail are not as high as theory or best practice would predict. Suppliers and retailers suspect each other of opportunistic behaviour detrimental to both. In a stylized dyadic supply chain model prior to the effective contracting of the category captain, we show why information asymmetry between both is preferred: the retailer will hint at or develop retaliatory power to keep the supplier in check whereas the supplier will try to extract a rent by taking advantage of available information about relationship specific investment. We model single-period interaction when the retailer has to invest in relationship specific assets and alternative category manager grooming. We provide normative and positive support both to the captain's potential opportunistic behaviour as well as the retailer's investment decision in alternative captains and monitoring ability. In a two-period extension, we show how the retailer can discipline the captain ex ante. The model and its results complement and extend research in pre-contractual category captainship and supplier-retailer collaboration and coordination. They represent a departure from the usual vision in which sharing information and collaborating generate higher supply chain rent.

---

Tasks, Approaches, and Avenues of Opinion Mining, Sentiment Analysis, and Emotion Analysis
10.4018/978-1-6684-6303-1.ch005
Every successful business aims to know how customers feel about its brands, services, and products. People freely express their views, ideas, sentiments, and opinions on social media for their day-to-day activities, for product reviews, for surveys, and even for their public opinions. This process provides a fortune of valuable resources about the market for any type of business. Unfortunately, it's impossible to manually analyze this massive quantity of information. Sentiment analysis (SA) and opinion mining (OM), as new fields of natural language processing, have the potential benefit of analyzing such a huge amount of data. SA or OM is the computational treatment of opinions, sentiments, and subjectivity of text. This chapter introduces the reader to a survey of different text SA and OM proposed techniques and approaches. The authors discuss in detail various approaches to perform a computational treatment for sentiments and opinions with their strengths and drawbacks.

---

Harnessing AI and Blockchain in Sustainability Assurance
10.4018/979-8-3373-0117-4.ch007
The integration of artificial intelligence (AI) and blockchain technology is revolutionizing sustainability assurance, setting new benchmarks for transparency, accuracy, and accountability. This article explores current trends and innovative applications of AI and blockchain in verifying corporate sustainability efforts, from automated data analysis to secure, decentralized record-keeping. By leveraging AI for real-time monitoring and data insights, organizations can better track environmental and social impacts, while blockchain ensures data integrity and traceability across supply chains. These advancements enhance stakeholder trust and support regulatory compliance, addressing increasing demands for robust sustainability verification in global markets. Furthermore, the paper discusses challenges such as data privacy and implementation costs, offering insights into the potential long-term impact of these technologies on corporate governance. By examining recent case studies and market trends, this article outlines a future where technology-driven assurance processes.

---

Reimagining Compliance
10.4018/979-8-3373-0209-6.ch013
This chapter develops an analytical model for Explainable AI (XAI) designed to support major types of banking regulatory compliance audits. Financial institutions increasingly rely on AI for efficiency in compliance tasks, but the opacity of complex models creates challenges in meeting regulatory transparency requirements. The proposed mathematical framework incorporates explainability as a quantifiable constraint in the optimization of AI models across five critical audit categories: CAMELS examinations, BSA/AML compliance, Consumer Compliance, IT Audits, and Internal Audits. By quantifying explainability through information-theoretic measures and Shapley values, the model balances predictive performance with regulatory requirements. The methodology is validated through a numerical example demonstrating significant improvement in the objective function while satisfying all regulatory constraints.The chapter addresses implementation challenges across different regulatory jurisdictions, human-AI collaboration considerations, and resource requirements for institutions of varying sizes.

---

Decoding Emotions in the Digital Age Through Sentiment Analysis for Customer Service Innovation
10.4018/979-8-3373-3658-9.ch002
In today's digital landscape, understanding customer emotions is vital for delivering exceptional service experiences. This chapter explores how sentiment analysis—merging Natural Language Processing (NLP), Artificial Intelligence (AI), and emotion analytics—is transforming customer service innovation. It outlines the evolution and core frameworks, including data collection, feature extraction, sentiment classification, and transformer models. The chapter emphasizes multimodal analysis that integrates text, audio, and visual inputs for deeper emotional insight. It also discusses real-time, multilingual applications embedded in tools like chatbots, CRMs, and helpdesks. By linking sentiment insights to business outcomes such as satisfaction, loyalty, and advocacy, it highlights a shift from reactive to proactive service. Ethical and technical challenges are addressed, alongside practical use cases and tool evaluations. This chapter positions sentiment analysis as a path to more human-centered, data-informed engagement.

---

AI-Driven Sustainable Business in CSR Reporting Regime
10.4018/979-8-3693-3478-2.ch015
This chapter explores the integration of AI-driven technologies in corporate social responsibility (CSR) reporting and its impact on sustainable business practices. As global emphasis on sustainability grows, AI technologies like machine learning and natural language processing are enhancing the efficiency, accuracy, and transparency of CSR reports. This systematic literature review synthesizes findings from 63 studies, highlighting how AI optimizes data management, predicts sustainability trends, and addresses ethical challenges in CSR. The study offers insights into the strategic and ethical implications of AI adoption in CSR reporting, emphasizing the potential of AI to transform sustainable business models.

---

Analysing the Textual Analysis of Different NLP Techniques to Classify the Stability of Corporate Reporting
10.4018/979-8-3693-7230-2.ch014
Presently, in certain cases, unstructured material like annual reports, news articles, and earnings call transcripts may give valuable sustainability data. Currently, scholars and specialists have started the collection of data from many sources employing a wide array of natural language processing (NLP) techniques. Although nearby many benefits to be obtained from these efforts, studies that use these techniques often fail to consider the accuracy and effectiveness of the selected approach in capturing sustainability information from text. This method is troublesome due to the variability in outcomes that arise from using multiple NLP algorithms for information extraction. Therefore, the selection of a particular approach might have an impact on the output of an application and subsequently influence the conclusions that users get from their findings. This research investigates the impact of several NLP techniques on the accuracy and excellence of retrieved information. The researcher specifically analyses and contrasts four main methods.

---

AI/ML Models With the Framework of Sustainability
10.4018/979-8-3693-8337-7.ch021
Reshaping of modern society artificial intelligence and machine learning continue to play vital roles in ecofriendly, social, &amp; economic sustainability. This research paper presents a laborious comparative study that investigates into the sustainability aspects of various AI/ML models. In this paper methodology involves a complete investigation of environmental factors, including energy consumption, carbon emissions, economic factors focusing on cost-effectiveness resource competence. Researcher analyzed data from a various set of models, a range of sources, and apply comparative study for measure their sustainability performance. The study on valuable insights into sustainability in different AI/ML models off on their strengths and weaknesses. The context of environmental protection, social responsibility, and economic capability. Researcher discuss the ethical implications and potential margins connected, contributes and deep understanding of the opportunities in the AI/ML technologies with sustainable approach.

---

Design of an Enterprise Public Opinion Monitoring System Based on Natural Language Processing
10.4018/jgim.381306
The widespread influence of social media and online discourse has made enterprise public opinion monitoring essential, yet existing sentiment analysis models struggle with sarcasm detection, multimodal sentiment integration, and high-risk user identification. Additionally, traditional text-based models face cross-domain generalization issues, limiting their effectiveness in real-world sentiment trend forecasting. To address these challenges, we propose HDLU-POMS (Hybrid Deep Learning and User Profiling-Enhanced Public Opinion Monitoring System), a novel framework that integrates Convolutional Neural Network-Long Short-Term Memory-Attention (CNN-LSTM-Attention) sentiment modeling, multimodal sarcasm detection, user profiling, and feature fusion. By incorporating audio-visual cues for sarcasm interpretation, behavioral metadata for user profiling, and feature fusion techniques for adaptability, HDLU-POMS enhances sentiment classification and trend forecasting.

---

The Impact of Firm-Level Shareholder Protections on Abnormal Returns on Insider Trading
10.4172/2167-0234.1000114
This study is the first to investigate the relationship between firm-level shareholder protections and abnormal returns on insider trading. We thereby extend the few studies that have analysed insider trading from a shareholder protection perspective. The novelty of this study is its concentration on firm-level shareholder protection. Our results show that firmlevel shareholder protections have a significantly positive impact on abnormal returns on insider purchases, indicating that firm-level shareholder protection is more influential than country-level shareholder protection. We found support that the information content hypothesis is valid for explanation of abnormal return on insider trading, which is in line with earlier studies. This result is an indication that insider purchases in firms that have adopt strict corporate governance rules are viewed by the market as trustworthy leading to positive market reactions on insider trading.

---

Credit supplementation system for unlocking SME and startup access to finance : The case of Japan
10.4324/9780429401060-11
The credit supplementation system in Japan is composed of a credit guarantee system operated by the 51 credit guarantee corporations established throughout Japan and a credit insurance system operated by the Japan Finance Corporation. The credit guarantee system and the credit insurance system function together to enable the credit supplementation system in Japan to fulfill the important task of facilitating the smooth supply of business funds for small and medium-sized enterprises (SMEs), and it plays a vital role in the Japanese government’s SME financial policy. The Japan Finance Corporation is a comprehensive policy-based financial institution fully owned by the Japanese government, and its credit insurance sub-unit reinsures guaranteed liabilities provided by credit guarantee corporations.                     Through repeated revisions, the current credit insurance system has functioned and adapted to support the credit guarantee system and has greatly contributed to establishment, growth/development, and preventing bankruptcy of SMEs, including at times of crisis. The Small Business Credit Insurance Law was amended in June 2017, and the system has been expanded to meet the financial demands that arise at various SME stages, from startup, business expansion, and business succession, to times of credit squeeze due to a large-scale economic crisis or disaster.

---

Research on small and medium-sized enterprises and sustainability
10.4324/9780429426377-2-2
Having set the scene and established some bridging points between business and sustainability, this chapter presents a strong case as to why the focus of this book should be on small and medium-sized enterprises (SMEs). While multinational enterprises (MNEs) have been highly visible under the environmental radar, and accordingly under increasing public scrutiny regarding their sustainability impacts, the SMEs have not been much in focus. In total, the effect of the roughly 400 million SMEs are critical to shaping a sustainable planet and, therefore, should not be overlooked. The chapter discusses environmental responsibility in relation to SMEs. The chapter concludes that society cannot afford not to pose the same environmental responsibility to SMEs as is being done to large enterprises. In doing so, however, it is vital, that it be recognised that SMEs are structurally, economically and culturally different.

---

Big data analytics for developing sustainable capabilities of digital business ecosystems
10.4337/9781839107191.00041
Research has indicated that alleviating unprecedented sustainability challenges requires mobilization of innovative digital technologies and cooperation of multiple actors. By drawing on and contributing to the literature on digital business ecosystems, this qualitative exploratory research empirically examines the synergies between two global imperatives, namely, sustainable development and digitalization. Specifically, this study illuminates the role of big data analytics in increasing sustainable capabilities in the online food delivery industry. The findings illuminate how data analytics enable and support human and non-human (i.e., artificial intelligence (AI) algorithms) decision makers, leading to the optimization of various business functions and an increase in sustainable capabilities of the digital business ecosystem. Moreover, this study introduces the concept of circular participation and highlights its crucial role in achieving such positive outcomes.

---

Development of an Automatic Summarization System based on Large Language Models for Annual Report Analysis
10.47709/brilliance.v5i2.6772
The increasing interest in stock market investment in Indonesia has highlighted a significant challenge for retail investors: the difficulty of analyzing lengthy and complex corporate annual reports. These documents, essential for fundamental analysis, are often hundreds of pages long and contain detailed narrative sections that require considerable time and effort to comprehend. This research addresses this issue by developing an automatic summarization system using a Large Language Model (LLM) to generate concise and insightful summaries of such reports. The primary objective was to develop and evaluate an LLM-based system specifically adapted for the structure and content of annual reports. The method involved creating a tailored dataset comprising 2,008 narrative text excerpts and their corresponding manual summaries sourced from the annual reports of companies listed on the Indonesia Stock Exchange (IDX). The open-source Llama-3.2-3B-Instruct model was then fine-tuned using the Parameter-Efficient Fine-Tuning (PEFT) technique, specifically Low-Rank Adaptation (LoRA). The research results demonstrated a significant improvement in the model's performance after fine-tuning. Quantitative evaluation using ROUGE metrics showed a relative increase of 18.63% in ROUGE-1, 44.45% in ROUGE-2, and 33.83% in ROUGE-L compared to the base model. Qualitative analysis confirmed that the fine-tuned model was capable of generating informative and relevant summaries aligned with the context of annual report analysis. In conclusion, this study demonstrates that fine-tuning LLMs with document-specific data is an effective approach for specialized tasks such as annual report summarization.

---

Enhancing Aspect-based Sentiment Analysis in Visitor Review using Semantic Similarity
10.47738/jads.v5i2.249
The global economy greatly depends on the tourism industry, which fosters job opportunities and stimulates economic development. With the growing reliance of tourists on online platforms for guidance, evaluations of tourist destinations have gained heightened significance. These assessments, frequently expressed through user-generated content, offer valuable perspectives on customer experiences, viewpoints, and levels of satisfaction. Nevertheless, analyzing and interpreting these reviews can pose difficulties because of the unstructured or semi-structured nature of user-generated content. Conventional sentiment analysis methods might not adequately grasp the intricacies and particular aspects of tourism encounters that user convey in their reviews. The efficacy of sentiment analysis can be augmented by integrating semantic similarity. This study explores methods to enhance aspect-based sentiment analysis within tourism reviews by utilizing semantic similarity approaches. Five aspects have been curated, representing keywords frequently reviewed by visitors to the tourist attraction. These aspects encompass scenery, dusk, surf, amenities, and sanitation. Based on the data analysis, F-Measure values with Semantic Similarity tend to increase for the scenery and dusk aspects. This is because in the sample data used, visitor reviews for the scenery and dusk categories may use other words that are semantically similar. The sample data used for these categories is also quite extensive, resulting in a better classification model for both categories. While it is valuable to analyze user-generated content data from visitor reviews, it's important to consider the limitations and potential biases associated with this data. The classification results per aspect need to be further reviewed in more depth. What aspects lead visitors to give positive reviews will certainly be maintained and even improved by stakeholders. Similarly, for negative review outcomes, it is necessary to investigate more deeply the factors contributing to visitor dissatisfaction so that they can be addressed by stakeholders.

---

Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach
10.48448/61cj-ma69
InfluenceMap's LobbyMap Platform monitors the climate policy engagement of over 500 companies and 250 industry associations, assessing each entity's support or opposition to science-based policy pathways for achieving the Paris Agreement's goal of limiting global warming to 1.5°C. Although InfluenceMap has made progress with automating key elements of the analytical workflow, a significant portion of the assessment remains manual, making it time- and labor-intensive and susceptible to human error. We propose an AI-assisted framework to accelerate the monitoring of corporate climate policy engagement by leveraging Retrieval-Augmented Generation to automate the most time-intensive extraction of relevant evidence from large-scale textual data. Our evaluation shows that a combination of layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies yields the best performance in extracting and classifying evidence from multilingual corporate documents. We conclude that while the automated RAG system effectively accelerates evidence extraction, the nuanced nature of the analysis necessitates a human-in-the-loop approach where the technology augments, rather than replaces, expert judgment to ensure accuracy.

---

ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge
10.48448/n075-ah42
We introduce \textbf{ESGenius}, a comprehensive benchmark for evaluating and enhancing the proficiency of Large Language Models (LLMs) in Environmental, Social and Governance (ESG) and sustainability-focused question answering. \textbf{ESGenius} comprises three key components: (i) \textbf{ESGenius-QA}, a collection of \textbf{1,136} MCQs generated by LLMs and rigorously validated by domain experts, covering a broad range of ESG pillars and sustainability topics. Each question is systematically linked to its corresponding source text, enabling transparent evaluation and supporting Retrieval-Augmented Generation (RAG) methods; and (ii) \textbf{ESGenius-Corpus}, a meticulously curated repository of \textbf{225} foundational frameworks, standards, reports, and recommendation documents from \textbf{7} authoritative sources. Moreover, to fully assess the capabilities and adaptation potential of the model, we implement a rigorous two-stage evaluation protocol---\emph{Zero-Shot} and \emph{RAG}. Extensive experiments across \textbf{50} LLMs (ranging from 0.5B to 671B parameters) demonstrate that state-of-the-art models achieve only moderate performance in zero-shot settings, with accuracies mostly around 55-70%, highlighting ESGenius's challenging nature. However, models employing RAG demonstrate significant performance improvements, particularly for smaller models. For example, DeepSeek-R1-Distill-Qwen 14B improves from 63.82% in the zero-shot setting to 80.46% with RAG. These results demonstrate the necessity of grounding responses in authoritative sources for enhanced ESG understanding. To our best of knowledge, ESGenius is the first benchmark curated for LLMs and the relevant enhancement technologies, focusing on ESG and sustainability topics.

---

IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages
10.48448/pgkn-4z96
Although region-specific large language models (LLMs) are increasingly developed, their safety remains underexplored, particularly in culturally diverse settings like Indonesia, where sensitivity to local norms is essential and highly valued by the community. In this work, we present IndoSafety, the first high-quality, human-verified safety evaluation dataset tailored for the Indonesian context, covering five language varieties: formal and colloquial Indonesian, along with three major local languages: Javanese, Sundanese, and Minangkabau. IndoSafety is constructed by extending prior safety frameworks to develop a taxonomy that captures Indonesia’s sociocultural context. We find that existing Indonesian-centric LLMs often generate unsafe outputs, particularly in colloquial and local language settings, while fine-tuning on IndoSafety significantly improves safety while preserving task performance. Our work highlights the critical need for culturally grounded safety evaluation and provides a concrete step toward responsible LLM deployment in multilingual settings. Warning: This paper contains example data that may be offensive, harmful, or biased.

---

ML-Promise: A Multilingual Dataset for Corporate Promise Verification
10.48448/vqzg-em46
Promises made by politicians, corporate leaders, and public figures have a significant impact on public perception, trust, and institutional reputation. However, the complexity and volume of such commitments, coupled with difficulties in verifying their fulfillment, necessitate innovative methods for assessing their credibility. This paper introduces the concept of Promise Verification, a systematic approach involving steps such as promise identification, evidence assessment, and the evaluation of timing for verification. We propose the first multilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Korean, aimed at facilitating in-depth verification of promises, particularly in the context of Environmental, Social, and Governance (ESG) reports. Given the growing emphasis on corporate environmental contributions, this dataset addresses the challenge of evaluating corporate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of public commitments across multiple languages and domains.

---

LORAXBENCH: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages
10.48448/w902-xw16
As one of the world's most populous countries, with 700 languages spoken, Indonesia is behind in terms of NLP progress. We introduce LORAXBENCH, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. Our dataset cover 20 languages, with the addition of two formality registers for three languages. We evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging. We note a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, we show that a change in register affects model performance, especially with registers not commonly found in social media, such as high-level politeness `Krama' Javanese.

---

LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document
  Understanding
10.48550/arxiv.2012.14740
Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 $\to$ 0.8420), CORD (0.9493 $\to$ 0.9601), SROIE (0.9524 $\to$ 0.9781), Kleister-NDA (0.8340 $\to$ 0.8520), RVL-CDIP (0.9443 $\to$ 0.9564), and DocVQA (0.7295 $\to$ 0.8672). We made our model and code publicly available at \url{https://aka.ms/layoutlmv2}.

---

Ontology-based Feature Selection: A Survey
10.48550/arxiv.2104.07720
The SemanticWeb emerged as an extension to the traditional Web, towards adding meaning to a distributed Web of structured and linked data. At its core, the concept of ontology provides the means to semantically describe and structure information and data and expose it to software and human agents in a machine and human-readable form. For software agents to be realized, it is crucial to develop powerful artificial intelligence and machine learning techniques, able to extract knowledge from information and data sources and represent it in the underlying ontology. This survey aims to provide insight into key aspects of ontology-based knowledge extraction, from various sources such as text, images, databases and human expertise, with emphasis on the task of feature selection. First, some of the most common classification and feature selection algorithms are briefly presented. Then, selected methodologies, which utilize ontologies to represent features and perform feature selection and classification, are described. The presented examples span diverse application domains, e.g., medicine, tourism, mechanical and civil engineering, and demonstrate the feasibility and applicability of such methods.

---

IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural
  Language Generation
10.48550/arxiv.2104.08200
Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resource -- yet widely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks -- despite using only one-fifth the parameters of a larger multilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, local languages to achieve more efficient learning and faster inference for very low-resource languages like Javanese and Sundanese.

---

Fantastically Ordered Prompts and Where to Find Them: Overcoming
  Few-Shot Prompt Order Sensitivity
10.48550/arxiv.2104.08786
When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are "fantastic" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.

---

FEVEROUS: Fact Extraction and VERification Over Unstructured and
  Structured information
10.48550/arxiv.2106.05707
Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.

---

Eider: Empowering Document-level Relation Extraction with Efficient
  Evidence Extraction and Inference-stage Fusion
10.48550/arxiv.2106.08657
Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED).

---

Mastering the Explicit Opinion-role Interaction: Syntax-aided Neural
  Transition System for Unified Opinion Role Labeling
10.48550/arxiv.2110.02001
Unified opinion role labeling (ORL) aims to detect all possible opinion structures of 'opinion-holder-target' in one shot, given a text. The existing transition-based unified method, unfortunately, is subject to longer opinion terms and fails to solve the term overlap issue. Current top performance has been achieved by employing the span-based graph model, which however still suffers from both high model complexity and insufficient interaction among opinions and roles. In this work, we investigate a novel solution by revisiting the transition architecture, and augmenting it with a pointer network (PointNet). The framework parses out all opinion structures in linear-time complexity, meanwhile breaks through the limitation of any length of terms with PointNet. To achieve the explicit opinion-role interactions, we further propose a unified dependency-opinion graph (UDOG), co-modeling the syntactic dependency structure and the partial opinion-role structure. We then devise a relation-centered graph aggregator (RCGA) to encode the multi-relational UDOG, where the resulting high-order representations are used to promote the predictions in the vanilla transition system. Our model achieves new state-of-the-art results on the MPQA benchmark. Analyses further demonstrate the superiority of our methods on both efficacy and efficiency.

---

Aspect Based Sentiment Analysis Using Spectral Temporal Graph Neural
  Network
10.48550/arxiv.2202.06776
The objective of Aspect Based Sentiment Analysis is to capture the sentiment of reviewers associated with different aspects. However, complexity of the review sentences, presence of double negation and specific usage of words found in different domains make it difficult to predict the sentiment accurately and overall a challenging natural language understanding task. While recurrent neural network, attention mechanism and more recently, graph attention based models are prevalent, in this paper we propose graph Fourier transform based network with features created in the spectral domain. While this approach has found considerable success in the forecasting domain, it has not been explored earlier for any natural language processing task. The method relies on creating and learning an underlying graph from the raw data and thereby using the adjacency matrix to shift to the graph Fourier domain. Subsequently, Fourier transform is used to switch to the frequency (spectral) domain where new features are created. These series of transformation proved to be extremely efficient in learning the right representation as we have found that our model achieves the best result on both the SemEval-2014 datasets, i.e., "Laptop" and "Restaurants" domain. Our proposed model also found competitive results on the two other recently proposed datasets from the e-commerce domain.

---

A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges
10.48550/arxiv.2203.01054
As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people's opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks are introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.

---

BERTopic: Neural topic modeling with a class-based TF-IDF procedure
10.48550/arxiv.2203.05794
Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.

---

Survey of Aspect-based Sentiment Analysis Datasets
10.48550/arxiv.2204.05232
Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.

---

Zero-shot Aspect-level Sentiment Classification via Explicit Utilization of Aspect-to-Document Sentiment Composition
10.48550/arxiv.2209.02276
As aspect-level sentiment labels are expensive and labor-intensive to acquire, zero-shot aspect-level sentiment classification is proposed to learn classifiers applicable to new domains without using any annotated aspect-level data. In contrast, document-level sentiment data with ratings are more easily accessible. In this work, we achieve zero-shot aspect-level sentiment classification by only using document-level reviews. Our key intuition is that the sentiment representation of a document is composed of the sentiment representations of all the aspects of that document. Based on this, we propose the AF-DSC method to explicitly model such sentiment composition in reviews. AF-DSC first learns sentiment representations for all potential aspects and then aggregates aspect-level sentiments into a document-level one to perform document-level sentiment classification. In this way, we obtain the aspect-level sentiment classifier as the by-product of the document-level sentiment classifier. Experimental results on aspect-level sentiment classification benchmarks demonstrate the effectiveness of explicit utilization of sentiment composition in document-level sentiment classification. Our model with only 30k training data outperforms previous work utilizing millions of data.

---

Sentiment Analysis of ESG disclosures on Stock Market
10.48550/arxiv.2210.00731
In this paper, we look at the impact of Environment, Social and Governance related news articles and social media data on the stock market performance. We pick four stocks of companies which are widely known in their domain to understand the complete effect of ESG as the newly opted investment style remains restricted to only the stocks with widespread information. We summarise live data of both twitter tweets and newspaper articles and create a sentiment index using a dictionary technique based on online information for the month of July, 2022. We look at the stock price data for all the four companies and calculate the percentage change in each of them. We also compare the overall sentiment of the company to its percentage change over a specific historical period.

---

Instruction Tuning for Few-Shot Aspect-Based Sentiment Analysis
10.48550/arxiv.2210.06629
Aspect-based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task which involves four elements from user-generated texts:aspect term, aspect category, opinion term, and sentiment polarity. Most computational approaches focus on some of the ABSA sub-taskssuch as tuple (aspect term, sentiment polarity) or triplet (aspect term, opinion term, sentiment polarity) extraction using either pipeline or joint modeling approaches. Recently, generative approaches have been proposed to extract all four elements as (one or more) quadrupletsfrom text as a single task. In this work, we take a step further and propose a unified framework for solving ABSA, and the associated sub-tasksto improve the performance in few-shot scenarios. To this end, we fine-tune a T5 model with instructional prompts in a multi-task learning fashion covering all the sub-tasks, as well as the entire quadruple prediction task. In experiments with multiple benchmark datasets, we show that the proposed multi-task prompting approach brings performance boost (by absolute 8.29 F1) in the few-shot learning setting.

---

NusaCrowd: Open Source Initiative for Indonesian NLP Resources
10.48550/arxiv.2212.09648
We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.

---

Long Text and Multi-Table Summarization: Dataset and Method
10.48550/arxiv.2302.03815
Automatic document summarization aims to produce a concise summary covering the input document's salient information. Within a report document, the salient information can be scattered in the textual and non-textual content. However, existing document summarization datasets and methods usually focus on the text and filter out the non-textual content. Missing tabular data can limit produced summaries' informativeness, especially when summaries require covering quantitative descriptions of critical metrics in tables. Existing datasets and methods cannot meet the requirements of summarizing long text and multiple tables in each report. To deal with the scarcity of available data, we propose FINDSum, the first large-scale dataset for long text and multi-table summarization. Built on 21,125 annual reports from 3,794 companies, it has two subsets for summarizing each company's results of operations and liquidity. To summarize the long text and dozens of tables in each report, we present three types of summarization methods. Besides, we propose a set of evaluation metrics to assess the usage of numerical information in produced summaries. Dataset analyses and experimental results indicate the importance of jointly considering input textual and tabular data when summarizing report documents.

---

Improving Interpretability of Deep Sequential Knowledge Tracing Models
  with Question-centric Cognitive Representations
10.48550/arxiv.2302.06885
Knowledge tracing (KT) is a crucial technique to predict students' future performance by observing their historical learning processes. Due to the powerful representation ability of deep neural networks, remarkable progress has been made by using deep learning techniques to solve the KT problem. The majority of existing approaches rely on the \emph{homogeneous question} assumption that questions have equivalent contributions if they share the same set of knowledge components. Unfortunately, this assumption is inaccurate in real-world educational scenarios. Furthermore, it is very challenging to interpret the prediction results from the existing deep learning based KT models. Therefore, in this paper, we present QIKT, a question-centric interpretable KT model to address the above challenges. The proposed QIKT approach explicitly models students' knowledge state variations at a fine-grained level with question-sensitive cognitive representations that are jointly learned from a question-centric knowledge acquisition module and a question-centric problem solving module. Meanwhile, the QIKT utilizes an item response theory based prediction layer to generate interpretable prediction results. The proposed QIKT model is evaluated on three public real-world educational datasets. The results demonstrate that our approach is superior on the KT prediction task, and it outperforms a wide range of deep learning based KT models in terms of prediction accuracy with better model interpretability. To encourage reproducible results, we have provided all the datasets and code at \url{https://pykt.org/}.

---

InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis
10.48550/arxiv.2302.08624
We introduce InstructABSA, an instruction learning paradigm for Aspect-Based Sentiment Analysis (ABSA) subtasks.Our method introduces positive, negative, and neutral examples to each training sample, and instruction tune the model (Tk-Instruct) for ABSA subtasks, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on Term Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair Extraction (ASPE) subtasks.In particular, InstructABSA outperforms the previous state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the Rest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37% points, surpassing 7x larger models.We get competitive results on AOOE, AOPE, AOSTE, and ACOSQE subtasks indicating strong generalization ability to all subtasks. Exploring sample efficiency reveals that just 50% train data is required to get competitive results with other instruction tuning approaches. Lastly, we assess the quality of instructions and observe that InstructABSA’s performance experiences a decline of ~10% when adding misleading examples

---

Structured Sentiment Analysis as Transition-based Dependency Parsing
10.48550/arxiv.2305.05311
Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a dependency parsing task. Although we can find in the literature how transition-based algorithms excel in dependency parsing in terms of accuracy and efficiency, all proposed attempts to tackle SSA following that approach were based on graph-based models. In this article, we present the first transition-based method to address SSA as dependency parsing. Specifically, we design a transition system that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions. To effectively implement our final transition-based model, we resort to a Pointer Network architecture as a backbone. From an extensive evaluation, we demonstrate that our model offers the best performance to date in practically all cases among prior dependency-based methods, and surpass recent task-specific techniques on the most challenging datasets. We additionally include an in-depth analysis and empirically prove that the overall time-complexity cost of our approach is quadratic in the sentence length, being more efficient than top-performing graph-based parsers.

---

A Weak Supervision Approach for Few-Shot Aspect Based Sentiment Analysis
10.48550/arxiv.2305.11979
We explore how weak supervision on abundant unlabeled data can be leveraged to improve few-shot performance in aspect-based sentiment analysis (ABSA) tasks. We propose a pipeline approach to construct a noisy ABSA dataset, and we use it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. We test the resulting model on three widely used ABSA datasets, before and after fine-tuning. Our proposed method preserves the full fine-tuning performance while showing significant improvements (15.84 absolute F1) in the few-shot learning scenario for the harder tasks. In zero-shot (i.e., without fine-tuning), our method outperforms the previous state of the art on the aspect extraction sentiment classification (AESC) task and is, additionally, capable of performing the harder aspect sentiment triplet extraction (ASTE) task.

---

MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction
10.48550/arxiv.2305.12627
Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MVP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MVP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MVP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MVP.

---

Sentiment Analysis in the Era of Large Language Models: A Reality Check
10.48550/arxiv.2305.15005
Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.

---

Measuring Your ASTE Models in The Wild: A Diversified Multi-domain Dataset For Aspect Sentiment Triplet Extraction
10.48550/arxiv.2305.17448
Aspect Sentiment Triplet Extraction (ASTE) is widely used in various applications. However, existing ASTE datasets are limited in their ability to represent real-world scenarios, hindering the advancement of research in this area. In this paper, we introduce a new dataset, named DMASTE, which is manually annotated to better fit real-world scenarios by providing more diverse and realistic reviews for the task. The dataset includes various lengths, diverse expressions, more aspect types, and more domains than existing datasets. We conduct extensive experiments on DMASTE in multiple settings to evaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is a more challenging ASTE dataset. Further analyses of in-domain and cross-domain settings provide promising directions for future research. Our code and dataset are available at https://github.com/NJUNLP/DMASTE.

---

MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis
10.48550/arxiv.2306.16956
Aspect-based sentiment analysis is a long-standing research interest in the field of opinion mining, and in recent years, researchers have gradually shifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA tasks. However, the datasets currently used in the research are limited to individual elements of specific tasks, usually focusing on in-domain settings, ignoring implicit aspects and opinions, and with a small data scale. To address these issues, we propose a large-scale Multi-Element Multi-Domain dataset (MEMD) that covers the four elements across five domains, including nearly 20,000 review sentences and 30,000 quadruples annotated with both explicit and implicit aspects and opinions for ABSA research. Meanwhile, we conduct experiments on multiple ABSA subtasks under the open domain setting to verify the effectiveness of several generative and non-generative baselines, and the results show that open domain ABSA as well as mining implicit aspects and opinions remain ongoing challenges to be addressed.

---

Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model
10.48550/arxiv.2307.14785
This paper presents a series of approaches aimed at enhancing the performance of Aspect-Based Sentiment Analysis (ABSA) by utilizing extracted semantic information from a Semantic Role Labeling (SRL) model. We propose a novel end-to-end Semantic Role Labeling model that effectively captures most of the structured semantic information within the Transformer hidden state. We believe that this end-to-end model is well-suited for our newly proposed models that incorporate semantic information. We evaluate the proposed models in two languages, English and Czech, employing ELECTRA-small models. Our combined models improve ABSA performance in both languages. Moreover, we achieved new state-of-the-art results on the Czech ABSA.

---

Neuro-Symbolic RDF and Description Logic Reasoners: The State-Of-The-Art and Challenges
10.48550/arxiv.2308.04814
Ontologies are used in various domains, with RDF and OWL being prominent standards for ontology development. RDF is favored for its simplicity and flexibility, while OWL enables detailed domain knowledge representation. However, as ontologies grow larger and more expressive, reasoning complexity increases, and traditional reasoners struggle to perform efficiently. Despite optimization efforts, scalability remains an issue. Additionally, advancements in automated knowledge base construction have created large and expressive ontologies that are often noisy and inconsistent, posing further challenges for conventional reasoners. To address these challenges, researchers have explored neuro-symbolic approaches that combine neural networks' learning capabilities with symbolic systems' reasoning abilities. In this chapter,we provide an overview of the existing literature in the field of neuro-symbolic deductive reasoning supported by RDF(S), the description logics EL and ALC, and OWL 2 RL, discussing the techniques employed, the tasks they address, and other relevant efforts in this area.

---

Comparative Analysis of Contextual Relation Extraction based on Deep
  Learning Models
10.48550/arxiv.2309.06814
Contextual Relation Extraction (CRE) is mainly used for constructing a knowledge graph with a help of ontology. It performs various tasks such as semantic search, query answering, and textual entailment. Relation extraction identifies the entities from raw texts and the relations among them. An efficient and accurate CRE system is essential for creating domain knowledge in the biomedical industry. Existing Machine Learning and Natural Language Processing (NLP) techniques are not suitable to predict complex relations from sentences that consist of more than two relations and unspecified entities efficiently. In this work, deep learning techniques have been used to identify the appropriate semantic relation based on the context from multiple sentences. Even though various machine learning models have been used for relation extraction, they provide better results only for binary relations, i.e., relations occurred exactly between the two entities in a sentence. Machine learning models are not suited for complex sentences that consist of the words that have various meanings. To address these issues, hybrid deep learning models have been used to extract the relations from complex sentence effectively. This paper explores the analysis of various deep learning models that are used for relation extraction.

---

OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for Aspect-Based Sentiment Analysis
10.48550/arxiv.2309.13297
Aspect-based sentiment analysis (ABSA) delves into understanding sentiments specific to distinct elements within a user-generated review. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 27,470 sentence-level quadruples and 17,092 review-level tuples. Our initiative seeks to bridge specific observed gaps in existing datasets: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS’s potential and shed light on various ABSA subtasks that OATS can solve, we conducted experiments, establishing initial baselines. We hope the OATS dataset augments current resources, paving the way for an encompassing exploration of ABSA (https://github.com/RiTUAL-UH/OATS-ABSA).

---

Empirical Study of Zero-Shot NER with ChatGPT
10.48550/arxiv.2310.10035
Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs.

---

Large language models for aspect-based sentiment analysis
10.48550/arxiv.2310.18025
Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA [@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We discuss the the cost-performance trade-offs of different models, and analyze the typical errors that they make. Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models. This evidence is relevant for practioners that are faced with the choice of prompt engineering versus fine-tuning when using LLMs for ABSA.

---

Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language
10.48550/arxiv.2311.01757
Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Earlier studies have applied generative pretrained language model for aspect-based sentiment analysis. An example of this is the LEGO-ABSA framework, which effectively utilized these models, specifically in English-based aspect-based sentiment analysis. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Indonesian language. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Indonesian language using generative pretrained language model. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sen-timent analysis model utilizing generative pretrained language model and trained with multitask learning and prompting. Indo LEGO-ABSA is trained with a hotel domain dataset in the Indonesian language. The obtained results include an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction, 86.09% for Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair Extraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term Extraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5 model, specifically mT5, by applying multitask learning to train all tasks within aspect-based sentiment analysis.11All works can be visited in https://github.com/rdyzakya/IndoLEGO-ABSA

---

Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models
10.48550/arxiv.2311.08921
Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations of self-improving does not guarantee further improvement, but the performance might be boosted via more advanced strategies for reliable annotation selection.

---

A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains, Methods, and Trends
10.48550/arxiv.2311.10777
,

---

Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective
10.48550/arxiv.2401.04374
Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a"data-centric"view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining operations on training and testing data across modalities, such as images, text, and tabular data, as well as on training logs, checkpoints, models and other DNN behavior descriptors. In this way, our study offers a comprehensive, data-centric examination of XAI from a lens of data mining methods and applications.

---

Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment
  Analysis
10.48550/arxiv.2402.07787
Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses. Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's superiority over existing ABSA methods.

---

FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis
10.48550/arxiv.2403.01063
Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture fine-grained sentiment across diverse domains. While existing research narrowly focuses on single-domain applications constrained by methodological limitations and data scarcity, the reality is that sentiment naturally traverses multiple domains. Although large language models (LLMs) offer a promising solution for ABSA, it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy. To alleviate this problem, we propose a novel framework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The core insight of FaiMA is to utilize in-context learning (ICL) as a feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA tasks. Specifically, we employ a multi-head graph attention network as a text encoder optimized by heuristic rules for linguistic, domain, and sentiment features. Through contrastive learning, we optimize sentence representations by focusing on these diverse features. Additionally, we construct an efficient indexing mechanism, allowing FaiMA to stably retrieve highly relevant examples across multiple dimensions for any given input. To evaluate the efficacy of FaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive experimental results demonstrate that FaiMA achieves significant performance improvements in multiple domains compared to baselines, increasing F1 by 2.07% on average. Source code and data sets are available at https://github.com/SupritYoung/FaiMA.

---

A Hybrid Approach to Aspect Based Sentiment Analysis Using Transfer Learning
10.48550/arxiv.2403.17254
Aspect-Based Sentiment Analysis ( ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated by LLMs, as they may overlook domain-specific aspect terms. Extensive experimentation on multiple datasets is performed to demonstrate the efficacy of our hybrid method for the tasks of aspect term extraction and aspect sentiment classification.

---

Evaluating Span Extraction in Generative Paradigm: A Reflection on Aspect-Based Sentiment Analysis
10.48550/arxiv.2404.11539
In the era of rapid evolution of generative language models within the realm of natural language processing, there is an imperative call to revisit and reformulate evaluation methodologies, especially in the domain of aspect-based sentiment analysis (ABSA). This paper addresses the emerging challenges introduced by the generative paradigm, which has moderately blurred traditional boundaries between understanding and generation tasks. Building upon prevailing practices in the field, we analyze the advantages and shortcomings associated with the prevalent ABSA evaluation paradigms. Through an in-depth examination, supplemented by illustrative examples, we highlight the intricacies involved in aligning generative outputs with other evaluative metrics, specifically those derived from other tasks, including question answering. While we steer clear of advocating for a singular and definitive metric, our contribution lies in paving the path for a comprehensive guideline tailored for ABSA evaluations in this generative paradigm. In this position paper, we aim to provide practitioners with profound reflections, offering insights and directions that can aid in navigating this evolving landscape, ensuring evaluations that are both accurate and reflective of generative capabilities.

---

Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis
10.48550/arxiv.2405.05496
Aspect-based sentiment analysis (ABSA) is an important subtask of sentiment analysis, which aims to extract the aspects and predict their sentiments. Most existing studies focus on improving the performance of the target domain by fine-tuning domain-specific models (trained on source domains) based on the target domain dataset. Few works propose continual learning tasks for ABSA, which aim to learn the target domain's ability while maintaining the history domains' abilities. In this paper, we propose a Large Language Model-based Continual Learning (\texttt{LLM-CL}) model for ABSA. First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint. Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge. In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample's domain ID. Extensive experiments over 19 datasets indicate that our \texttt{LLM-CL} model obtains new state-of-the-art performance.

---

Amplifying Aspect-Sentence Awareness: A Novel Approach for Aspect-Based Sentiment Analysis
10.48550/arxiv.2405.13013
Aspect-Based Sentiment Analysis (ABSA) is increasingly crucial in Natural Language Processing (NLP) for applications such as customer feedback analysis and product recommendation systems. ABSA goes beyond traditional sentiment analysis by extracting sentiments related to specific aspects mentioned in the text; existing attention-based models often need help to effectively connect aspects with context due to language complexity and multiple sentiment polarities in a single sentence. Recent research underscores the value of integrating syntactic information, such as dependency trees, to understand long-range syntactic relationships better and link aspects with context. Despite these advantages, challenges persist, including sensitivity to parsing errors and increased computational complexity when combining syntactic and semantic information. To address these issues, we propose Amplifying Aspect-Sentence Awareness (A3SN), a novel technique designed to enhance ABSA through amplifying aspect-sentence awareness attention. Following the transformer's standard process, our innovative approach incorporates multi-head attention mechanisms to augment the model with sentence and aspect semantic information. We added another multi-head attention module: amplify aspect-sentence awareness attention. By doubling its focus between the sentence and aspect, we effectively highlighted aspect importance within the sentence context. This enables accurate capture of subtle relationships and dependencies. Additionally, gated fusion integrates feature representations from multi-head and amplified aspect-sentence awareness attention mechanisms, which is essential for ABSA. Experimental results across three benchmark datasets demonstrate A3SN's effectiveness and outperform state-of-the-art (SOTA) baseline models.

---

ROAST: Review-level Opinion Aspect Sentiment Target Joint Detection
10.48550/arxiv.2405.20274
Aspect-Based Sentiment Analysis (ABSA) has experienced tremendous expansion and diversity due to various shared tasks spanning several languages and fields and organized via SemEval workshops and Germeval. Nonetheless, a few shortcomings still need to be addressed, such as the lack of low-resource language evaluations and the emphasis on sentence-level analysis. To thoroughly assess ABSA techniques in the context of complete reviews, this research presents a novel task, Review-Level Opinion Aspect Sentiment Target (ROAST). ROAST seeks to close the gap between sentence-level and text-level ABSA by identifying every ABSA constituent at the review level. We extend the available datasets to enable ROAST, addressing the drawbacks noted in previous research by incorporating low-resource languages, numerous languages, and a variety of topics. Through this effort, ABSA research will be able to cover more ground and get a deeper comprehension of the task and its practical application in a variety of languages and domains (https://github.com/RiTUAL-UH/ROAST-ABSA).

---

Korean Aspect-Based Sentiment Analysis via Implicit-Feature Alignment with Corpus Filtering
10.48550/arxiv.2407.00342
Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean restaurant reviews are notably lacking in the existing literature. Our research proposes an intuitive and effective framework for ABSA in low-resource languages such as Korean. It optimizes prediction labels by integrating translated benchmark and unlabeled Korean data. Using a model fine-tuned on translated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we applied LaBSE and MSP-based filtering to this pseudo-NLI set as implicit feature, enhancing Aspect Category Detection and Polarity determination through additional training. Incorporating dual filtering, this model bridged dataset gaps, achieving positive results in Korean ABSA with minimal resources. Through additional data injection pipelines, our approach aims to utilize high-resource data and construct effective models within communities, whether corporate or individual, in low-resource language countries. Compared to English ABSA, our framework showed an approximately 3% difference in F1 scores and accuracy. We release the dataset and our code for Korean ABSA, at this link.

---

Neuro-Symbolic AI: Explainability, Challenges, and Future Trends
10.48550/arxiv.2411.04383
Explainability is an essential reason limiting the application of neural networks in many vital fields. Although neuro-symbolic AI hopes to enhance the overall explainability by leveraging the transparency of symbolic learning, the results are less evident than imagined. This article proposes a classification for explainability by considering both model design and behavior of 191 studies from 2013, focusing on neuro-symbolic AI, hoping to inspire scholars who want to understand the explainability of neuro-symbolic AI. Precisely, we classify them into five categories by considering whether the form of bridging the representation differences is readable as their design factor, if there are representation differences between neural networks and symbolic logic learning, and whether a model decision or prediction process is understandable as their behavior factor: implicit intermediate representations and implicit prediction, partially explicit intermediate representations and partially explicit prediction, explicit intermediate representations or explicit prediction, explicit intermediate representation and explicit prediction, unified representation and explicit prediction. We also analyzed the research trends and three significant challenges: unified representations, explainability and transparency, and sufficient cooperation from neural networks and symbolic learning. Finally, we put forward suggestions for future research in three aspects: unified representations, enhancing model explainability, ethical considerations, and social impact.

---

A Comprehensive Evaluation of Large Language Models on Aspect-Based Sentiment Analysis
10.48550/arxiv.2412.02279
Recently, Large Language Models (LLMs) have garnered increasing attention in the field of natural language processing, revolutionizing numerous downstream tasks with powerful reasoning and generation abilities. For example, In-Context Learning (ICL) introduces a fine-tuning-free paradigm, allowing out-of-the-box LLMs to execute downstream tasks by analogy learning without any fine-tuning. Besides, in a fine-tuning-dependent paradigm where substantial training data exists, Parameter-Efficient Fine-Tuning (PEFT), as the cost-effective methods, enable LLMs to achieve excellent performance comparable to full fine-tuning. However, these fascinating techniques employed by LLMs have not been fully exploited in the ABSA field. Previous works probe LLMs in ABSA by merely using randomly selected input-output pairs as demonstrations in ICL, resulting in an incomplete and superficial evaluation. In this paper, we shed light on a comprehensive evaluation of LLMs in the ABSA field, involving 13 datasets, 8 ABSA subtasks, and 6 LLMs. Specifically, we design a unified task formulation to unify ``multiple LLMs for multiple ABSA subtasks in multiple paradigms.'' For the fine-tuning-dependent paradigm, we efficiently fine-tune LLMs using instruction-based multi-task learning. For the fine-tuning-free paradigm, we propose 3 demonstration selection strategies to stimulate the few-shot abilities of LLMs. Our extensive experiments demonstrate that LLMs achieve a new state-of-the-art performance compared to fine-tuned Small Language Models (SLMs) in the fine-tuning-dependent paradigm. More importantly, in the fine-tuning-free paradigm where SLMs are ineffective, LLMs with ICL still showcase impressive potential and even compete with fine-tuned SLMs on some ABSA subtasks.

---

Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models
10.48550/arxiv.2412.12564
Aspect-based sentiment analysis (ABSA), a sequence labeling task, has attracted increasing attention in multilingual contexts. While previous research has focused largely on fine-tuning or training models specifically for ABSA, we evaluate large language models (LLMs) under zero-shot conditions to explore their potential to tackle this challenge with minimal task-specific adaptation. We conduct a comprehensive empirical evaluation of a series of LLMs on multilingual ABSA tasks, investigating various prompting strategies, including vanilla zero-shot, chain-of-thought (CoT), self-improvement, self-debate, and self-consistency, across nine different models. Results indicate that while LLMs show promise in handling multilingual ABSA, they generally fall short of fine-tuned, task-specific models. Notably, simpler zero-shot prompts often outperform more complex strategies, especially in high-resource languages like English. These findings underscore the need for further refinement of LLM-based approaches to effectively address ABSA task across diverse languages.

---

DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for
  Few-Shot Aspect-Based Sentiment Analysis
10.48550/arxiv.2412.14849
Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \textit{key-point-driven} and \textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.

---

M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis
10.48550/arxiv.2502.11824
Aspect-based sentiment analysis (ABSA) is a crucial task in information extraction and sentiment analysis, aiming to identify aspects with associated sentiment elements in text. However, existing ABSA datasets are predominantly English-centric, limiting the scope for multilingual evaluation and research. To bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7 domains and 21 languages, making it the most extensive multilingual parallel dataset for ABSA to date. Our primary focus is on triplet extraction, which involves identifying aspect terms, aspect categories, and sentiment polarities. The dataset is constructed through an automatic translation process with human review to ensure quality. We perform extensive experiments using various baselines to assess performance and compatibility on M-ABSA. Our empirical findings highlight that the dataset enables diverse evaluation tasks, such as multilingual and multi-domain transfer learning, and large language model evaluation, underscoring its inclusivity and its potential to drive advancements in multilingual ABSA research.

---

An Aspect Extraction Framework using Different Embedding Types, Learning Models, and Dependency Structure
10.48550/arxiv.2503.03512
Aspect-based sentiment analysis has gained significant attention in recent years due to its ability to provide fine-grained insights for sentiment expressions related to specific features of entities. An important component of aspect-based sentiment analysis is aspect extraction, which involves identifying and extracting aspect terms from text. Effective aspect extraction serves as the foundation for accurate sentiment analysis at the aspect level. In this paper, we propose aspect extraction models that use different types of embeddings for words and part-of-speech tags and that combine several learning models. We also propose tree positional encoding that is based on dependency parsing output to capture better the aspect positions in sentences. In addition, a new aspect extraction dataset is built for Turkish by machine translating an English dataset in a controlled setting. The experiments conducted on two Turkish datasets showed that the proposed models mostly outperform the studies that use the same datasets, and incorporating tree positional encoding increases the performance of the models.

---

Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis
10.48550/arxiv.2504.11331
Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract fine-grained information from image-text pairs to identify aspect terms and determine their sentiment polarity. However, existing approaches often fall short in simultaneously addressing three core challenges: Sentiment Cue Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise Elimination (SNE). To overcome these limitations, we propose DASCO (\textbf{D}ependency Structure \textbf{A}ugmented \textbf{Sco}ping Framework), a fine-grained scope-oriented framework that enhances aspect-level sentiment reasoning by leveraging dependency parsing trees. First, we designed a multi-task pretraining strategy for MABSA on our base model, combining aspect-oriented enhancement, image-text matching, and aspect-level sentiment-sensitive cognition. This improved the model's perception of aspect terms and sentiment cues while achieving effective image-text alignment, addressing key challenges like SCP and MIM. Furthermore, we incorporate dependency trees as syntactic branch combining with semantic branch, guiding the model to selectively attend to critical contextual elements within a target-specific scope while effectively filtering out irrelevant noise for addressing SNE problem. Extensive experiments on two benchmark datasets across three subtasks demonstrate that DASCO achieves state-of-the-art performance in MABSA, with notable gains in JMASA (+2.3\% F1 and +3.5\% precision on Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .

---

Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples
10.48550/arxiv.2505.10389
This paper explores the design of an aspect-based sentiment analysis system using large language models (LLMs) for real-world use. We focus on quadruple opinion extraction -- identifying aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages. We investigate whether a single fine-tuned model can effectively handle multiple domain-specific taxonomies simultaneously. We demonstrate that a combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity. We also share lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks.

---

Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation
10.48550/arxiv.2505.16237
Large language models (LLMs) have demonstrated remarkable capabilities, but still struggle with issues like hallucinations and outdated information. Retrieval-augmented generation (RAG) addresses these issues by grounding LLM outputs in external knowledge with an Information Retrieval (IR) system. Building on this foundation, graph-based RAG systems go a step further by retrieving subgraphs, which preserve the relationships between knowledge entities and provide more comprehensive context. However, graph RAG faces two challenges: (1) Retrieving relevant information introduces irrelevant nodes (especially in dense graph databases, where retrieval usually extends to adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2) The representation gap between graph and language during generation with LLMs limits the ability to fully leverage graph structures for enhanced understanding. To address these limitations, we propose Align-GRAG, a novel reasoning-guided dual alignment framework in post-retrieval phrase. It first formulates a subgraph by retrieving nodes and edges. Then an Aligner is proposed to jointly optimize a graph encoder with an LLM-summarized reasoning chain. It achieves dual alignment of graph node and representation by leveraging KL divergence loss and contrastive loss, facilitating efficient pruning of irrelevant knowledge and establishing a unified semantic space. The Generator integrates the aligned graph data with LLM to produce coherent and accurate answers. Experiments on the GraphQA benchmark across three tasks (including common sense reasoning, scene graph understanding, and knowledge graph reasoning) validate the effectiveness of our method. The codes are available in this repository\footnote{https://anonymous.4open.science/r/Align-GRAG-F3D8/}.

---

Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task
10.48550/arxiv.2505.18703
This paper introduces the Unified Opinion Concepts (UOC) ontology to integrate opinions within their semantic context. The UOC ontology bridges the gap between the semantic representation of opinion across different formulations. It is a unified conceptualisation based on the facets of opinions studied extensively in NLP and semantic structures described through symbolic descriptions. We further propose the Unified Opinion Concept Extraction (UOCE) task of extracting opinions from the text with enhanced expressivity. Additionally, we provide a manually extended and re-annotated evaluation dataset for this task and tailored evaluation metrics to assess the adherence of extracted opinions to UOC semantics. Finally, we establish baseline performance for the UOCE task using state-of-the-art generative models.

---

FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis
10.48550/arxiv.2505.21040
In this paper, we address the task of targeted sentiment analysis , which involves two sub-tasks, i.e., identifying specific aspects from reviews and determining their corresponding senti-ments. Aspect extraction forms the foundation for sentiment prediction, highlighting the critical dependency between these two tasks for effective cross-task knowledge transfer.

 While most existing studies adopt a multi-task learning paradigm to align task-specific features in the latent space, they predominantly rely on coarse-grained knowledge transfer. Such approaches lack fine-grained control over aspect-sentiment relationships, often assuming uniform sentiment polarity within related aspects. This oversimplification neglects contextual cues that differentiate sentiments, leading to negative transfer.

 To overcome these limitations, we propose FCKT, a fine-grained cross-task knowledge transfer framework tailored for TSA. By explicitly incorporating aspect-level information into sentiment prediction, our framework achieves fine-grained knowledge transfer, effectively mitigating negative transfer and enhancing task performance.

 Extensive experiments on three real-world datasets, including comparisons with various baselines and large language models (LLMs), demonstrate the effectiveness of FCKT. The source code

is available on https://github.com/cwei01/FCKT.

---

Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis
10.48550/arxiv.2506.12991
Aspect-based sentiment analysis (ABSA) generally requires a deep understanding of the contextual information, including the words associated with the aspect terms and their syntactic dependencies. Most existing studies employ advanced encoders (e.g., pre-trained models) to capture such context, especially large language models (LLMs). However, training these encoders is resource-intensive, and in many cases, the available data is insufficient for necessary fine-tuning. Therefore it is challenging for learning LLMs within such restricted environments and computation efficiency requirement. As a result, it motivates the exploration of plug-and-play methods that adapt LLMs to ABSA with minimal effort. In this paper, we propose an approach that integrates extendable components capable of incorporating various types of syntactic knowledge, such as constituent syntax, word dependencies, and combinatory categorial grammar (CCG). Specifically, we propose a memory module that records syntactic information and is incorporated into LLMs to instruct the prediction of sentiment polarities. Importantly, this encoder acts as a versatile, detachable plugin that is trained independently of the LLM. We conduct experiments on benchmark datasets, which show that our approach outperforms strong baselines and previous approaches, thus demonstrates its effectiveness.

---

Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis
10.48550/arxiv.2507.09485
Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in social media scenarios to identify the sentiment polarity of specific aspect terms in a sentence. Although many existing studies leverage large language models (LLMs) to perform ABSA due to their strong context understanding capabilities, they still face challenges to learn the context information in the running text because of the short text, as well as the small and unbalanced labeled training data, where most data are labeled with positive sentiment. Data augmentation (DA) is a feasible strategy for providing richer contextual information, especially when using LLMs to create synthetic training data, but faces challenges in ensuring a high quality of the augmented data.In this paper, we propose an LLM-based ABSA approach with training data augmentation.Specifically, an LLM is prompted to generate augmented training data based on the original training data, so as to construct a new training data with larger size and balanced label distributions to better train an ABSA model. Meanwhile, in order to improve the quality of the augmented data, we propose a reinforcement learning approach to optimize the data augmentation. LLM.Experiment results and further analyses on English benchmark datasets for ABSA demonstrate the effectiveness of our approach, where superior performance is observed over strong baselines and most existing studies.

---

A Survey of Context Engineering for Large Language Models
10.48550/arxiv.2507.13334
The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1400 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.

---

Large Language Models for Czech Aspect-Based Sentiment Analysis
10.48550/arxiv.2508.07860
Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to identify sentiment toward specific aspects of an entity. While large language models (LLMs) have shown strong performance in various natural language processing (NLP) tasks, their capabilities for Czech ABSA remain largely unexplored. In this work, we conduct a comprehensive evaluation of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show that small domain-specific models fine-tuned for ABSA outperform general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs achieve state-of-the-art results. We analyze how factors such as multilingualism, model size, and recency influence performance and present an error analysis highlighting key challenges, particularly in aspect term prediction. Our findings provide insights into the suitability of LLMs for Czech ABSA and offer guidance for future research in this area.

---

Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models
10.48550/arxiv.2508.07866
Aspect-based sentiment analysis (ABSA) has received substantial attention in English, yet challenges remain for low-resource languages due to the scarcity of labelled data. Current cross-lingual ABSA approaches often rely on external translation tools and overlook the potential benefits of incorporating a small number of target language examples into training. In this paper, we evaluate the effect of adding few-shot target language examples to the training set across four ABSA tasks, six target languages, and two sequence-to-sequence models. We show that adding as few as ten target language examples significantly improves performance over zero-shot settings and achieves a similar effect to constrained decoding in reducing prediction errors. Furthermore, we demonstrate that combining 1,000 target language examples with English data can even surpass monolingual baselines. These findings offer practical insights for improving cross-lingual ABSA in low-resource and domain-specific settings, as obtaining ten high-quality annotated examples is both feasible and highly effective.

---

Large Language Models for Subjective Language Understanding: A Survey
10.48550/arxiv.2508.07959
Subjective language understanding refers to a broad set of natural language processing tasks where the goal is to interpret or generate content that conveys personal feelings, opinions, or figurative meanings rather than objective facts. With the advent of large language models (LLMs) such as ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach these inherently nuanced tasks. In this survey, we provide a comprehensive review of recent advances in applying LLMs to subjective language tasks, including sentiment analysis, emotion recognition, sarcasm detection, humor understanding, stance detection, metaphor interpretation, intent detection, and aesthetics assessment. We begin by clarifying the definition of subjective language from linguistic and cognitive perspectives, and we outline the unique challenges posed by subjective language (e.g. ambiguity, figurativeness, context dependence). We then survey the evolution of LLM architectures and techniques that particularly benefit subjectivity tasks, highlighting why LLMs are well-suited to model subtle human-like judgments. For each of the eight tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based methods, and remaining challenges. We provide comparative insights, discussing commonalities and differences among tasks and how multi-task LLM approaches might yield unified models of subjectivity. Finally, we identify open issues such as data limitations, model bias, and ethical considerations, and suggest future research directions. We hope this survey will serve as a valuable resource for researchers and practitioners interested in the intersection of affective computing, figurative language processing, and large-scale language models.

---

Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding
10.48550/arxiv.2508.10369
While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10\%. We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times. We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain.

---

EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks
10.48550/arxiv.2508.17008
Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.

---

Query-Centric Graph Retrieval Augmented Generation
10.48550/arxiv.2509.21237
Graph-based retrieval-augmented generation (RAG) enriches large language models (LLMs) with external knowledge for long-context understanding and multi-hop reasoning, but existing methods face a granularity dilemma: fine-grained entity-level graphs incur high token costs and lose context, while coarse document-level graphs fail to capture nuanced relations. We introduce QCG-RAG, a query-centric graph RAG framework that enables query-granular indexing and multi-hop chunk retrieval. Our query-centric approach leverages Doc2Query and Doc2Query{-}{-} to construct query-centric graphs with controllable granularity, improving graph quality and interpretability. A tailored multi-hop retrieval mechanism then selects relevant chunks via the generated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG consistently outperforms prior chunk-based and graph-based RAG methods in question answering accuracy, establishing a new paradigm for multi-hop reasoning.

---

ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents
10.48550/arxiv.2510.16381
Large Language Models (LLMs) have demonstrated impressive capabilities, yet their deployment in high-stakes domains is hindered by inherent limitations in trustworthiness, including hallucinations, instability, and a lack of transparency. To address these challenges, we introduce a generic neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The core of our approach lies in decoupling tasks into two distinct phases: Offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. This formal representation is crucial as it can be verified and refined by human experts, ensuring its correctness and alignment with domain requirements. In the subsequent task processing phase, each incoming input is encoded into the same formal language. A symbolic decision engine then utilizes this encoded input in conjunction with the formal knowledge base to derive a reliable result. Through an extensive evaluation on a complex reasoning task, we demonstrate that a concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. Crucially, with a human-verified and corrected knowledge base, our approach significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks. By generating decisions grounded in symbolic reasoning, ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.

---

Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model
10.48550/arxiv.2511.00024
In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.

---

Analyzing Sustainability Messaging in Large-Scale Corporate Social Media
10.48550/arxiv.2511.01550
In this work, we introduce a multimodal analysis pipeline that leverages large foundation models in vision and language to analyze corporate social media content, with a focus on sustainability-related communication. Addressing the challenges of evolving, multimodal, and often ambiguous corporate messaging on platforms such as X (formerly Twitter), we employ an ensemble of large language models (LLMs) to annotate a large corpus of corporate tweets on their topical alignment with the 17 Sustainable Development Goals (SDGs). This approach avoids the need for costly, task-specific annotations and explores the potential of such models as ad-hoc annotators for social media data that can efficiently capture both explicit and implicit references to sustainability themes in a scalable manner. Complementing this textual analysis, we utilize vision-language models (VLMs), within a visual understanding framework that uses semantic clusters to uncover patterns in visual sustainability communication. This integrated approach reveals sectoral differences in SDG engagement, temporal trends, and associations between corporate messaging, environmental, social, governance (ESG) risks, and consumer engagement. Our methods-automatic label generation and semantic visual clustering-are broadly applicable to other domains and offer a flexible framework for large-scale social media analysis.

---

ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports
10.48550/arxiv.2511.16438
We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.

---

EulerESG: Automating ESG Disclosure Analysis with LLMs
10.48550/arxiv.2511.21712
Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.

---

EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection
10.48550/arxiv.2512.11506
As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.

---

Unearthing Corporate Greenwashing: A Content Analysis of Sustainability Reporting in the Mining Sector
10.51698/tripodos.2025.57.06
Greenwashing is a common form of corporate disinformation that is often challenging for the general public to detect. Companies engage in greenwashing in their reporting to mislead shareholders and stakeholders about their environmental efforts, seeking to enhance legitimacy and gain competitive advantages. While systems for identifying greenwashing exist, they typically focus on advertising, leaving greenwashing in corporate reports underexamined. This study aims to address this gap by conducting a qualitative content analysis of a climate report from a mining and commodity trading company, considering the company’s profile, the report’s target audience, and current research. The findings reveal that the company employs numerous subtle greenwashing strategies to project a more sustainable image through strategic omissions, hedging and vague long-term goals. Moreover, this research broadens the scope of greenwashing analysis to include corporate reporting, which targets a more informed audience, such as shareholders and experts, rather than the general public. The results underscore the need for stricter regulation of sustainability reports and highlight the importance of fostering greater scrutiny among shareholders.

---

Greenwashing Risk and Its Mitigation for Sustainable Finance
10.52132/ajrsp.e.2024.61.1
Greenwashing presents a significant risk to sustainable financing, prompting researchers to investigate its impact on investors. In response to the urgency of combating this deceptive practice, financial market regulators have increasingly adopted ESG (environmental, social, governance) criteria. This study aims to comprehensively understand the effects of greenwashing by employing a mixed-method approach. Initially, a survey was conducted among participants to analyze their perspectives. The findings reveal a prevailing trend among individuals who seek to optimize investment returns, with a remarkable 50% of respondents demonstrating familiarity with greenwashing tactics. Notably, upon becoming aware of greenwashing practices, a substantial proportion of participants express a strong inclination to revise their investment strategies accordingly. Further exploration into the phenomenon uncovers a combination of factors contributing to greenwashing, including ambiguous legislation, stakeholder pressures, organizational qualities, and individual mindsets. Respondents emphasize the critical importance of enhanced transparency, a heightened emphasis on quality standards, and the enforcement of stringent regulations as key measures to mitigate the prevalence of greenwashing. This study sheds light on investors' sentiments regarding greenwashing, providing valuable insights into the impacts of this unethical practice on sustainable financing. Additionally, it offers practical recommendations to effectively combat greenwashing. By implementing suggested measures such as promoting transparency, prioritizing quality, and enforcing robust regulations.

---

EXPLAINING ESG-DRIVEN FINANCIAL AND SUSTAINABILITY PERFORMANCE IN INDIAN CORPORATIONS USING SHAP AND LIME-BASED INTERPRETABILITY APPROACH
10.52152/9bcaq945
In recent years, Environmental, Social, and Governance (ESG) factors have gained significant attention in shaping corporate financial performance and sustainability strategies. This study investigates the impact of ESG initiatives on the financial and sustainability performance of Indian corporations using a machine learning-based interpretability approach. Specifically, we employ SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) to analyze and interpret the influence of ESG components on corporate performance metrics. By leveraging these explainable AI techniques, we provide insights into how ESG factors drive profitability, risk management, and long-term value creation. This study investigates the impact of ESG initiatives on the financial and sustainability performance of leading Indian corporations, specifically Tata Consultancy Services (TCS), Infosys, Reliance Industries, Mahindra &amp; Mahindra, and ITC Limited. The findings highlight the critical ESG determinants that contribute to sustainable financial growth, offering valuable implications for policymakers, investors, and corporate decision-makers. This research bridges the gap between ESG investment strategies and their quantifiable impact, enhancing transparency in corporate sustainability practices within the Indian business landscape.

---

IMPLEMENTATION OF ESG METRICS IN SMES: LEADERSHIP, GOVERNANCE, AND ORGANIZATIONAL DEVELOPMENT TOWARD BUSINESS SUSTAINABILITY
10.52152/9y5s5c69
This article explores the applicability of ESG (environmental, social, and governance) metrics in small and medium-sized enterprises (SMEs), highlighting their strategic value in the transition to sustainable business models. Using a qualitative, exploratory, and cross-cutting approach, it conducts a theoretical and documentary review of international sources (OECD, GRI, UN, WEF), as well as academic literature and case studies. It analyzes the origins and evolution of ESG criteria, the role of transformational leadership in their implementation, and the barriers SMEs face in adopting them, such as lack of resources, technical knowledge, and cultural resistance to change. As a main contribution, it proposes a simplified table of ESG metrics adapted to the SME context, with clearly defined indicators, formulas, and purposes. The study concludes that these metrics not only allow for assessing performance beyond the financial level, but also strengthen organizational legitimacy, improve decision-making, and contribute to achieving the Sustainable Development Goals, provided there is ethical leadership and governance geared toward positive impact.

---

SMART CONTRACTS IN ESG REPORTING: A FINTECH-BASED FRAMEWORK FOR ENHANCING CORPORATE GOVERNANCE TRANSPARENCY
10.52152/bkv20y90
Environmental, Social, and Governance (ESG) reporting has emerged as a cornerstone of sustainable corporate governance, yet existing disclosure mechanisms remain fragmented, prone to subjectivity, and often criticized for greenwashing. Traditional ESG reports suffer from inconsistent standards, delayed verification, and limited stakeholder trust. This study proposes a fintech-driven framework leveraging smart contracts on blockchain to transform ESG reporting into an automated, transparent, and tamper-proof process. Drawing from a systematic review of existing ESG disclosure practices and fintech applications in governance, the paper outlines how smart contracts can encode reporting obligations, automatically validate sustainability indicators, and provide immutable audit trails. The framework emphasizes three dimensions: (i) automation, where ESG metrics such as carbon footprint, labor diversity, and compliance records are directly linked to predefined smart contract logic; (ii) transparency, as blockchain ensures real-time accessibility and verification for regulators, investors, and stakeholders; and (iii) accountability, by reducing information asymmetry and minimizing risks of manipulation in corporate disclosures. Comparative analysis of traditional vs. blockchain-based ESG systems demonstrates superior efficiency, reliability, and regulatory alignment when smart contracts are deployed. Findings suggest that adopting this fintech-based approach can significantly enhance corporate governance transparency, strengthen investor confidence, and support global sustainability objectives. The study contributes to the growing literature on digital governance by offering a scalable model that integrates ESG reporting, blockchain infrastructure, and policy oversight.

---

A FRAMEWORK FOR STRUCTURED KNOWLEDGE EXTRACTION AND REPRESENTATION FROM NATURAL LANGUAGE THROUGH DEEP SENTENCE ANALYSIS
10.5220/0003663702820287
We present a framework that we are currently developing, that allows one to extract knowledge from natural language sentences using a deep analysis technique based on linguistic dependencies. The extracted knowledge is represented in OOLOT, an intermediate format that we have introduced, inspired by the Language of Thought (LOT) and based on Answer Set Programming (ASP). OOLOT uses an ontologyoriented lexicon and syntax. Therefore, it is possible to export the extracted knowledge into OWL and native ASP.

---

Fuzzy Clustering based Approach for Ontology Alignment
10.5220/0005916805940599
Recently, several ontologies have been proposed for real life domains, where these propositions are large and voluminous due to the complexity of the domain. Consequently, Ontology Aligning has been attracting a great deal of interest in order to establish interoperability between heterogeneous applications. Although, this research has been addressed, most of existing approaches do not well capture suitable correspondences when the size and structure vary vastly across ontologies. Addressing this issue, we propose in this paper a fuzzy clustering based alignment approach which consists on improving the ontological structure organization. The basic idea is to perform the fuzzy clustering technique over the ontologyâ€™s concepts in order to create clusters of similar concepts with estimation of medoids and membership degrees. The uncertainty is due to the fact that a concept has multiple attributes so to be assigned to different classes simultaneously. Then, the ontologies are aligned based on the generated fuzzy clusters with the use of different similarity techniques to discover correspondences between conceptual entities.

---

Question and Answer Classification in Czech Question Answering Benchmark Dataset
10.5220/0007396907010706
In this paper, we introduce a new updated version of the Czech
Question Answering database SQAD v2.1 (Simple Question
Answering Database) with the update being devoted to improved
question and answer classification. The SQAD v2.1 database
contains more than 8,500 question-answer pairs with all
appropriate metadata for QA training and evaluation. We present
the details and changes in the database structure as well as a
new algorithm for detecting the question type and the actual
answer type from the text of the question. The algorithm is
evaluated with more than 4,000 question answer pairs reaching
the F1-measure of 88% for question typed and 85% for answer
type detection.

---

Performance Enhancement of Formula One Drivers with the Use of Group Driven Learning
10.5220/0011274400003274
: Within motorsports less experienced drivers lack pace and performance compared to their peers. Training these drivers requires time, which, due to the regulations and resources, teams often do not have. Less experienced drivers are expected to perform at the same level as experienced drivers. This paper has the aim of analyzing the abilities and performances of both drivers within a Formula One team to redesign the driver training method. The focus is to provide drivers with real-time insights and feedback on their performance during a simulator training session. By using a combination of the principles of process mining and statistical analysis, data markers are created on the track. Based on the differences in telemetry, visual feedback is provided to the driver. Throughout the research, this manner of training has proven to be promising. Drivers showed an increase in their overall performance and an increase in car control and confidence. Despite these promising results more experiments need to be done to guarantee a consistent outcome and to prove the effectiveness of this training program. To continue developments, further research can be conducted on the topic of visualization and communication.

---

Modeling Syntactic Knowledge With Neuro-Symbolic Computation
10.5220/0011718500003393
.

---

IoT-Enabled Agroecology: Advancing Sustainable Smart Farming Through Knowledge-Based Reasoning
10.5220/0012183500003598
: The global increase in population necessitates enhanced food security, yet current agricultural practices are in-adequate in feeding everyone and are detrimental to the environment. Consequently, agriculture faces the task of increasing production while minimizing resource usage and prioritizing sustainability. To assist farmers, new technological tools using AI, Robotics and IoT have been developed in a new ﬁeld called Smart Farming. Unfortunately, these tools are primarily employed in unsustainable farming practices, such as mono-cropping. However, sustainable methods like Agroecology exist, which involve observing how plants interact with their environment to devise crop management strategies that work harmoniously with nature, requiring minimal resources and ensuring sustainability. In this paper, we propose an Internet of Things (IoT) platform that utilizes an ontology and a set of rules to provide farmers with recommendations for optimizing crop development while adhering to agroecology principles. This platform employs Knowledge-based reasoning to correlate crop requirements with local environmental data obtained through a wireless sensor network deployed on the farm. It can suggest crop layouts, crop calendars, detect relevant events, and manage irrigation. Our sys-tem has been tested in a simulated environment and yielded promising results, leaving ample room for future improvements and developments.

---

Dynamic Path Planning for Autonomous Vehicles: A Neuro-Symbolic Approach
10.5220/0012374700003636
: The rise of autonomous vehicles has transformed transportation, promising safer and more efficient mobility. Dynamic path planning is crucial in autonomous driving, requiring real-time decisions for navigating complex environments. Traditional approaches, like rule-based methods or pure machine learning, have limitations in addressing these challenges. This paper explores integrating Neuro-Symbolic Artificial Intelligence (AI) for dynamic path planning in self-driving cars, creating two regression models with the Logic Tensor Networks (LTN) Neuro-Symbolic framework. Tested on the CARLA simulator, the project effectively followed road lanes, avoided obstacles, and adhered to speed limits. Root mean square deviation (RMSE) gauged the LTN models’ performance, revealing significant improvement, particularly with small datasets, showcasing Neuro-Symbolic AI’s data efficiency. However, LTN models had longer training times compared to linear and XGBoost regression models.

---

Leveraging Multimodal Large Language Models and Natural Language Processing Techniques for Comprehensive ESG Risk Score Prediction
10.5220/0012725700003717
: Companies are subject to stringent expectations in terms of social responsibility, particularly in managing risks associated with their environmental, social, and governance (ESG) practices. These practices are evaluated us-ing ESG risk scores. Traditionally, ESG risk scores are generated by firms like Sustainalytics and MSCI, which primarily focus on larger corporations. Consequently, entities investing in smaller companies, such as venture capital firms, private equity firms, and individual investors, face a challenging and resource-intensive process for initial risk assessment. However, our research has uncovered a novel approach through the application of machine learning techniques and the use of multimodal large language models based on publicly released company reports. This approach enables the prediction of ESG risk scores with an accuracy of 68.09%, offering a viable tool for preliminary analysis. Significantly, this research introduces a pioneering framework that utilizes a new architecture for analyzing ESG practices, transforming the traditional assessment process for both large and small companies alike. Our research shows high accuracy in predicting risk assessments and simplifies the evaluation process. Nonetheless, there is potential for enhancing this accuracy through further refinement of the models, improvements in data extraction, and continued exploration of additional modeling techniques.

---

Advancing Cross-Lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models
10.5220/0013349400003890
Aspect-based sentiment analysis (ABSA) has made significant strides, yet challenges remain for low-resource languages due to the predominant focus on English. Current cross-lingual ABSA studies often centre on simpler tasks and rely heavily on external translation tools. In this paper, we present a novel sequence-to-sequence method for compound ABSA tasks that eliminates the need for such tools. Our approach, which uses constrained decoding, improves cross-lingual ABSA performance by up to 10\%. This method broadens the scope of cross-lingual ABSA, enabling it to handle more complex tasks and providing a practical, efficient alternative to translation-dependent techniques. Furthermore, we compare our approach with large language models (LLMs) and show that while fine-tuned multilingual LLMs can achieve comparable results, English-centric LLMs struggle with these tasks.

---

Digitization of High-Stakes Exams
10.53846/goediss-10080
The opportunities for digitization in education have been addressed in research and practice for a long time. These extend across all components of the Curriculum-Instruction-Assessment (CIA) triad according to PELLEGRINO (2010). Thus, digitization is not only changing the required competencies of future workers, but also the way of teaching, learning, and testing. This development is further accelerated by the technological progress in the field of artificial intelligence (AI). In this context, both the STÄNDIGE KONFERENZ DER BILDUNGS- UND KULTUSMINISTER (2022) and the STÄNDIGE WISSENSCHAFTLICHE KOMMISSION (2022) point to the need for increased addressing of digital assessment. A literature analysis conducted as part of this dissertation shows that current research on digital exam execution often focuses on the usage perspective. Thus, primarily an isolated consideration of individual factors influencing the examinees (e.g., stress, familiarity, etc.) takes place. A comprehensive consideration of potential interrelationships between these factors is largely omitted. In the case of digital exam scoring, the use of AI is said to have a high potential in essay scoring. It is shown that currently only the scoring accuracy, but not the design of essay scoring systems, is addressed. This purely technical focus also means that the user perspective (e.g., trust) is not taken into account. Building on these findings, five studies on digital exam execution and scoring are conducted in this cumulative dissertation. In conjunction with the findings from the literature analysis, a total of 13 recommendations for practice were derived based on the results of these five studies. These show that examiners can address usage-oriented factors even before digital exams are conducted. This can reduce the influence of construct-irrelevant factors on test results and thus increase test quality. In the area of digital exam scoring, it is shown that despite technological advances, human scoring involvement can increase confidence in AI-based scorings. Based on these findings, specific design recommendations for semi-automatic AI-based scoring systems are derived. This simplifies the general transfer of technical research results on AI-based exam scoring into productive systems. Finally, further starting points for future research are derived. In particular, the development of large language models (LLM) is expected to have potential.

---

The Truth behind ESG Disclosures: Detecting Greenwashing through Text Readability
10.54097/hks0kp94
As the concept of sustainable development advances, ESG disclosure has become a key component of corporate non-financial reporting. However, some firms engage in strategic obfuscation by complicating the language of ESG reports to exaggerate their actual performance, resulting in greenwashing. This study examines 1133 non-financial A-share listed firms in China from 2019 to 2023, extracting ESG sections from annual reports to construct a text readability indicator. Empirical findings suggest that lower readability is significantly associated with a higher degree of greenwashing. This effect is mitigated in larger firms, where reputation and regulatory pressure impose constraints. Further heterogeneity analysis reveals that such textual manipulation is more prevalent in high-pollution sectors. The study sheds light on the “soft manipulation” path of ESG disclosures, enriching the literature on greenwashing and offering language-based warning indicators for regulators.

---

The Predictive Power of Credit Scores: Examining Default Probability in Taiwanese Credit Card Clients
10.54254/2754-1169/42/20232097
The concept of a scorecard originated from the need to establish a standardized and objective approach to evaluate credit applicants. Various techniques have been utilized to build scoring model. This research chooses Logistic regression to construct a scorecard using SPSS modeler. In this way, the study enhances the understanding of the relationship between credit scores and default behavior. By using a scorecard constructed through logistic regression, the study provides a comprehensive and interpretable model for evaluating creditworthiness. The study also employs linear probability models (LPM), logit, and probit models to assess the predictive power of credit scores on default probability. By utilizing these statistical techniques, the research presents robust empirical evidence on the significance of credit scores in predicting default behavior. Moreover, the research paper systematically analyzes prediction effects with and without control variables. By incorporating control variables such as demographic characteristics, the study adds depth to the understanding of scoring models. Overall, the findings provide valuable guidance for credit risk assessment practices and contribute to the ongoing development of effective credit evaluation frameworks.

---

Impact of ESG News Sentiment Analysis Based on Natural Language Processing on Investment Portfolio Performance
10.54254/2755-2721/2025.gl27106
Environmental, Social, and Governance (ESG) investing has gained unprecedented momentum in global financial markets, driving the need for sophisticated analytical frameworks that can process vast amounts of unstructured information. This research presents a comprehensive investigation into the application of natural language processing techniques for ESG news sentiment analysis and its subsequent impact on investment portfolio performance. The study develops a multi-dimensional sentiment analysis model that extracts ESG-related information from financial news sources, incorporating advanced text mining algorithms to quantify sentiment scores across environmental, social, and governance dimensions. Through empirical analysis of portfolio performance metrics, the research demonstrates that ESG sentiment-driven investment strategies yield superior risk-adjusted returns compared to traditional approaches. The methodology integrates real-time news processing capabilities with portfolio optimization algorithms, enabling dynamic allocation decisions based on sentiment-derived ESG signals. Experimental results indicate a 50.8% improvement in Sharpe ratio and 17.3% reduction in portfolio volatility when incorporating ESG sentiment analysis. The findings contribute to the advancement of sustainable finance technology and provide practical insights for institutional investors seeking to enhance portfolio performance through alternative data integration.

---

CEO Statements in Corporate Sustainability Reports - Substantive Information or Background Noise?
10.5465/ambpp.2012.265
The aim of this paper is to shed light on the question whether or to what extent corporate sustainability reports can actually serve as accurate and fair representations of corporate sustainability-related performance. It presents the results of a sentiment analysis of corporate sustainability reports and corporate financial reports. The analysis builds on previous research in the context of corporate financial reporting which has identified a robust relationship between corporate financial performance and the rhetoric that is used in corresponding corporate financial reports. If corporate sustainability reports are in fact accurate reflections of corporate sustainability performance, then this should also be reflected in the rhetoric used in these reports.

---

The Grammar of Decoupling: A Cognitive-Linguistic Perspective on Firms’ Sustainability Claims and Stakeholders’ Interpretation
10.5465/amj.2015.0171
Can firms deceive their stakeholders, by failing to deliver on their commitments to undertake sustainability practices without being detected? Extant theory posits that, due to information asymmetr...

---

A Data Analytics–Driven Model for Supplier Onboarding and ERP-Based Compliance Management
10.54660/.ijmrge.2021.2.6.538-555
This paper presents a data analytics–driven model for supplier onboarding and ERP-based compliance management that accelerates qualification, strengthens assurance, and mitigates lifecycle risk. The model unifies master-data governance, rule-based eligibility screening, and machine-learning risk scoring with a closed-loop workflow embedded in the enterprise resource planning environment. A standardized digital intake captures identity, regulatory, ESG, cybersecurity, and tax credentials; deterministic rules validate required evidence, while an interpretable gradient-boosted model estimates residual risk using features such as sector, jurisdiction, beneficial ownership depth, sanctions proximity, and historical incident rates. A policy engine links risk tiers to control actions, including enhanced due diligence, dual approvals, and conditional release. All steps write back to ERP vendor master and procurement modules via governed APIs. Operationally, the model defines a golden-record strategy, reference taxonomies, and data-quality rules to prevent duplicate or incomplete vendor profiles. It maps risks to control objectives and embeds preventive gates at supplier creation, contract activation, and first-order release. Continuous monitoring uses event streams and dashboards to detect status changes, expired certificates, adverse media, late attestations, and control drift. Exceptions trigger guided remediation workflows, while feedback loops retrain the model and update thresholds for concept drift. We validate the model through a quasi-experimental design comparing matched business units before and after implementation. Results indicate a 32–45% reduction in onboarding lead time, a 28% decrease in first-year compliance exceptions, and a 19% improvement in audit-readiness scores, while maintaining competition and diversity thresholds. Ablation analyses show the largest effects arise from master-data quality controls and the policy engine’s automated ERP gates. A reference architecture, governance RACI, and value tracking framework are included to support scale-out across multi-ERP landscapes. The contribution is threefold: first, a unified, analytics-first approach that treats onboarding and compliance as a single, data-centric process; second, an interpretable risk scoring method aligned to auditable controls; and third, practical change-management guidance with value realization. Future work will extend causal inference, integrate document intelligence, and explore privacy-preserving data sharing.

---

&lt;b&gt;Exploring the Potential of Using AI Language Models in Democratising Global Language Test Preparation&lt;/b&gt;
10.54855/ijte.24447
This paper delves into the potential of AI language models for democratising global language test preparation, focusing on the accuracy and consistency of assessment in the context of writing essays for IELTS. This quantitative study compares the assessment scores generated by a Human Examiner (HE) and four AI Language Models: ChatGPT, Google Bard, Writing9.com, and Upscore.ai. Evaluation uses Mean Absolute Errors (MEA) and Bland Altman analysis. The findings reveal varying levels of accuracy, with Upscore.ai showcasing the lowest MEA of 0.5, followed by Google Bard at 0.85, ChatGPT at 0.9, and Writing9.com at 1.9. Bland Altman Plots visually represent the agreements between each alternative evaluation system and the Human Examiner, shedding light on their alignment. These results hold significant implications for assisting IELTS test takers in their preparation and advancing the democratisation of IELTS and global language assessment by harnessing AI technology to provide more accessible evaluation methods. AI evaluation systems can support teaching and learning by providing automated feedback when human assistance is unavailable, helping students practice independently. However, the findings show that AI's accuracy is not absolute and varies between models, meaning human involvement remains crucial for comprehensive evaluation.

---

A decision support system architecture for the development and implementation of ESG strategies at SMEs
10.54941/ahfe1002916
Strategic management and business development can be delivered easier in large scale organizations and Multinational Enterprisers (MNEs) due to their excess in human resources, expertise and time orientation. Small and Medium Size Enterprises (SMEs) on the other hand operate in unpredicted environments, with limited resources aiming for their survival first and then their development. The contrast between the two types of enterprises seems chaotic in numbers but not in plans and intentions. The contribution of the MNEs to national economies and impact to the society and the environment can be measured with a variety of standards, metrics and practices such as Corporate Social Responsibility (CSR), the Environmental, Social and Governance Index (ESG), the Social Responsible Investments (SRI) and other. On the other hand, SMEs that also have direct impact to the society, the local and regional economy, the employment, and offer the same, if not more, opportunities to their limited human resources in terms of skills development and effort recognition, cannot record and report such actions, plans and strategies and recognized for their responsible management and leadership. The ESG criteria help organizations develop reputational capital that evolves into financial capital. However, the effort and investments needed to score on the ESG indexes is forbidden for the SMEs who intentionally or intention successfully delivering ESG activities, in a smaller scale, without a system to record and report them. This paper introduces the core design of a decision support system that guides SMEs to maps their operations against the ESG criteria. An extensive literature review has been conducted to identity software systems that coordinate, propose and support ESG activities and analyze elements of such systems that can be extracted for the development of a light system. Such a system, with the relevant enhancements, presented in this paper can be useful tool for the SMEs to report their ESG performance. The system functions as an assessment tool providing a staged evaluation of the SMEs activities, identifies ESG gaps and proposes actions needed to fulfill the requirements of ESG criteria. The final output of the system graphs the distance between the current and the target stage of the SME on the most common ESG criteria. However, the proposed system does not evaluate the scale of each ESG activity implementation but its existence in the SME operations and the degree of its adaptation. The paper also highlights the pre and post conditions for the utilization of the proposed technology using a process map, the social economic impact, research limitations and areas of further research. The goal of this research is to indicate that SMEs can and should be awarded ESG scores as well. For this SMEs can use such supportive technologies to direct them towards ESG compliance, and report achievements and contributions that can help them attract investments needed for them to keep on delivering a greater contribution the local, and why not international, economy and society.

---

An organizational and operational capability and maturity assessment for SMEs in emerging markets towards the ESG criteria adaptation
10.54941/ahfe1004075
SMEs are considered the backbone of every economy forming nearly 95% of the private sector globally. As they are mostly family businesses, start-ups or specialized enterprises, they operate mainly locally or regionally with a direct impact to the society, the, employment and the national economy. On the other hand, due to their limited size and operations they cannot afford the cost and effort needed for long term planning and strategy development that can secure the value, volume, reputation and recognition needed to attract investments. The ESG criteria can be considered as a privilege system primarily for the large-scale organizations and the Multinational Enterprises whose financial and human resources can be easier allocated on ESG activities that return serious financial and reputational benefits. Research indicates that more than 90% of the investors invest only in companies with high social and sustainable profiles. The message from investors and consumers is that if companies cant show any sign of changing their business models, they wont be able to sell. The same applies at country level. Finland for example does not offer any business opportunities to any company that does not have a solid and proven sustainability strategy and track record. This trend, for clients and investors, to consider only Green (sustainable) or Pink (Social) products and companies for their purchases and investments, drives all types of organizations towards that adaptation of the ESG criteria. However, and since such a strategy requires investments, resources, funds and time that only Multinational organizations can provide, SMEs are left out from any opportunity to develop, grow, and compete. This paper highlights this ESG discrimination among the SMEs and MNEs and intends to bridge this gap by identifying, in a smaller scale, activities that reflect the ESG criteria and can be implemented by the SMEs at organizational and operational level. Such an approach provides SMEs the opportunity to record, report and receive credit, visibility and recognition for their sustainable, social and ethical governance efforts and actions, that can potentially enlarge their customer base and attract the investments needed to further develop themselves. The proposed approach is based on an assessment that creates an SME ESG roadmap framework for the SMEs to initially identify their ESG awareness and maturity before adopting any ESG related strategy and commitment. The assessment highlights the SMEs capability and maturity to adopt such a mid-range strategy and align their operations with the ESG criteria on a smaller scale. The results of the assessment formulate an achievable ESG related strategy for each SME, identify the key ESG activities to be implemented, indicate their order of execution, and predict a performance score upon the completion of the proposed strategy. This score can be utilized by SMES to receive social, reputational and financial recognition for their contribution to the local and regional society and economy. To secure the relevance of the SMEs ESG oriented activities with the actual ESG requirements, the proposed approach has been developed after studying several ESG reporting and scoring methodologies such as the Refinitiv, FTSE Russel, BHI, and others, and extractive the most relevant ESG requirements and metrics that can be implemented and measured with the abilities of an SME.Due to the significant variations of the SMEs operations, the proposed framework is targeted primarily to SMEs in emerging markets where the economic development and a structured business environments can help such a novel approach in SMEs strategic management and leadership.

---

Multimodal Sentiment Analysis in Quick Commerce: LSTM Networks for Text, Image, Video Feedback in FMCG Platforms
10.54963/dtra.v4i1.979
This research explores the application of Long Short-Term Memory (LSTM) networks for performing sentiment analysis on customer reviews gathered from prominent FMCG e-commerce platforms such as Blinkit, Zepto, and JioMart. In the fiercely competitive landscape of online retail, accurately interpreting customer sentiment is essential for sustaining customer satisfaction and achieving strategic growth. These platforms accumulate massive amounts of unstructured data—ranging from feedback on product quality to delivery efficiency and overall user experience—which are challenging to process using traditional manual methods. To address this, the study leverages advanced Natural Language Processing (NLP) techniques, with a particular focus on LSTM networks due to their superior ability to model sequential dependencies and retain contextual meaning across review texts. To further enhance performance, pretrained word embeddings are used, enabling the model to understand nuanced language and improve accuracy across varying review structures. Beyond analyzing textual data, the research also integrates visual components into a multimodal sentiment classification framework, offering a holistic understanding of consumer emotions. This dual-modality approach captures subtle sentiments that may not be evident in text alone. The findings yield practical insights for enhancing customer service, optimizing product selections, and improving overall brand engagement. Ultimately, this study empowers data-driven strategies that elevate user experience and market responsiveness in the dynamic FMCG e-commerce industry.

---

From Lexicons to Transformers: An AI View of Sentiment Analysis
10.54963/jic.v4i2.1434
Understanding public opinion at scale is both a scientific challenge and a practical necessity in the digital era, as the proliferation of online communication platforms has created unprecedented opportunities to monitor attitudes in near real time. Early work in subjectivity detection and semantic orientation laid the methodological foundations for automated sentiment extraction, focusing on distinguishing objective from subjective content and determining polarity. Contemporary applications, however, face far more complex requirements, demanding systems capable of processing massive, noisy, and dynamic data streams while integrating multimodal signals from text, images, audio, and video. This paper presents a historical review of sentiment analysis and opinion monitoring through the lens of artificial intelligence, tracing developments from the early 1990s to the present and classifying approaches from lexicon‑based heuristics to classical machine learning, deep neural architectures, transfer learning, and multimodal fusion, with an emphasis on both technical and conceptual advances. Extensive tables summarize algorithms, datasets, and case studies across various domains, including politics, finance, and entertainment, highlighting practical lessons and performance trends. The review also addresses pressing ethical concerns, including bias, fairness, and transparency, and considers the implications of rapidly evolving AI capabilities. We conclude by outlining future directions that emphasize adaptability, context awareness, and the seamless integration of emerging technologies into scalable and reliable opinion analysis systems.

---

Corporate Perspectives in Achieving Sustainable Development Goals (SDGS) Integration with Sustainability Reporting
10.55927/ministal.v4i2.13709
The issue of climate change that continues to hit the world has become the centre of global attention. Indonesia's participation in the SDGs is not only an effort to achieve sustainable development in the global world. The purpose of this study is to observe and explore the Company's perspective on achieving Sustainable Development goals in the Sustainability Report it submits. The method used is a literature study approach from research articles on the same topic for the period 2020-2024. The results of the study indicate that entities are increasingly aware of the importance of the SDGs and support their inclusion in business strategies and sustainability reports. In addition, this study found that factors that determine the success of SDGs implementation include integrated sustainability governance, cross-sectoral collaboration, and a dedicated sustainability officer. The results of this study also show that there is a positive relationship between SDGs implementation and return on equity (ROE) in terms of financial performance. This conclusion suggests that SDGs implementation can provide economic and social benefits if carried out with Strengthening sustainability governance and alignment between reporting and SDGs implementation in corporate strategy are recommendations from this study.

---

Sustainable performance management in the EU SME sector. A review and analysis of concepts and methods of strategic management accounting
10.5604/01.3001.0054.0890
Purpose: The paper’s main objective is to structure the knowledge of the existing regulatory frameworks, projects and actions that support sustainable performance management (SPM) in small and medium-sized enterprises (SMEs) against the backdrop of the growing significance of the circular economy. The paper will also evidence how strategic management accounting (SMA) assists this process, what individual metrics, dashboards or scorecard concepts have been proposed and how their use may be assessed. Methodology/approach: The research methods include (1) a descriptive systematic analysis of the policies established by the policymakers and regulators in the EU, (2) a comparative analysis of various organisations’ initiatives, actions, delivered toolkits and their outputs, and (3) a literature review of the essential works and research studies on sustainable business models (SBMs), circular business models (CBMs) and SPM in the context of SMEs. Findings: The policies established in the EU support SPM by encouraging SMEs to be involved in sustainable business practices and indicating how to embark on a green transition. Recently, multiple initiatives have been aimed at indicating the opportunities the closed-loop economy system offers SMEs. Many toolkits have also been developed to help SMEs measure their sustainable performance. Although this measurement is vital for SMEs, the business practice falls behind the regulatory framework. Therefore, internal initiatives from the SME sector are needed to popularise the sustainability concept. Research limitations/implications: SMEs show lower levels of compliance with environmental requirements and are unaware of how their activities affect the environment. A limitation of the research is that, in many cases, SMEs are not ready to respond properly to the ESG requirements imposed by regulators. The reason may be a lack of knowledge, experience and limited funds. Therefore, future research should focus on recognizing the gaps in this area and identifying what may be impeding the development of sustainability performance management in SMEs. Originality/value: The work presents the evolution path from the traditional business models (TBMs) through the SBMs to innovative CBMs and addresses their distinctive features. It contributes to the existing knowledge about SMA in SMEs by analysing its links with SPM.

---

Neuro- Symbolic Compliance Architectures: Real-Time Detection of Evolving Financial Crimes Using Hybrid AI
10.56127/ijst.v4i1.1961
This paper proposes NeuroSym-AML, a new neuro-symbolic AI framework explicitly designed for the real-time detection of evolving financial crimes with a special focus on cross-border transactions. By combining Graph Neural Networks (GNNs) with interpretable rule-based reasoning, our system dynamically adapts to emerging money laundering patterns while ensuring strict compliance with FATF/OFAC regulations. In contrast to static rule-based systems, NeuroSym-AML shows better performance-an 83.6% detection accuracy to identify financial criminals, which demonstrated a 31% higher uplift compared with conventional systems-produced by utilizing datasets from 14 million SWIFT transactions. Furthermore, it is continuously learning new criminal typologies, providing decision trails that are available to regulatory audit in real-time. Key innovations include: (1) the continuous self-updating of detection heuristics, (2) automatic natural language processing of the latest regulatory updates, and (3) adversarial robustness against evasion techniques. This hybrid architecture bridges the scalability of machine learning with interpretability of symbolic AI, which can address crucial gaps for financial crime prevention, therefore delivering a solution for satisfying both adaptive fraud detection and transparency in decision-making in high-stakes financial environments.

---

CFA Institute Survey Report on the ESG Regulatory Framework in the EU
10.56227/24.2.10
The European Union has been advancing its sustainable finance agenda. But despite progress in ESG integration, complex regulations and unreliable ESG data are challenges. Our survey recommends clarifying ESG terms and aligning global standards to enhance investor protection.

---

An Ontological Framework for Knowledge Management in Systems Engineering Processes
10.5772/9554
Systems Engineering (SE) processes comprise highly creative and knowledge-intensive tasks
that involve extensive problem-solving and decision-making activities among
interdisciplinary teams (Meinadier, 2002). SE projects involve the definition of multiple
artifacts that present different formalization degrees, such as requirements specification,
system architecture, and hardware/ software components. Transitions between the project
phases stem from decision making processes supported both by generally available domain
and design knowledge.
We argue that Knowledge about engineering processes constitutes one of the most valuable
assets for SE organizations. Most often, this knowledge is only known implicitly, relying
heavily on the personal experience background of system engineers. To fully exploit this
intellectual capital, it must be made explicit and shared among project teams. Consistent
and comprehensive knowledge management methods need to be applied to capture and
integrate the individual knowledge items emerging in the course of a system engineering
project.

---

ESG Reporting: Assessing Compliance of Polish Public Companies with EU Taxonomy Guidelines
10.58691/man/196152
The paper aims to evaluate how well Polish public companies' activities align with the European Union Taxonomy guidelines (EU Taxonomy). The research involved a review of literature on ESG and non-financial reporting, identification of key guidelines for mandatory disclosures, and analysis of reports from companies listed on the Warsaw Stock Exchange’s three main indices. This approach allowed for determining the shares of activities that meet the EU Taxonomy's compliance and eligibility criteria, focusing on turnover, capital expenditures (CapEx), and operating expenditures (OpEx). The research shows a relatively low percentage of activities compliant with the EU Taxonomy among the studied companies. There are significant sectoral differences in the level of compliance of companies activity with the EU Taxonomy, with compliance level highest in industries with substantial environmental impacts (like industrial production and construction) and lowest in sectors with limited environmental effects (such as finance and healthcare). A key limitation is the analysis's scope, which covers only one year's reports due to the non-financial reporting standards' implementation schedule. Continuous monitoring of this issue is necessary. The ESG activities of companies are crucial from the perspective of building competitive advantage, influencing customer awareness, meeting consumer expectations or finally meeting non-financial reporting requirements, but involve complex processes, including operationalization, implementation, monitoring, and auditing of ESG practices. Non-financial reporting entails significant direct and indirect costs. Non-financial reporting enhances the transparency of companies' operations and is likely to influence company ratings for investment purposes. However, the costs associated with reporting could lead to higher prices and potentially burden consumers. The article addresses a current, practical problem faced by companies required to conduct non-financial reporting. It is relevant for various stakeholders, including consumers, shareholders, business partners, and financiers.

---

PRISMA systematic review: The application of natural language processing (NLP) to identify greenwashing in sustainability reports within the oil and gas industry
10.61511/jimese.v3i1.2025.2004
Background: Greenwashing refers to misleading sustainability claims not backed by real actions, commonly seen in the oil and gas industry due to its dependence on fossil fuels. While companies may publicly commit to sustainability, their investments often contradict these claims, obstructing global renewable energy efforts. This mismatch between statements and actions misleads stakeholders and complicates audit processes. As demands for transparency grow, there is a pressing need for systematic tools to detect greenwashing. Prior research highlights that the narrative format of sustainability reports makes manual detection difficult, underscoring the need for technology-based solutions. Methods: This study aims to examine the application of Natural Language Processing (NLP), particularly the N-Gram model, in identifying indications of greenwashing in the oil and gas industry. The research uses a qualitative approach with a Systematic Literature Review (SLR) method and applies the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework. Findings: The N-Gram model aids in feature extraction by converting raw text from sustainability reports into structured representations and detecting linguistic patterns commonly found in overstated sustainability claims. When combined with classification methods like Support Vector Machine (SVM), it improves the accuracy of greenwashing detection. Key findings show that NLP can support auditors in assessing greenwashing risks and improving the efficiency of sustainability audits. Moreover, the integration of this technology promotes greater transparency in corporate disclosures. Conclusion: The application of the N-Gram model in the NLP context is effective in detecting greenwashing practices that were previously difficult to identify manually. Novelty/Originality of this article: This study offers novelty through the application of the N-Gram NLP model within the oil and gas industry context, which has been rarely explored in previous research. The practical implications of this study open opportunities for cross-sectoral implementation and the development of data-driven greenwashing identification standards in the future.

---

AI-POWERED SENTIMENT ANALYSIS IN DIGITAL MARKETING: A REVIEW OF CUSTOMER FEEDBACK LOOPS IN IT SERVICES
10.63125/61pqqq54
This systematic review critically examines the evolving role of AI-powered sentiment analysis in optimizing digital marketing strategies, with a specific focus on its application within customer feedback loops in IT service environments. In the era of data-driven marketing, the ability to decode consumer emotions from unstructured textual sources—such as social media, product reviews, helpdesk transcripts, and chat logs—has become increasingly valuable for enhancing personalization, engagement, and service responsiveness. Adhering to the PRISMA 2020 methodology, this review rigorously analyzed 87 peer-reviewed articles published between 2010 and 2024, encompassing diverse disciplines including artificial intelligence, natural language processing, marketing analytics, and service operations. The findings reveal that while traditional stochastic models like Support Vector Machines remain widely used due to their computational efficiency and interpretability, deep learning architectures—particularly CNNs, LSTMs, and GRUs—have demonstrated superior performance in managing complex, context-rich sentiment patterns. Moreover, transformer-based models such as BERT and RoBERTa have emerged as state-of-the-art tools, excelling in multilingual sentiment interpretation and capturing nuanced emotional dynamics in long-form or domain-specific feedback. The integration of these models into customer feedback loops has enabled real-time marketing decision-making, automated customer relationship management, and sentiment-driven content optimization. However, the review also identifies key gaps, notably the underutilization of internal enterprise data sources and the lack of comprehensive adoption of explainable AI practices. Increasing scrutiny under data protection regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) has further underscored the need for transparency, user consent, and ethical handling of inferred emotional data. Overall, this review contributes to the growing body of literature by offering a comprehensive evaluation of current technologies, identifying operational challenges, and highlighting the need for ethically aligned and context-aware sentiment analytics frameworks in digital marketing ecosystems, particularly within the IT services sector.

---

Few-Shot Sentiment Adaptation: A MAML -Based Framework for Low-Resource NLP
10.63503/j.ijcma.2025.94
Sentiment analysis in low-resource languages has a tough obstacle to jump over because there just isn’t enough labeled data. Traditional deep learning models tend to hit a wall in these situations since they need large datasets to really shine. To tackle this issue, we came up with Few-Shot Sentiment Adaptation (FSSA), a meta-learning framework based on MAML. This cool approach lets us classify sentiment with just a few labeled examples. By training on languages that have lots of resources and then adapting to those that don’t, we can quickly pick up on sentiment patterns using a 5-way, 5-shot method. We tested FSSA on some public low-resource sentiment datasets and compared it with fine- tuned BERT models, zero-shot learning, and other few-shot classification techniques. Our results showed a major improvement over existing methods, proving to be pretty adaptable even when dealing with limited data. Unlike the usual transfer learning methods, FSSA allows for quick tweaks without needing extensive fine-tuning, making it ideal for real-world low-resource Natural Language Processing (NLP) applications. This research helps bridge the gap between high- and low-resource languages in the NLP field, reducing the reliance on large annotated datasets. We’ve made few-shot meta-learning for sentiment analysis easier to understand, paving the way for more strong and efficient language models.

---

Contrastive Cross-Lingual Calibration for Large Language Models
10.64748/8gkysx36
Large language models (LLMs) are increasingly deployed in multilingual settings, yet their probability estimates are often miscalibrated, particularly for low-resource languages and code-switched inputs. We present C³, a post-hoc calibration framework that reduces cross-lingual miscalibration by optimizing language-aware temperature and bias parameters using contrastive counterfactuals generated via translation/back-translation and meaning-preserving perturbations. C³ aligns confidence across languages without retraining the base model. On classification (XNLI) and extractive/generative QA (XQuAD, MLQA, TyDi QA GoldP), C³ lowers Expected Calibration Error by 35–57% and Brier score by 9–18%, with modest accuracy gains (0.7–2.1 pp). For generative QA, hallucination rate decreases by 21% while maintaining answer quality. Benefits are largest for Swahili and Arabic and persist under code-switch and spelling noise. Ablations show that contrastive counterfactuals and language-specific scaling both contribute, and isotonic fusion improves tails of the confidence distribution. We release calibration recipes and evaluation scripts to support responsible multilingual deployment.

---

Uncovering hidden signals for sustainable investing using Big Data: Artificial intelligence, machine learning and natural language processing
10.69554/cikj7477
Risk managers and investors have increasingly been seeking high-quality environment, social and governance (ESG) data in order to assess nonfinancial risks as well as allocate capital towards companies that manage themselves in a ‘socially responsible’ way and adhere to their contract with society. The problem is that due to the lack of agreed-upon standards for companies to use for reporting on sustainability issues, there is a paucity of high-quality firm-level data to serve as key inputs in assessing a company’s risks and adherence to ESG criteria. Big Data, developed through cutting-edge statistical models, artificial intelligence (AI) and natural language processing (NLP) covering dozens of languages, provides the solution for ESG rankings and ratings and can help combat self-reported bias and ‘greenwashing’ and provide high-quality data. The ‘next generation’ measures of firms ‘doing good’ are the UN sustainable development goals (SDGs), which are this decade’s benchmarks against which millennials and many investors are beginning to assess companies. The SDGs go beyond the more narrowly focused set of sustainability issues embedded in ESGs, and quality data to measure performance against the SDGs are even more sparse. Using Big Data, Global AI Corporation uncovers data measuring companies’ and counties’ performance on all 17 SDGs, which can enable the integration of SDG factors into investment, risk management and national policy decision-making processes. Big Data is providing statistical indicators and performance metrics data to national governments and the United Nations to benchmark progress towards achieving the SDGs. It is also producing the SDG footprint of the private sector at the regional and global levels for policy purposes as shown in the United Nations Conference on Trade and Development’s (UNCTAD) SDG Pulse publication. Using Big Data, Global AI Corporation eliminates self-reporting biases and uncovers hidden data, which results in negative as well as positive ESG/SDG scores, while the self-reporting data only produces positive scores.

---

AI-Based Data Governance: Empowering Trust and Compliance in Complex Data Ecosystems
10.70153/ijcmi/2021.13301
In today’s interconnected digital environment, data governance is critical for ensuring regulatory compliance, data quality, and user trust. Traditional rule-based systems are often rigid, unable to cope with the dynamic and heterogeneous nature of modern data ecosystems. This paper presents an AI-driven data governance framework designed to automate policy enforcement, detect anomalies, and ensure continuous compliance across complex infrastructures. Leveraging machine learning and natural language processing, the system can adapt to evolving regulatory requirements, perform real-time data classification, and recommend corrective actions. Our proposed solution demonstrates significant improvements in compliance assurance, data quality scores, and governance efficiency. Experimental results across multi-cloud datasets reveal a 92% accuracy in detecting policy violations and a 38% reduction in manual auditing tasks, illustrating the transformative potential of AI in governance landscapes.

---

Most Cited AI Research (2024–2025): A Cross-Sector Review
10.70470/edraak/2025/011
The blistering pace of generative and foundational AI models being deployed in 2024 and 2025 is transforming experiences in education, healthcare, science, sustainability and business. This narrative review consolidates findings from the 50 most cited peer reviewed publications in this time frame, providing a cross-cutting overview on the state of development, the application and the challenges concerning technology. We start by discussing the architectural origins behind both large language models, multimodal generators, as well as domain-specific foundation models including SpectralGPT and scGPT. Then, we evaluate their application on vertical-industrial applications of academic teaching, clinical diagnosis, supply chain operation, and environmental monitoring. The review discusses also important ethical and societal issues, including fairness, explainability, AI angst, and academic responsibility. We also discuss lingering technical challenges including hallucination, data privacy, and availability barriers despite recent progress made. Finally, we discuss some of the emerging frontiers that this work opens and their exciting implications, including controllable generation, symbolic-neural integration, and divergence between open and proprietary model ecosystems. This citation-motivated review provides a timely snapshot of how the most influential research is leading the development of generative AI across domains.

---

Explainable Artificial Intelligence (XAI) as a foundation for trustworthy artificial intelligence
10.70593/978-81-981367-4-9_1
"The rapid integration of artificial intelligence (AI) into various sectors necessitates a focus on trustworthiness, characterized by principles such as fairness, transparency, accountability, robustness, privacy, and ethics. Explainable AI has become essential and central to the achievement of trustworthy AI by answering the ""black box"" nature of top-of-the-line AI models through its interpretability. The research further develops the core principles relating to trustworthy AI

---

An Exploratory Study of the Relationship Between Corporate Social Responsibility and Financial Performance: The Role of Artificial Intelligence in Enhancing CSR and Financial Outcomes
10.70670/sra.v3i1.458
This paper aims to look into the concept of CSR and its link to financial performance, especially when AI is integrated into these activities. Businesses are gradually embracing sustainability and ethical practices in carrying out their activities, and the use of artificial intelligence in metrics, analytics, and automation, as well as ESG reports, are transforming CSR practice, measurement, and reporting. They have examined the link between CSR and financial performance but have not delved much into the role of AI in implementing CSR activities. This research is based on the qualitative research method; thus, the impact of AI in CSR is analyzed by conducting secondary data analysis and case studies with companies that have adopted AI in CSR. The study focuses on the factors of CSR that include improvement of AI efficiency, and how they help increase the company’s reputation and financial profit in the long run. In the literature, it has been observed that Level 4 AI helps in greater effectiveness of CSR execution in terms of operational impact, ESG monitoring, risk profiling, and investors’ confidence. AI-implemented CSR is more effective and efficient, M and has high financial returns compared to traditional CSR techniques. The paper extends the knowledge in this field by integrating AI as a moderator between CSR practices and financial performance and offers value for organizations with guidelines for the proper use of AI for profitable subsequent CSR practices. Lastly, policy implications that need to be considered are outlined to help the regulators create standardized frameworks for using AI in CSR. Further research direction includes analyzing CSR in relation to artificial intelligence ethics, legal issues, and the long-term financial effects of AI-facilitated sustainable development projects.

---

Statistical Relational Learning for Natural Language Information Extraction
10.7551/mitpress/7432.003.0021
1.1 Introduction Understanding natural language presents many challenging problems that lend themselves to statistical relational learning (SRL). Historically, both logical and probabilistic methods have found wide application in natural language processing (NLP). NLP inevitably involves reasoning about an arbitrary number of entities (people, places, and things) that have an unbounded set of complex relationships between them. Representing and reasoning about unbounded sets of entities and relations has generally been considered a strength of predicate logic. However, NLP also requires integrating uncertain evidence from a variety of sources in order to resolve numerous syntactic and semantic ambiguities. Effectively integrating multiple sources of uncertain evidence has generally been considered a strength of Bayesian probabilistic methods and graphical models. Consequently, NLP problems are particularly suited for SRL methods that combine the strengths of first-order predicate logic and probabilistic graphical models. In this article, we review our recent work (Bunescu and Mooney [2004]) on using Relational Markov Networks

---

Evolving techniques in sentiment analysis: a comprehensive review
10.7717/peerj-cs.2592
With the rapid expansion of social media and e-commerce platforms, an unprecedented volume of user-generated content has emerged, offering organizations, governments, and researchers invaluable insights into public sentiment. Yet, the vast and unstructured nature of this data challenges traditional analysis methods. Sentiment analysis, a specialized field within natural language processing, has evolved to meet these challenges by automating the detection and categorization of opinions and emotions in text. This review comprehensively examines the evolving techniques in sentiment analysis, detailing foundational processes such as data gathering and feature extraction. It explores a spectrum of methodologies, from classical word embedding techniques and machine learning algorithms to recent contextual embedding and advanced transformer models like Generative Pre-trained Transformer (GPT), Bidirectional Encoder Representations from Transformers (BERT), and T5. With a critical comparison of these methods, this article highlights their appropriate uses and limitations. Additionally, the review provides a thorough overview of current trends, insights into future directions, and a critical exploration of unresolved challenges. By synthesizing these developments, this review equips researchers with a solid foundation for assessing the current state of sentiment analysis and guiding future advancements in this dynamic field.

---

A semi-permeable attention network for ESG score prediction
10.7717/peerj-cs.3333
Environmental, Social, and Governance (ESG) metrics have become critical indicators of corporate sustainability, ethical behavior, and long-term financial performance. However, accurately predicting ESG scores remains challenging due to the tabular nature of ESG datasets and their small size, which often limits the effectiveness of traditional deep learning approaches. In this study, we propose an attention-based deep learning model specifically designed for tabular ESG data. Our model leverages the semi-permeable attention mechanism in the ExcelFormer architecture to selectively regulate feature interactions based on their predictive importance. This design enables the model to mitigate noise from less informative features while preserving critical dependencies within structured data. We evaluate our method on a simulated dataset comprising ESG and financial performance data from 1,000 companies across multiple industries and regions. The proposed model consistently outperforms traditional machine learning models and state-of-the-art tabular deep learning models, achieving the lowest errors across all ESG dimensions. Specifically, it attains a coefficient of determination of 0.7373 for overall ESG prediction, with a mean squared error of 0.0063. These results demonstrate the potential of attention-augmented tabular neural networks in advancing ESG forecasting, offering meaningful contributions to the field of sustainable finance.